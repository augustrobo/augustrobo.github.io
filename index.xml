<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on NOWHERESVILLE</title>
    <link>/</link>
    <description>Recent content in Home on NOWHERESVILLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CS231n-CNN for Visual Recognition-Assignment1</title>
      <link>/post/2018/03/07/cs231n/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/07/cs231n/</guid>
      <description>Image Classification Challenges  Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning</title>
      <link>/post/2018/02/27/nn/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/27/nn/</guid>
      <description>Introduction What is a Neural Network?  ReLU = Rectified Linear Unit     Input Output Application Model     Home Features Price Real Estate NN   Ad, User info Click on ad? 0/1 Online Advertising NN   Image Object Photo Tagging CNN   Audio Text transcript Speech Recognition RNN   English Chinese Machine Translation RNN   Image, Radar info Position of other cars Autonomous Driving Hybrid     Image - convolutional neural network, CNN sequence data (temporal data, time series) - recurrent neural network, RNN   Structured data: database Unstructured data: audio, image, text  Why is Deep Learning taking off?</description>
    </item>
    
    <item>
      <title>MMDS - MapReduce</title>
      <link>/post/2018/02/05/mapreduce/</link>
      <pubDate>Mon, 05 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/05/mapreduce/</guid>
      <description> 1 </description>
    </item>
    
    <item>
      <title>Data Manipulation at Scale 大规模数据处理</title>
      <link>/post/2018/02/01/data-manipulation-at-scale/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/01/data-manipulation-at-scale/</guid>
      <description>课程介绍  Data Manipulation at Scale: Systems and Algorithms 大规模数据处理：系统和算法【 Coursera链接 】 Data Science at Scale Specialization 的第一门课，共4门 制作方： University of Washington 教学方： Bill Howe  Twitter Sentiment Analysis 情感分析 分析目的  access the twitter Application Programming Interface(API) using python estimate the public&amp;rsquo;s perception (the sentiment) of a particular term or phrase analyze the relationship between location and mood based on a sample of twitter data  步骤  CLI安装 oauth2 package [用于授权登录]  pip3 install oauth2   创建 twitter 账号 到 https://apps.</description>
    </item>
    
    <item>
      <title>Algorithms-Design and Analysis(Stanford) Notes</title>
      <link>/post/2018/01/25/algorithms/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/25/algorithms/</guid>
      <description>PDF格式笔记见：Notes
Divide and Conquer 分而治之  DIVIDE into smaller sub-problems CONQUER via recursive calls COMBINE solutions of sub-problems into one for the original problem  Master Method  Cool feature: a &amp;ldquo;black-box&amp;rdquo; method for solving recurrences
 Determine the upper bound of running time for most of the D&amp;amp;C algos
 Assumption: all sub-problems have equal size
    unbalanced sub-problems? more than one recurrence?    Recurrence format:</description>
    </item>
    
    <item>
      <title>Leetcode算法题目（不断更新）</title>
      <link>/post/2018/01/25/leetcode/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/25/leetcode/</guid>
      <description>Maximum Subarray  Find the contiguous subarray within an array (containing at least one number) which has the largest sum.
For example, given the array [-2,1,-3,4,-1,2,1,-5,4], the contiguous subarray [4,-1,2,1] has the largest sum = 6.
 Merge Sorted Array  Given two sorted integer arrays nums1 and nums2, merge nums2 into nums1 as one sorted array.
Note: You may assume that nums1 has enough space (size that is greater or equal to m + n) to hold additional elements from nums2.</description>
    </item>
    
    <item>
      <title>Python Data Structures</title>
      <link>/post/2018/01/25/python-data-structure/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/25/python-data-structure/</guid>
      <description> Operations:
 insert 插入 remove 移除 iterate 遍历 test if empty 检验空否  Stack  Examine the item most recently added &amp;lt;- LIFO = last in first out
 push (insert)
 pop (remove)
  ​
Queue  Examine the item least recently added &amp;lt;- FIFO = first in first out enqueue (insert) dequeue (remove)  Heap Tree </description>
    </item>
    
    <item>
      <title>Python中的取整方式</title>
      <link>/post/2018/01/19/python-float-to-int/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/19/python-float-to-int/</guid>
      <description>方式一：round() 四舍五入 Python中的 round() 有两个参数，第一个参数是需要处理的数，第二个参数是数位精度，默认为0。
round(3.4) ## 3 round(3.5) ## 4  而有时候会出现奇怪的情况，比如：
round(3.24, 1) #是四舍五入 ## 3.2 round(3.26, 1) #是四舍五入 ## 3.3 round(3.25, 1) #不是四舍五入 ## 3.2 ################################### round(0.44, 1) #是四舍五入 ## 0.4 round(0.46, 1) #是四舍五入 ## 0.5 round(0.45, 1) #是四舍五入 ## 0.5  很多人说Python3中采用的是【四舍六入五留双】，上面的例子说明这种说法是不正确的。其实是因为：
 十进制小数在计算机内是通过二进制小数来近似，在舍和进两个选项中选择更接近的一个 而当舍和进的两个选项十分接近时，round 选择偶数的选项  这就导致出现的结果非常复杂了。
进一步解释：十进制小数 $0.2$ 和 $0.3$ 的二进制表示分别为：
$$ \begin{align} (0.2)_{10} &amp;amp; = \left(\frac{1}{8}+\frac{1}{16}\right)+\left(\frac{1}{128}+\frac{1}{256}\right)+\cdots =\frac{\frac{1}{8}+\frac{1}{16}}{1-\frac{1}{16}} =\frac{3}{15}=\frac{1}{5}\newline &amp;amp;=(0.\dot{0}\dot{0}\dot{1}\dot{1})_2 \end{align} $$
以及 $$ \begin{align} (0.</description>
    </item>
    
    <item>
      <title>Python Type Conversion</title>
      <link>/post/2018/01/01/python-type-conversion/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/python-type-conversion/</guid>
      <description>Types  Variables in Python are dynamically-typed: declared without an explicit type. objects have a type, so Python knows the type of a variable. No char in Python! Both &#39; and &amp;quot; create string literals  type(1) # &amp;lt;class &#39;int&#39;&amp;gt; type(1.0) # &amp;lt;class &#39;float&#39;&amp;gt; type(2/1) # &amp;lt;class &#39;float&#39;&amp;gt; type(&amp;quot;hello&amp;quot;) # &amp;lt;class &#39;str&#39;&amp;gt; type(&#39;a&#39;) # &amp;lt;class &#39;str&#39;&amp;gt; type(True) # &amp;lt;class &#39;bool&#39;&amp;gt; type(None) # &amp;lt;class &#39;NoneType&#39;&amp;gt; type(int) # &amp;lt;class &#39;type&#39;&amp;gt; type(type(int)) # &amp;lt;class &#39;type&#39;&amp;gt; type(print) # &amp;lt;class &#39;builtin_function_or_method&#39;&amp;gt;   bool is a subtype of int, where True == 1 and False == 0  True == 1 # True False == 0 # True True == 2 # False True or False # True (short-circuits)  Truthy and Falsy # &#39;Falsy&#39; bool(None) bool(False) bool(0) bool(0.</description>
    </item>
    
    <item>
      <title>Python练习题（不断更新）</title>
      <link>/post/2018/01/01/python-exercises/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/python-exercises/</guid>
      <description>基础 Tic-tac-toe  Write a program using print() that, when run, prints out a tic-tac-toe board.   | | -------- | | -------- | |   Write a program that, when run, prints out a SUPER tic-tac-toe board.   | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | | ========+========+======== | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | | ========+========+======== | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | | --+--+--H--+--+--H--+--+-- | | H | | H | |  数据结构 算法 Greatest Common Divisor 最大公因数 Euclid&amp;rsquo;s algorithm:</description>
    </item>
    
    <item>
      <title>Searching 搜索算法</title>
      <link>/post/2018/01/01/searching/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/searching/</guid>
      <description>Linear Search Search an Unsorted List def linearSearch(arr, target): for i in range(len(arr)): if arr[i] == target: return i return None  Analysis of running time:
 worst-case: $O(n)$ best-case: $O(1)$  More efficient linear search:
 put a sentinel in the list to avoid checking i&amp;lt;len(arr) each time
 faster in practice, but only by a constant factor
  def linearSearch2(arr, target): last, arr[-1] = arr[-1], target i = 0 while arr[i] !</description>
    </item>
    
    <item>
      <title>Sorting 排序算法</title>
      <link>/post/2018/01/01/sorting/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/sorting/</guid>
      <description>Visualization of Sorting Algorithms https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html
https://visualgo.net/bn/sorting
https://www.toptal.com/developers/sorting-algorithms
References  MIT, 6.006, Introduction to Algorithms, LECTURE 3 CLRS , Chapter 2  Summary     Best-Case $T(n)$ Worst-Case $T(n)$ Average-Case $T(n)$? Space Complexity     Bubble Sort $O(n)$ $O(n^2)$ $O(n^2)$ $O(1)$   Selection Sort $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$   Insertion Sort $O(n)$ $O(n^2)$ $O(n^2)$ $O(1)$   Merge Sort $O(n\log n)$ $O(n\log n)$ $O(n\log n)$ $O(n)$   Quick Sort $O(n\log n)$ $O(n^2)$ $O(n\log n)$ $O(1)$    Can We Sort Faster?</description>
    </item>
    
    <item>
      <title>ML - Application Example - Photo OCR</title>
      <link>/post/2017/11/11/ocr/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/11/ocr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Large Scale Machine Learning</title>
      <link>/post/2017/11/10/large-scale-ml/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/10/large-scale-ml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Recommender Systems</title>
      <link>/post/2017/11/09/recommender-systems/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/09/recommender-systems/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Anomaly Detection</title>
      <link>/post/2017/11/08/anomaly/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/08/anomaly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Dimensionality Reduction</title>
      <link>/post/2017/11/08/dimension-reduction/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/08/dimension-reduction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Unsupervised Learning</title>
      <link>/post/2017/11/07/unsupervised/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/07/unsupervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Support Vector Machines</title>
      <link>/post/2017/11/06/svms/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/06/svms/</guid>
      <description>Cost Function Logistic regression $$ \min_{\theta}\frac{1}{N}\sum_{i=1}^N \left[y^{(i)}\left(-\log \frac{1}{1+e^{-\theta^T x^{(i)}}}\right)+(1-y^{(i)})\left(-\log \frac{e^{-\theta^T x^{(i)}}}{1+e^{-\theta^T x^{(i)}}}\right)\right] +\frac{\lambda}{2N}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\gg 0$ if $y=0$, want $\theta^Tx\ll 0$  SVM $$ \min_{\theta} C\sum_{i=1}^N \left[y^{(i)}cost_1(\theta^Tx^{(i)}) +(1-y^{(i)})cost_0(\theta^T x^{(i)})\right] +\frac{1}{2}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\geq 1$ if $y=0$, want $\theta^Tx\leq -1$
  Large Margin Classification  SVM Decision Boundary  $$ \min_{\theta} \frac{1}{2}\sum_{j=1}^p\theta_j^2\\
s.t. \begin{cases} \theta^Tx^{(i)}\geq 1 &amp;amp;\text{if } y^{(i)}=1\newline \theta^Tx^{(i)}\leq -1 &amp;amp;\text{if } y^{(i)}=0\newline \end{cases} $$ Simplification: $\theta_0=0, p=2$ $$ \min_{\theta} \frac{1}{2}\Vert\theta\Vert_2^2\\</description>
    </item>
    
    <item>
      <title>ML - Machine Learning System Design</title>
      <link>/post/2017/11/05/system-design/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/05/system-design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Neural Networks</title>
      <link>/post/2017/11/04/neural-networks/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/04/neural-networks/</guid>
      <description>Model Representation $$ a_i^{[j]} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$} \\
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} $$
Forward Propagation: Vectorized Implementation $$ [x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots $$
 input: $x$. Setting $a^{[1]}=x$ linear combination: $z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$ activation: $a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$  Non-linear Classification Example: XNOR operator    x1 x2 XNOR = NOT XOR     0 0 1   0 1 0   1 0 0   1 1 1     using a hidden layer with two nodes (sigmoid activation function)  Multiclass Classification Cost Function  $\{x^{(i)},y^{(i)}\}$, $y\in\mathbb{R}^K$</description>
    </item>
    
    <item>
      <title>ML - Logistic Regression</title>
      <link>/post/2017/11/02/logistic/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/02/logistic/</guid>
      <description>Logistic Regression Model  goal: want $h_{\theta}(x) \in [0,1]$ $h_{\theta}(x)=g(\theta^Tx)$, where $g(z)=\frac{1}{1+e^{-z}}$ (sigmoid function/ logistic function) interpretations: $h_{\theta}(x)$ = estimated probability that $y=1$ on input $x$, that is  $$h(x)=h_{\theta}(x) = \Pr(y=1|x;\theta)$$
 prediction: predict $y=1$ if $h_{\theta}(x)\geq 0.5 \Leftrightarrow \theta^Tx\geq 0$
 Decision Boundary: $\theta^Tx= 0$
 Nonlinear Decision Boundary: add complex (i.e. polynomial) terms
 Notations: training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where
  $$ x = \begin{pmatrix}x_0\newline x_1\newline\vdots\newline x_p\end{pmatrix}\in\mathbb{R}^{p+1}, x_0=1,y\in\{0,1\} $$</description>
    </item>
    
    <item>
      <title>ML - Linear Regression with Multiple Variables</title>
      <link>/post/2017/11/01/lr/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/01/lr/</guid>
      <description>Linear Regression with One Variable  $X$: space of input values, $Y$: space of output values
 training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$
 goal: given a training set, to learn a function $h : X \rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. For historical reasons, this function $h$ is called a hypothesis.
 $h(x)=h_{\theta}(x) = \theta_0 + \theta_1 x$
 cost function (squared error function, or mean squared error, MSE):</description>
    </item>
    
    <item>
      <title>ISLR CH10 Unsupervised Learning</title>
      <link>/post/2017/10/10/unsupervised/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/10/unsupervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH9 Support Vector Machines</title>
      <link>/post/2017/10/09/svm/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/09/svm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH8 Tree-Based Methods</title>
      <link>/post/2017/10/08/tree-based-methods/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/08/tree-based-methods/</guid>
      <description>IntuitionModelsRegression TreesPredicting Baseball Players’ SalariesAlgorithmTree PruningReferencesIntuitionRegression/Classification
Stratify or segment the predictor space into simple regions
Prediction for a given observation: mean or mode of the training observations in the region to which it belongs
The set of splitting rules can be summarized in a tree \(\Rightarrow\) Decision Tree Methods
Pros: simple, easy to interpret, and nice to visualize</description>
    </item>
    
    <item>
      <title>ISLR CH7 Moving Beyond Linearity</title>
      <link>/post/2017/10/07/beyond-linearity/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/07/beyond-linearity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH6 Linear Model Selection and Regularization</title>
      <link>/post/2017/10/06/model-selection/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/06/model-selection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH5 Resampling Method</title>
      <link>/post/2017/10/05/resampling/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/05/resampling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH4 Classification</title>
      <link>/post/2017/10/04/classification/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/04/classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH3 Linear Regression</title>
      <link>/post/2017/10/03/linear-regression/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/03/linear-regression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PCA</title>
      <link>/post/2017/07/02/pca/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/07/02/pca/</guid>
      <description>Applications of PCAData Visualizationsummary(iris)## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # Customize the colorssetcols = c(&amp;quot;#00AFBB&amp;quot;, &amp;quot;#E7B800&amp;quot;, &amp;quot;#FC4E07&amp;quot;)# Customize the lower panel: correlation efficientscorrelation.</description>
    </item>
    
  </channel>
</rss>