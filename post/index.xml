<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on NOWHERESVILLE</title>
    <link>/post/</link>
    <description>Recent content in Posts on NOWHERESVILLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>面试想法 - CNN</title>
      <link>/post/2018/04/11/diary411/</link>
      <pubDate>Wed, 11 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/04/11/diary411/</guid>
      <description>今天面试岗位的面试官让我讲一下CNN，就糊里糊涂把最近学的和实践的一点点东西讲了一下，脑海中却有一个疑问，貌似一直都没思考过CNN为什么这么强大？
在吴恩达的深度学习专项课程中，并不是我第一次知道卷积神经网络了，读研时就和同学在讨论班讨论过，模模糊糊知道做了什么数学运算（学数学的毛病，一切都从公式出发，才是舒适区），至于为什么要做convolution？CNN有多强大？为什么这么强大？完全不了解，以为是个很艰深的课题。吴恩达的课程讲的真的很直观透彻，边听边有种感觉：懂了，原来这么简单。然而事后回想，真的懂了吗？
读研时借了一本机器学习图像识别的书，看完了第一章，觉得自己懂了，后面长篇大论的方法根本懒得看：所谓图像识别，就是把图像像素灰度或RGB数据拉长成为一个向量嘛，那跟一般的特征有什么区别？殊途同归，没什么意思。然而心中隐隐知道有种不对劲：真的这么简单粗暴的方法就可以吗？假如按照列拉成向量，普通的机器学习方法真的能够自动发现同一行像素之间的关联吗？能发现在附近的像素之间的关联吗？假如通过神经网络来学习，需要多深的网络才能把握这些复杂的邻近关系呢？正是心中有这些疑问，对这本书有点不以为然。
后来又听了一个图像识别的统计报告，听众寥寥无几，但是我对作者的方法很感兴趣，貌似是照列和行都拉长成两个向量？记不太清，总之作者是试图把握上下左右像素之间的关系，令我很感兴趣，一直念念不忘这个想法（虽然作者自称成果不太好，后续还要改进）。
今日突然联系起来，恍然大悟，这不就是CNN的想法吗？每次的convolution运算，都是学习了图像上一小片邻近像素的特征！心中感到一重震惊：为什么会有人这么聪明，能想出这样的方法；二重震惊：为什么之前没有人想出这样的简单明快的方法；三重震惊：LeNet-5在1995年就提出了，我竟然在2014年开始读研直到2017年毕业都不知道这么强大的方法。
在CNN之前，图像识别都要通过复杂的方法去提取线条等等特征，而CNN可以直接免去痛苦的特征工程，让算法自己去学习图像中从简单、到复杂的特征，在吴恩达的课中，贴出了这篇论文【Visualizing and understanding convolutional networks】，完全直观地感受到CNN的奇妙。
由此想到，对一些高维数据，很多算法都需要先做特征选择或者模型选择，以免特征高度共线性影响算法的表现。然而，高度相关的特征真的不应该共存吗？这难道不是算法的局限，而不是数据的问题吧！假如把这些特征排列成一定的几何结构，比如说和图像一样的矩阵，邻近的都是相关的特征，那么同样可以适用CNN的想法，使用同一个filter和这些特征去相乘。就像课程中提到的：
 Why Convolutions?
 parameter sharing: a feature detector (such as a vertical edge detector) that is useful in one part of the image is probably useful in another part of the image; reduces the total number of parameters, thus reducing overfitting sparsity of connections: in each layer, each output value depends only on a small number of inputs   学完CNN后，我立刻去参加了hackathon的一个图像年龄识别比赛，使用ResNet有点过拟合，于是就稍微改进了一下，加了几层Dropout，又调换了BN和ReLU的顺序，很顺利正确率达到了83%，排到了13名，如果再做ensemble，data augmentation, 10-crop，结果肯定会更好，只是有点犯懒了（对已知结果的事情我都有点犯懒）……并且有点惭愧，只是做了一点微小的工作，剩下的都是电脑自己在算。图像识别有了目前这些强大的CNN算法，提高准确率只是一些调参或者计算机算力的问题了吧，功绩是属于提出这些方法的人，让我再一次感觉：为什么能想到？为什么想到的又不是我？</description>
    </item>
    
    <item>
      <title>Tips - 2018 Apr</title>
      <link>/post/2018/04/10/tips1/</link>
      <pubDate>Tue, 10 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/04/10/tips1/</guid>
      <description>Colaboratory - Download Files from google.colab import files files.download(&#39;filename&#39;)  Conversion Between Class Labels and One Hot # class label to integer label from sklearn import preprocessing le = preprocessing.LabelEncoder() y_factor = le.fit_transform(y_class) # integer label to one hot from keras.utils import np_utils y_onehot = np_utils.to_categorical(y_factor) # keras Sequence() model prediction to class label y_pred_factor = model.predict_classes(Xtest) y_pred_class = le.inverse_transform(y_pred_factor) # keras Model() model prediction to class label y_pred_prob = model.</description>
    </item>
    
    <item>
      <title>Sequence Models</title>
      <link>/post/2018/03/20/rnns/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/20/rnns/</guid>
      <description>Examples of Sequence Data  speech recognition music generation sentiment classification DNA sequence analysis machine translation video activity recognition name entity recognition  Recurrent Neural Networks  Why not a standard network?  inputs and outputs can be different lengths in different examples, e.g. sentence as input does not share features learned across different positions of text   Forward Propagation  at time $t$:  $$ a^{t} = g(W_{aa}a^{t-1} + W_{ax}x^{t} + b_a)\\</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>/post/2018/03/15/cnns/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/15/cnns/</guid>
      <description>Building Blocks of CNN Edge Detection Vertical Edge Detection Vertical and Horizontal Edge Detection Convolution Operation  tensorflow: tf.nn.conv2d keras: conv2D  Padding  $n\times n$ image, convolves with a $f\times f$ filter $\Rightarrow (n-f+1)\times(n-f+1)$ output downside:  shrinking output pixels at the corner is touched only once, information from the edge is thrown away  padding with zeros, $p=$ padding amount output: $(n+2p-f+1)\times (n+2p-f+1)$  Valid and Same Convolutions  valid: no padding same: output size = input size, $2p+1=f$, $f$ is usually odd  Strided Convolution  $n\times n$ image, convolves with a $f\times f$ filter, with padding $p$ and stride $s$  $\Rightarrow$ output size $=\left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)\times \left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)$   Convolution Over Volume  input size: $n\times n\times n_C$ ($n_C = $ # of channels) $n_C&#39;$ filters, each of size: $f\times f\times n_C$ (each has $f\times f\times n_C$ parameters) output size: $(n-f+1)\times (n-f+1)\times n_C&#39;$  One Layer of a Convolutional Network  Notations: layer $\ell$  $f^{[\ell]}$ = filter size $p^{[\ell]}$ = padding $s^{[\ell]}$ = stride $n^{[\ell]}_C$ = # of filters input size: $n^{[\ell-1]}_H\times n^{[\ell-1]}_W\times n^{[\ell-1]}_C$ each filter size: $f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C$ output size: $n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$ , where $$n^{[\ell]}_H=\left\lfloor \frac{n^{[\ell-1]}_H +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor,\\</description>
    </item>
    
    <item>
      <title>CS231n-CNN for Visual Recognition-Assignment1</title>
      <link>/post/2018/03/07/cs231n/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/07/cs231n/</guid>
      <description>Image Classification Challenges  Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning</title>
      <link>/post/2018/02/27/nn/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/27/nn/</guid>
      <description>Introduction What is a Neural Network?  ReLU = Rectified Linear Unit     Input Output Application Model     Home Features Price Real Estate NN   Ad, User info Click on ad? 0/1 Online Advertising NN   Image Object Photo Tagging CNN   Audio Text transcript Speech Recognition RNN   English Chinese Machine Translation RNN   Image, Radar info Position of other cars Autonomous Driving Hybrid     Image - convolutional neural network, CNN sequence data (temporal data, time series) - recurrent neural network, RNN   Structured data: database Unstructured data: audio, image, text  Why is Deep Learning taking off?</description>
    </item>
    
    <item>
      <title>Algorithms-Design and Analysis(Stanford) Notes</title>
      <link>/post/2018/01/25/algorithms/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/25/algorithms/</guid>
      <description>PDF格式笔记见：Notes
Divide and Conquer 分而治之  DIVIDE into smaller sub-problems CONQUER via recursive calls COMBINE solutions of sub-problems into one for the original problem  Master Method  Cool feature: a &amp;ldquo;black-box&amp;rdquo; method for solving recurrences
 Determine the upper bound of running time for most of the D&amp;amp;C algos
 Assumption: all sub-problems have equal size
    unbalanced sub-problems? more than one recurrence?    Recurrence format:</description>
    </item>
    
    <item>
      <title>Python中的取整方式</title>
      <link>/post/2018/01/19/python-float-to-int/</link>
      <pubDate>Fri, 19 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/19/python-float-to-int/</guid>
      <description>方式一：round() 四舍五入 Python中的 round() 有两个参数，第一个参数是需要处理的数，第二个参数是数位精度，默认为0。
round(3.4) ## 3 round(3.5) ## 4  而有时候会出现奇怪的情况，比如：
round(3.24, 1) #是四舍五入 ## 3.2 round(3.26, 1) #是四舍五入 ## 3.3 round(3.25, 1) #不是四舍五入 ## 3.2 ################################### round(0.44, 1) #是四舍五入 ## 0.4 round(0.46, 1) #是四舍五入 ## 0.5 round(0.45, 1) #是四舍五入 ## 0.5  很多人说Python3中采用的是【四舍六入五留双】，上面的例子说明这种说法是不正确的。其实是因为：
 十进制小数在计算机内是通过二进制小数来近似，在舍和进两个选项中选择更接近的一个 而当舍和进的两个选项十分接近时，round 选择偶数的选项  这就导致出现的结果非常复杂了。
进一步解释：十进制小数 $0.2$ 和 $0.3$ 的二进制表示分别为：
$$ \begin{align} (0.2)_{10} &amp;amp; = \left(\frac{1}{8}+\frac{1}{16}\right)+\left(\frac{1}{128}+\frac{1}{256}\right)+\cdots =\frac{\frac{1}{8}+\frac{1}{16}}{1-\frac{1}{16}} =\frac{3}{15}=\frac{1}{5}\newline &amp;amp;=(0.\dot{0}\dot{0}\dot{1}\dot{1})_2 \end{align} $$
以及 $$ \begin{align} (0.</description>
    </item>
    
    <item>
      <title>Searching 搜索算法</title>
      <link>/post/2018/01/01/searching/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/searching/</guid>
      <description>Linear Search Search an Unsorted List def linearSearch(arr, target): for i in range(len(arr)): if arr[i] == target: return i return None  Analysis of running time:
 worst-case: $O(n)$ best-case: $O(1)$  More efficient linear search:
 put a sentinel in the list to avoid checking i&amp;lt;len(arr) each time
 faster in practice, but only by a constant factor
  def linearSearch2(arr, target): last, arr[-1] = arr[-1], target i = 0 while arr[i] !</description>
    </item>
    
    <item>
      <title>Sorting 排序算法</title>
      <link>/post/2018/01/01/sorting/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/01/01/sorting/</guid>
      <description>Visualization of Sorting Algorithms https://www.cs.usfca.edu/~galles/visualization/ComparisonSort.html
https://visualgo.net/bn/sorting
https://www.toptal.com/developers/sorting-algorithms
References  MIT, 6.006, Introduction to Algorithms, LECTURE 3 CLRS , Chapter 2  Summary     Best-Case $T(n)$ Worst-Case $T(n)$ Average-Case $T(n)$? Space Complexity     Bubble Sort $O(n)$ $O(n^2)$ $O(n^2)$ $O(1)$   Selection Sort $O(n^2)$ $O(n^2)$ $O(n^2)$ $O(1)$   Insertion Sort $O(n)$ $O(n^2)$ $O(n^2)$ $O(1)$   Merge Sort $O(n\log n)$ $O(n\log n)$ $O(n\log n)$ $O(n)$   Quick Sort $O(n\log n)$ $O(n^2)$ $O(n\log n)$ $O(1)$    Can We Sort Faster?</description>
    </item>
    
    <item>
      <title>ML - Support Vector Machines</title>
      <link>/post/2017/11/06/svms/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/06/svms/</guid>
      <description>Cost Function Logistic regression $$ \min_{\theta}\frac{1}{N}\sum_{i=1}^N \left[y^{(i)}\left(-\log \frac{1}{1+e^{-\theta^T x^{(i)}}}\right)+(1-y^{(i)})\left(-\log \frac{e^{-\theta^T x^{(i)}}}{1+e^{-\theta^T x^{(i)}}}\right)\right] +\frac{\lambda}{2N}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\gg 0$ if $y=0$, want $\theta^Tx\ll 0$  SVM $$ \min_{\theta} C\sum_{i=1}^N \left[y^{(i)}cost_1(\theta^Tx^{(i)}) +(1-y^{(i)})cost_0(\theta^T x^{(i)})\right] +\frac{1}{2}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\geq 1$ if $y=0$, want $\theta^Tx\leq -1$
  Large Margin Classification  SVM Decision Boundary  $$ \min_{\theta} \frac{1}{2}\sum_{j=1}^p\theta_j^2\\
s.t. \begin{cases} \theta^Tx^{(i)}\geq 1 &amp;amp;\text{if } y^{(i)}=1\newline \theta^Tx^{(i)}\leq -1 &amp;amp;\text{if } y^{(i)}=0\newline \end{cases} $$ Simplification: $\theta_0=0, p=2$ $$ \min_{\theta} \frac{1}{2}\Vert\theta\Vert_2^2\\</description>
    </item>
    
    <item>
      <title>ML - Machine Learning System Design</title>
      <link>/post/2017/11/05/system-design/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/05/system-design/</guid>
      <description> How to Improve Learning Algorithms  get more training examples feature selection get additional features add polynomial features decrease/increase $\lambda$  Evaluate a Learning Algorithm  training/validation/test set, e.g. 60/20/20 split training/validation/test error model selection:  optimize parameters by minimizing training error for each model select the model with the least validation error (e.g. select polynomial degree $d$)  estimate generalization error using test error  Machine Learning Diagnostic: Bias vs Variance  can rule out certain courses of action as being unlikely to improve the performance of your learning algorithm significantly high bias = underfit = high training error, validation error $\approx$ training error high variance = overfit = low training error, validation error $\gg$ training error  Regularization and Bias/Variance  large $\lambda$ $\Rightarrow$ high bias = underfit small $\lambda$ $\Rightarrow$ high variance = overfit choose regularization parameter  create a list of $\lambda$s create a list of models iterate through the $\lambda$s and for each $\lambda$ go through all the models to optimize parameter $\Theta$ compute validation error using the learned $\Theta$ select the best combo with the least validation error estimate generalization error using test error   Learning Curves  as the training set gets larger, the training error increases error value will plateau out after a certain training set size Experiencing high bias:  Low training set size: low training error, high validation error Large training set size: high training error, validation error $\approx$ training error getting more training data will not (by itself) help much    Experiencing high variance:  Low training set size: low training error, high validation error Large training set size: training error increases with training set size, validation error continues to decrease without leveling off; difference between 2 errors remains significant getting more training data is likely to help   Debugging a Learning Algorithm  get more training examples $\rightarrow$ fit high variance feature selection $\rightarrow$ fit high variance get additional features $\rightarrow$ fit high bias add polynomial features $\rightarrow$ fit high bias increase $\lambda$ $\rightarrow$ fit high variance decrease $\lambda$ $\rightarrow$ fit high bias  Neural Networks and Overfitting  small nn  fewer parameters more prone to underfitting computationally cheaper  larger nn  more parameters more prone to overfitting computationally expensive use regularization to address overfitting   </description>
    </item>
    
    <item>
      <title>ML - Neural Networks</title>
      <link>/post/2017/11/04/neural-networks/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/04/neural-networks/</guid>
      <description>Model Representation $$ a_i^{[j]} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$} \\
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} $$
Forward Propagation: Vectorized Implementation $$ [x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots $$
 input: $x$. Setting $a^{[1]}=x$ linear combination: $z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$ activation: $a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$  Non-linear Classification Example: XNOR operator    x1 x2 XNOR = NOT XOR     0 0 1   0 1 0   1 0 0   1 1 1     using a hidden layer with two nodes (sigmoid activation function)  Multiclass Classification Cost Function  $\{x^{(i)},y^{(i)}\}$, $y\in\mathbb{R}^K$</description>
    </item>
    
    <item>
      <title>ML - Logistic Regression</title>
      <link>/post/2017/11/02/logistic/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/02/logistic/</guid>
      <description>Logistic Regression Model  goal: want $h_{\theta}(x) \in [0,1]$ $h_{\theta}(x)=g(\theta^Tx)$, where $g(z)=\frac{1}{1+e^{-z}}$ (sigmoid function/ logistic function) interpretations: $h_{\theta}(x)$ = estimated probability that $y=1$ on input $x$, that is  $$h(x)=h_{\theta}(x) = \Pr(y=1|x;\theta)$$
 prediction: predict $y=1$ if $h_{\theta}(x)\geq 0.5 \Leftrightarrow \theta^Tx\geq 0$
 Decision Boundary: $\theta^Tx= 0$
 Nonlinear Decision Boundary: add complex (i.e. polynomial) terms
 Notations: training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where
  $$ x = \begin{pmatrix}x_0\newline x_1\newline\vdots\newline x_p\end{pmatrix}\in\mathbb{R}^{p+1}, x_0=1,y\in\{0,1\} $$</description>
    </item>
    
    <item>
      <title>ML - Linear Regression with Multiple Variables</title>
      <link>/post/2017/11/01/lr/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/01/lr/</guid>
      <description>Linear Regression with One Variable  $X$: space of input values, $Y$: space of output values
 training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$
 goal: given a training set, to learn a function $h : X \rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. For historical reasons, this function $h$ is called a hypothesis.
 $h(x)=h_{\theta}(x) = \theta_0 + \theta_1 x$
 cost function (squared error function, or mean squared error, MSE):</description>
    </item>
    
  </channel>
</rss>