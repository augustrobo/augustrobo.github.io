<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ML - Neural Networks | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">ML - Neural Networks</span></h1>

<h2 class="date">2017/11/04</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/python">Python</a> <a href="/categories/machinelearning">machineLearning</a> 
  
  
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#model-representation">Model Representation</a>
<ul>
<li><a href="#forward-propagation-vectorized-implementation">Forward Propagation: Vectorized Implementation</a></li>
<li><a href="#non-linear-classification-example-xnor-operator">Non-linear Classification Example: XNOR operator</a></li>
<li><a href="#multiclass-classification">Multiclass Classification</a></li>
</ul></li>
<li><a href="#cost-function">Cost Function</a></li>
<li><a href="#back-propagation">Back Propagation</a></li>
</ul>
</nav>


<main>


<h1 id="model-representation">Model Representation</h1>

<p><img src="/stanfordML/nn.png" alt="Neural Network" /></p>

<p>$$
a_i^{[j]} = \text{&ldquo;activation&rdquo; of unit $i$ in layer $j$} \\<br />
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}
$$</p>

<hr />

<h2 id="forward-propagation-vectorized-implementation">Forward Propagation: Vectorized Implementation</h2>

<p>$$
[x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots
$$</p>

<ul>
<li>input: <code>$x$</code>. Setting <code>$a^{[1]}=x$</code></li>
<li>linear combination: <code>$z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$</code></li>
<li>activation: <code>$a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$</code></li>
</ul>

<hr />

<h2 id="non-linear-classification-example-xnor-operator">Non-linear Classification Example: XNOR operator</h2>

<table>
<thead>
<tr>
<th align="center">x1</th>
<th align="center">x2</th>
<th align="center">XNOR = NOT XOR</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>

<ul>
<li>using a hidden layer with two nodes (sigmoid activation function)</li>
</ul>

<p><img src="/stanfordML/xnor.png" alt="XNOR" /></p>

<h2 id="multiclass-classification">Multiclass Classification</h2>

<p><img src="/stanfordML/multiclass.png" alt="Multiclass Classification" /></p>

<h1 id="cost-function">Cost Function</h1>

<ul>
<li><p><code>$\{x^{(i)},y^{(i)}\}$</code>, <code>$y\in\mathbb{R}^K$</code></p></li>

<li><p><code>$L$</code>: total # of layers in network</p></li>

<li><p><code>$s_{\ell}$</code>: # of neurons in layer <code>$\ell$</code></p></li>

<li><p>cost function (generalization of logistic regression cost function)</p></li>
</ul>

<p>$$
h_{\Theta}(x)\in\mathbb{R}^K, h_{\Theta}(x)_i = i\text{th output}
$$</p>

<p>$$
J(\Theta)=-\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^K\left[
y_{k}^{(i)}\log h_{\Theta}(x^{(i)})_k +
(1-y_{k}^{(i)})\log (1-h_{\Theta}(x^{(i)})_k)
\right]+
\frac{\lambda}{2N}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$</p>

<h1 id="back-propagation">Back Propagation</h1>

<p>min <code>$J(\Theta)$</code>, need to compute</p>

<ul>
<li><code>$J(\Theta)$</code></li>
<li><code>$\frac{\partial}{\partial\Theta}J(\Theta)$</code></li>
</ul>

<p>Given one training example <code>$(x,y)$</code>ï¼š</p>

<p><strong><u>Forward Propagation</u></strong>
$$
\begin{aligned}
a^{[1]}&amp;=x\newline
z^{[\ell]}&amp;=\Theta^{[\ell-1]}a^{[\ell-1]},\ a^{[\ell]}=g(z^{[\ell]}) \ (\text{add } a^{[\ell]}_0), L=2,3,\ldots,L-1\newline
z^{[L]}&amp;=\Theta^{[L-1]}a^{[L-1]},\ a^{[L]}=h_{\Theta}(x)=g(z^{[L]})
\end{aligned}
$$
<strong><u>Backward Propagation</u></strong></p>

<ul>
<li><code>$\delta_{j}^{[\ell]}=$</code> &ldquo;error&rdquo; of node <code>$j$</code> in layer <code>$\ell$</code></li>
<li>formally, <code>$\delta_{j}^{[\ell]}=\frac{\partial}{\partial z_j^{[\ell]}}\text{cost}$</code>, where <code>$\text{cost}=-y\log h_{\Theta}(x) -(1-y)\log(1-h_{\Theta}(x))$</code></li>
<li>vectorized:</li>
</ul>

<p>$$
\begin{aligned}
\delta^{[L]}&amp;=a^{[L]}-y\newline
\delta^{[\ell]}&amp;=(\Theta^{[\ell]})^T\delta^{[\ell+1]}*g&rsquo;(z^{[\ell]}), \ \ \ell=L-1,L-2,\ldots,2\newline
g&rsquo;(z^{[\ell]})&amp;=a^{[\ell]}*(1-a^{[\ell]})
\end{aligned}
$$</p>

<p>$$
\Rightarrow \frac{\partial}{\partial\Theta_{ji}^{[\ell]}}J(\Theta)=a_j^{[\ell]}\delta_i^{[\ell+1]}\ \  (\text{if }\lambda=0)
$$</p>

<ul>
<li>with training set <code>$\{(x^{(i)}, y^{(i)})\}_{i=1}^N$</code>. Set <code>$\Delta_{ij}^{[\ell]}=0$</code>.</li>
</ul>

<blockquote>
<ul>
<li>For training example from <code>$i=1$</code> to <code>$N$</code>:

<ol>
<li>Set <code>$a^{(1)}=x^{(i)}$</code></li>
<li>Perform forward propagation to compute <code>$a^{[\ell]}, \ell=2,3,\ldots,L$</code></li>
<li>Compute <code>$\delta^{[L]}=a^{[L]}-y^{(i)}$</code></li>
<li>Compute <code>$\delta^{[\ell]}, \ell=L-1,L-2,\ldots,2$</code> using <code>$\delta^{[\ell]}=(\Theta^{[\ell]})^T\delta^{[\ell+1]}*a^{[\ell]}(1-a^{[\ell]})$</code> (<code>$*$</code> elementwise multiplication)</li>
</ol></li>
<li><code>$\Delta^{[l]} := \Delta^{[l]} + \delta^{[l+1]}(a^{[l]})^T$</code></li>
<li><code>$D^{[\ell]}=\begin{cases}\frac{1}{N}(\Delta^{[\ell]}+\lambda\Theta^{[\ell]}),&amp;j\neq 0\newline \frac{1}{N}\Delta^{[\ell]}, &amp; j=0\end{cases}$</code></li>
</ul>
</blockquote>

<p>$$
\Rightarrow \frac \partial {\partial \Theta_{ij}^{[l]}} J(\Theta)=D_{ij}^{[l]}
$$</p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

