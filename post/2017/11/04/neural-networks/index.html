<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ML - Neural Networks | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">ML - Neural Networks</span></h1>

<h2 class="date">2017/11/04</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/python">Python</a> <a href="/categories/machinelearning">machineLearning</a> 
  
  
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#model-representation">Model Representation</a>
<ul>
<li><a href="#forward-propagation-vectorized-implementation">Forward Propagation: Vectorized Implementation</a></li>
<li><a href="#non-linear-classification-example-xnor-operator">Non-linear Classification Example: XNOR operator</a></li>
<li><a href="#multiclass-classification">Multiclass Classification</a></li>
</ul></li>
<li><a href="#cost-function">Cost Function</a></li>
<li><a href="#back-propagation">Back Propagation</a></li>
<li><a href="#nn-for-handwritten-digits-recognition-python-code">NN for Handwritten Digits Recognition - Python Code</a></li>
</ul>
</nav>


<main>


<h1 id="model-representation">Model Representation</h1>

<p><img src="/stanfordML/nn.png" alt="Neural Network" /></p>

<p>$$
a_i^{[j]} = \text{&ldquo;activation&rdquo; of unit $i$ in layer $j$} \\<br />
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}
$$</p>

<hr />

<h2 id="forward-propagation-vectorized-implementation">Forward Propagation: Vectorized Implementation</h2>

<p>$$
[x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots
$$</p>

<ul>
<li>input: <code>$x$</code>. Setting <code>$a^{[1]}=x$</code></li>
<li>linear combination: <code>$z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$</code></li>
<li>activation: <code>$a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$</code></li>
</ul>

<hr />

<h2 id="non-linear-classification-example-xnor-operator">Non-linear Classification Example: XNOR operator</h2>

<table>
<thead>
<tr>
<th align="center">x1</th>
<th align="center">x2</th>
<th align="center">XNOR = NOT XOR</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">0</td>
<td align="center">0</td>
<td align="center">1</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">1</td>
<td align="center">0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">0</td>
<td align="center">0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1</td>
<td align="center">1</td>
</tr>
</tbody>
</table>

<ul>
<li>using a hidden layer with two nodes (sigmoid activation function)</li>
</ul>

<p><img src="/stanfordML/xnor.png" alt="XNOR" /></p>

<h2 id="multiclass-classification">Multiclass Classification</h2>

<p><img src="/stanfordML/multiclass.png" alt="Multiclass Classification" /></p>

<h1 id="cost-function">Cost Function</h1>

<ul>
<li><p><code>$\{x^{(i)},y^{(i)}\}$</code>, <code>$y\in\mathbb{R}^K$</code></p></li>

<li><p><code>$L$</code>: total # of layers in network</p></li>

<li><p><code>$s_{\ell}$</code>: # of neurons in layer <code>$\ell$</code></p></li>

<li><p>cost function (generalization of logistic regression cost function)</p></li>
</ul>

<p>$$
h_{\Theta}(x)\in\mathbb{R}^K, h_{\Theta}(x)_i = i\text{th output}
$$</p>

<p>$$
J(\Theta)=-\frac{1}{N}\sum_{i=1}^N\sum_{k=1}^K\left[
y_{k}^{(i)}\log h_{\Theta}(x^{(i)})_k +
(1-y_{k}^{(i)})\log (1-h_{\Theta}(x^{(i)})_k)
\right]+
\frac{\lambda}{2N}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$</p>

<h1 id="back-propagation">Back Propagation</h1>

<p>min <code>$J(\Theta)$</code>, need to compute</p>

<ul>
<li><code>$J(\Theta)$</code></li>
<li><code>$\frac{\partial}{\partial\Theta}J(\Theta)$</code></li>
</ul>

<p>Given one training example <code>$(x,y)$</code>ï¼š</p>

<p><strong><u>Forward Propagation</u></strong></p>

<blockquote>
<p><strong>Single training example</strong>:
$$
\begin{aligned}
a^{[1]}&amp;=x  \ (\text{add } a^{[1]}_0)\newline
z^{[\ell]}&amp;=\Theta^{[\ell-1]}a^{[\ell-1]},\ a^{[\ell]}=g(z^{[\ell]}) \ (\text{add } a^{[\ell]}_0), L=2,3,\ldots,L-1\newline
z^{[L]}&amp;=\Theta^{[L-1]}a^{[L-1]},\ a^{[L]}=h_{\Theta}(x)=\sigma(z^{[L]})
\end{aligned}
$$</p>

<hr />

<p><code>$N$</code> <strong>training examples</strong>:
$$
X = \begin{pmatrix}
x^{(1)T}\newline
x^{(2)T}\newline
\vdots\newline
x^{(N)T}
\end{pmatrix}
$$</p>

<p>$$
\begin{aligned}
A^{[1]}&amp;=\left[\textbf{1}\ X\right] \newline
Z^{[\ell]}&amp;=A^{[\ell-1]}\Theta^{[\ell-1]T},\ A^{[\ell]}=\left[\textbf{1}\ g(Z^{[\ell]})\right] , L=2,3,\ldots,L-1\newline
Z^{[L]}&amp;=A^{[L-1]}\Theta^{[L-1]T},\ A^{[L]}=h_{\Theta}(X)=\sigma(Z^{[L]})
\end{aligned}
$$</p>
</blockquote>

<p><strong><u>Backward Propagation</u></strong></p>

<blockquote>
<p><strong>Single training example</strong>:</p>

<ul>
<li><code>$\delta_{j}^{[\ell]}=$</code> &ldquo;error&rdquo; of node <code>$j$</code> in layer <code>$\ell = \frac{\partial J}{\partial z_j^{[\ell]}}$</code></li>
<li><code>$\delta^{[L]}=a^{[L]}-y$</code></li>
</ul>

<p>$$
\begin{aligned}
J &amp;= -y^T\log(a^{[L]}) - (1-y)^T\log (1-a^{[L]})\newline
\Rightarrow \frac{\partial J}{\partial a^{[L]}_j} &amp;=-\frac{y_j}{a^{[L]}_j} - \frac{1-y_j}{1-a^{[L]}_j}\newline
\Rightarrow \frac{\partial J}{\partial z_j^{[\ell]}} &amp;=  \frac{\partial J}{\partial a^{[L]}_j}\frac{d a^{[L]}_j}{dz^{[L]}_j}=
\left(-\frac{y_j}{a^{[L]}_j} - \frac{1-y_j}{1-a^{[L]}_j}
\right)a^{[L]}_j(1-a^{[L]}_j)=a^{[L]}_j-y_j\newline
\Rightarrow \frac{\partial J}{\partial z^{[L]}}&amp;=a^{[L]}-y
\end{aligned}
$$</p>

<ul>
<li><code>$\delta^{[\ell]}=(\tilde{\Theta}^{[\ell]})^T\delta^{[\ell+1]}*g'(z^{[\ell]}), \ \ \ell=L-1,\ldots,2$</code> and <code>$\tilde{\Theta}=\Theta$</code> without the first column</li>
</ul>

<p>$$
\begin{aligned}
\frac{\partial J}{\partial a^{[\ell]}_{j}}&amp;=\sum_{i=1}^{s_{\ell+1}}
\frac{\partial J}{\partial z^{[\ell+1]}_{i}}
\frac{\partial z^{[\ell+1]}_{i}}{\partial a^{[\ell]}_{j}}\newline
&amp;=\sum_{i=1}^{s_{\ell+1}}\delta^{[\ell+1]}_i\Theta_{ij}^{[\ell]}\newline
&amp;=\Theta^{[\ell]T}_j\delta^{[\ell+1]}\text{   , where }\Theta^{[\ell]}_j\text{ is the jth column of }\Theta^{[\ell]}
\end{aligned}
$$</p>

<p>$$
\begin{aligned}
&amp;\frac{\partial J}{\partial z^{[\ell]}_j} = \frac{\partial J}{\partial a^{[\ell]}_j}\frac{\partial a^{[\ell]}_j}{z^{[\ell]}_j}
=\Theta^{[\ell]T}_j\delta^{[\ell+1]} g&rsquo;(z^{[\ell]})_j \newline
\Rightarrow &amp;\frac{\partial J}{\partial z^{[\ell]}}=\tilde{\Theta}^{[\ell]T}\delta^{[\ell+1]}*g&rsquo;(z^{[\ell]})
\end{aligned}
$$</p>

<ul>
<li>unregularized case: <code>$ \frac{\partial J}{\partial\Theta^{[\ell]}}=\delta^{[\ell+1]}(a^{[\ell]})^T$</code></li>
</ul>

<p>$$
\begin{aligned}
\frac{\partial J}{\partial \Theta^{[\ell]}_{ij}}&amp;=\frac{\partial J}{\partial z^{[\ell+1]}_i}\frac{\partial z^{[\ell+1]}_i}{\partial \Theta^{[\ell]}_{ij}}\newline
&amp;=\delta^{[\ell+1]}_i a^{[\ell]}_j
\end{aligned}
$$</p>

<p>$$
\Rightarrow \frac{\partial J}{\partial\Theta^{[\ell]}}=\delta^{[\ell+1]}a^{[\ell]T}
$$</p>

<hr />

<p><code>$N$</code> <strong>training examples</strong>:</p>

<ul>
<li><p><code>$\Delta^{[L]}=A^{[L]}-Y$</code></p></li>

<li><p><code>$\Delta^{[\ell]}=\Delta^{[\ell+1]}\tilde{\Theta}^{[\ell]}*g'(Z^{[\ell]}), \ \ \ell=L-1,\ldots,2$</code></p></li>

<li><p>Gradient of <code>$\Theta^{[\ell]}$</code>:</p></li>
</ul>

<p>$$
 D^{[\ell]}=\frac{1}{N}\Delta^{[\ell+1]T}A^{[\ell]}+\frac{\lambda}{N}\hat{\Theta}^{[\ell]},\text{ where } \hat{\Theta}^{[\ell]}=\Theta^{[\ell]}  \text{ with first column being all 0}
$$</p>
</blockquote>

<h1 id="nn-for-handwritten-digits-recognition-python-code">NN for Handwritten Digits Recognition - Python Code</h1>

<p><a href="https://github.com/augustrobo/machine-learning/blob/master/04-Neural-Networks/Neural%20Network%20for%20Handwritten%20Digits%20Recognition.ipynb">jupyter notebook</a></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

