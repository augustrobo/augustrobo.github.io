<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Neural Networks and Deep Learning | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Neural Networks and Deep Learning</span></h1>

<h2 class="date">2018/02/27</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> 
  
  
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a></li>
</ul></li>
<li><a href="#logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</a>
<ul>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#explanation-of-logistic-regression-cost-function">Explanation of logistic regression cost function</a></li>
<li><a href="#code-and-notes">Code and Notes</a></li>
</ul></li>
<li><a href="#shallow-neural-network">Shallow Neural Network</a></li>
</ul>
</nav>


<main>


<h1 id="introduction">Introduction</h1>

<h2 id="what-is-a-neural-network">What is a Neural Network?</h2>

<ul>
<li><strong>ReLU</strong> = Rectified Linear Unit</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Input</th>
<th align="center">Output</th>
<th align="center">Application</th>
<th align="center">Model</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Home Features</td>
<td align="center">Price</td>
<td align="center">Real Estate</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Ad, User info</td>
<td align="center">Click on ad? 0/1</td>
<td align="center">Online Advertising</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Image</td>
<td align="center">Object</td>
<td align="center">Photo Tagging</td>
<td align="center">CNN</td>
</tr>

<tr>
<td align="center">Audio</td>
<td align="center">Text transcript</td>
<td align="center">Speech Recognition</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">English</td>
<td align="center">Chinese</td>
<td align="center">Machine Translation</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">Image, Radar info</td>
<td align="center">Position of other cars</td>
<td align="center">Autonomous Driving</td>
<td align="center">Hybrid</td>
</tr>
</tbody>
</table>

<ul>
<li>Image - convolutional neural network, <strong>CNN</strong></li>
<li>sequence data (temporal data, time series) - recurrent neural network, <strong>RNN</strong></li>
</ul>

<hr />

<ul>
<li><strong>Structured data</strong>: database</li>
<li><strong>Unstructured data</strong>: audio, image, text</li>
</ul>

<hr />

<h2 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h2>

<p>Being able to train a big enough neural network:</p>

<ul>
<li><strong>Data</strong>: huge amount of labeled data</li>
<li><strong>Computation</strong></li>
<li><strong>Algorithms</strong>:

<ul>
<li>try to make NNs run faster</li>
<li>e.g. switching from sigmoid to ReLU makes gradient descent algorithm run much faster</li>
</ul></li>
</ul>

<p><img src="/NN/scale.PNG" alt="scale" /></p>

<hr />

<h1 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h1>

<ul>
<li>binary classification problem: <code>$(x,y), x\in \mathbb{R}^{n_x}, y\in\{0,1\}$</code></li>
<li><code>$m$</code> training examples: <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code></li>
</ul>

<p>$$
X = \begin{pmatrix}
x^{(1)} &amp;x^{(2)} &amp;\cdots &amp;x^{(m)}
\end{pmatrix} \in \mathbb{R}^{n_x \times m}
$$</p>

<p>$$
y= \begin{pmatrix}
y^{(1)} &amp;y^{(2)} &amp;\cdots &amp;y^{(m)}
\end{pmatrix} \in \mathbb{R}^{1 \times m}
$$</p>

<ul>
<li>logistic regression:</li>
</ul>

<blockquote>
<p>Given <code>$x\in\mathbb{R}^{n_x}$</code>, want <code>$\hat{y}=\Pr(y=1|x)$</code></p>

<p>Output: <code>$\hat{y}=\sigma(\omega^Tx+b)$</code>, where sigmoid function <code>$\sigma(z)=\frac{1}{1+e^{-z}}$</code></p>

<ul>
<li>if <code>$z$</code> is large, <code>$\sigma(z)\approx 1$</code></li>
<li>if <code>$z$</code> is large negative number, <code>$\sigma(z)\approx 0$</code></li>
<li><code>$\sigma(0)=0.5$</code></li>
</ul>
</blockquote>

<ul>
<li>cost function:</li>
</ul>

<blockquote>
<p>Given <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code>, want <code>$\hat{y}^{(i)}=\sigma(\omega^Tx^{(i)}+b)\approx y^{(i)}$</code></p>

<p>loss(error) function: <code>$L(\hat{y},y)=-\left[y\log \hat{y}+(1-y)\log(1-\hat{y})\right]$</code></p>

<p>cost function: <code>$J(\omega,b)=\frac{1}{m}\sum\limits_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}\log \hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right]$</code></p>
</blockquote>

<ul>
<li>gradient descent:</li>
</ul>

<blockquote>
<ul>
<li><code>$J(\omega,b)$</code> is <strong>convex</strong>!</li>
<li>algorithm:</li>
</ul>

<p>repeat{</p>

<p><code>$\omega\leftarrow \omega - \alpha\frac{\partial}{\partial\omega} J(\omega,b)$</code></p>

<p><code>$b\leftarrow b - \alpha\frac{\partial}{\partial b} J(\omega,b)$</code></p>

<p>}</p>
</blockquote>

<ul>
<li>computation graph:

<ul>
<li><strong>forward propagation</strong>: compute current loss</li>
<li><strong>back propagation</strong>: compute current gradient</li>
</ul></li>
<li>logistic regression gradient descent: update parameters</li>
</ul>

<p><strong>One training example</strong>:</p>

<blockquote>
<p><code>$z= \omega^Tx+b$</code></p>

<p><code>$\hat{y} = a = \sigma(z)$</code></p>

<p><code>$L(a,y)=-(y\log a+(1-y)\log(1-a))$</code></p>

<hr />

<p><code>da</code> := <code>$\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$</code></p>

<p><code>dz</code> := <code>$\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz}=\left[-\frac{y}{a}+\frac{1-y}{1-a}\right]a(1-a)=a-y$</code></p>

<p><code>dwi</code> := <code>$\frac{\partial L}{\partial \omega_i}=x_i\frac{dL}{dz}$</code> = <code>xi*dz</code></p>

<p><code>db</code> := <code>$\frac{\partial L}{\partial b}=\frac{dL}{dz}$</code> = <code>dz</code></p>

<p><code>$\omega_i\leftarrow \omega_i - \alpha$</code> <code>dwi</code></p>

<p><code>$b\leftarrow b - \alpha$</code> <code>db</code></p>
</blockquote>

<p><code>$m$</code> <strong>training examples</strong>:</p>

<blockquote>
<p><code>$z^{(i)}= \omega^Tx^{(i)}+b$</code></p>

<p><code>$\hat{y}^{(i)} = a^{(i)} = \sigma(z^{(i)})$</code></p>

<p><code>$L(a^{(i)},y^{(i)})=-(y^{(i)}\log a^{(i)}+(1-y^{(i)})\log(1-a^{(i)}))$</code></p>

<p><code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^mL(a^{(i)},y^{(i)})$</code></p>

<hr />

<p><code>$\frac{\partial J}{\partial\omega_i} = \frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial\omega_i}L(a^{(i)},y^{(i)})$</code></p>
</blockquote>

<p>2 <code>for</code> loops: loop over all entries (<code>$m$</code>), loop over all features  (<code>$n$</code>)</p>

<p><code>$\Rightarrow$</code> vectorization, more efficient!</p>

<p><img src="/NN/logistic.PNG" alt="scale" /></p>

<hr />

<h2 id="vectorization">Vectorization</h2>

<ul>
<li>Whenever possible, avoid explicit for-loops.</li>
</ul>

<pre><code class="language-python">def sigmoid(u):
    return 1/(1 + np.exp(-u))

# X: data, n*m, n = NO. of features, m = NO. of examples
# Y: labels, 1*m
# w: weights, n*1
# b: bias, scalar

# compute activation
A = sigmoid(np.dot(w.T, X) + b)

# cost
J = - np.mean(np.log(A) * Y +  np.log(1-A) *(1-Y))

# back propagation (compute gradient)
m = X.shape[1]
dZ = A - Y
db = np.mean(dZ)
dw = np.dot(X, dZ.T)/m

# update params
w -= learning_rate*dw
b -= learning_rate*db
</code></pre>

<h2 id="explanation-of-logistic-regression-cost-function">Explanation of logistic regression cost function</h2>

<p>$$
y|x \sim Binom(1, \hat{y}), \hat{y} =\sigma(\omega^Tx+b)
$$</p>

<p>$$
\Rightarrow \Pr(y|x)=\hat{y}^y(1-\hat{y})^{1-y}=\begin{cases}
1-\hat{y}, &amp;y=0\newline
\hat{y}, &amp;y=1
\end{cases}
$$</p>

<p>$$
\Rightarrow \log\Pr(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})=-L(\hat{y},y)
$$</p>

<p><strong>Goal</strong>: maximize <code>$\Pr(y|x)$</code> <code>$\Leftrightarrow$</code> minimize <code>$L(\hat{y},y)$</code></p>

<hr />

<p>Cost on <code>$m$</code> examples:</p>

<p>maximize <code>$\Pr(\text{labels in training set}) = \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> maximize <code>$\log  \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})=\sum_{i=1}^m\log\Pr(y^{(i)}|x^{(i)})=-\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> minimize <code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})$</code></p>

<hr />

<h2 id="code-and-notes">Code and Notes</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/01-Logistic-Regression-with-a-Neural-Network-mindset/LogisticRegressionwNN.ipynb">Jupyter Notebook</a></p>

<h1 id="shallow-neural-network">Shallow Neural Network</h1>

<ul>
<li>Neural Network representation: $k$th layer, with activation function <code>$g^{[k]}$</code></li>
</ul>

<p>$$
A^{[0]} = X
$$</p>

<p>$$
Z^{[k]} = W^{[k]}A^{[k-1]} + b^{[k]}, \ \ A[k]=g^{[k]}(Z^{[k]}), \ \ k=1,2,\ldots
$$</p>

<ul>
<li><p><strong>activation function</strong> can be <em>sigmoid</em>, <em>tanh</em>, ReLU, Leaky ReLU&hellip;</p>

<ul>
<li><code>$tanh(z) = \frac{e^{-z}-e^z}{e^{-z}+e^z}$</code></li>
<li><code>$ReLU(z)=\max(0, z)$</code></li>
<li><code>$Leaky ReLU(z)=\max(0.01z, z)$</code></li>
<li>for hidden layers, <em>tanh</em> always works better than <em>sigmoid</em> - zero mean, like data centering</li>
<li>for output layer of binary classification (0/1), <em>sigmoid</em> may be better</li>
<li>different layers can have different activation functions</li>
<li>ReLU is increasingly the default choice (will learn faster)</li>
</ul></li>

<li><p>derivatives of activation functions:</p></li>
</ul>

<table>
<thead>
<tr>
<th align="center">activation function</th>
<th align="center">derivative</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">sigmoid</td>
<td align="center"><code>$g(z)(1-g(z))$</code></td>
</tr>

<tr>
<td align="center">tanh</td>
<td align="center"><code>$1-(g(z))^2$</code></td>
</tr>

<tr>
<td align="center">ReLU</td>
<td align="center"><code>$g'(z)=\begin{cases}0, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>

<tr>
<td align="center">Leaky ReLU</td>
<td align="center"><code>$g'(z)=\begin{cases}0.01, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>
</tbody>
</table>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

