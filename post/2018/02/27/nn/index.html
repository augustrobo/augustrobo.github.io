<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Neural Networks and Deep Learning | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Neural Networks and Deep Learning</span></h1>

<h2 class="date">2018/02/27</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> 
  
  
  
  Tags: <a href="/tags/logisticregression">logisticRegression</a> <a href="/tags/imageclassification">imageClassification</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a></li>
</ul></li>
<li><a href="#logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</a>
<ul>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</a></li>
<li><a href="#image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</a></li>
</ul></li>
<li><a href="#shallow-neural-network">Shallow Neural Network</a>
<ul>
<li><a href="#forward-propagation">Forward Propagation</a></li>
<li><a href="#activation-function">Activation Function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
<li><a href="#planar-data-classification-python-code">Planar Data Classification - Python Code</a></li>
</ul></li>
</ul>
</nav>


<main>


<h1 id="introduction">Introduction</h1>

<h2 id="what-is-a-neural-network">What is a Neural Network?</h2>

<ul>
<li><strong>ReLU</strong> = Rectified Linear Unit</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Input</th>
<th align="center">Output</th>
<th align="center">Application</th>
<th align="center">Model</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Home Features</td>
<td align="center">Price</td>
<td align="center">Real Estate</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Ad, User info</td>
<td align="center">Click on ad? 0/1</td>
<td align="center">Online Advertising</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Image</td>
<td align="center">Object</td>
<td align="center">Photo Tagging</td>
<td align="center">CNN</td>
</tr>

<tr>
<td align="center">Audio</td>
<td align="center">Text transcript</td>
<td align="center">Speech Recognition</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">English</td>
<td align="center">Chinese</td>
<td align="center">Machine Translation</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">Image, Radar info</td>
<td align="center">Position of other cars</td>
<td align="center">Autonomous Driving</td>
<td align="center">Hybrid</td>
</tr>
</tbody>
</table>

<ul>
<li>Image - convolutional neural network, <strong>CNN</strong></li>
<li>sequence data (temporal data, time series) - recurrent neural network, <strong>RNN</strong></li>
</ul>

<hr />

<ul>
<li><strong>Structured data</strong>: database</li>
<li><strong>Unstructured data</strong>: audio, image, text</li>
</ul>

<hr />

<h2 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h2>

<p>Being able to train a big enough neural network:</p>

<ul>
<li><strong>Data</strong>: huge amount of labeled data</li>
<li><strong>Computation</strong></li>
<li><strong>Algorithms</strong>:

<ul>
<li>try to make NNs run faster</li>
<li>e.g. switching from sigmoid to ReLU makes gradient descent algorithm run much faster</li>
</ul></li>
</ul>

<p><img src="/NN/scale.PNG" alt="scale" /></p>

<hr />

<h1 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h1>

<ul>
<li>binary classification problem: <code>$(x,y), x\in \mathbb{R}^{n_x}, y\in\{0,1\}$</code></li>
<li><code>$m$</code> training examples: <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code></li>
</ul>

<p>$$
X = \begin{pmatrix}
x^{(1)} &amp;x^{(2)} &amp;\cdots &amp;x^{(m)}
\end{pmatrix} \in \mathbb{R}^{n_x \times m}
$$</p>

<p>$$
y= \begin{pmatrix}
y^{(1)} &amp;y^{(2)} &amp;\cdots &amp;y^{(m)}
\end{pmatrix} \in \mathbb{R}^{1 \times m}
$$</p>

<ul>
<li>logistic regression:</li>
</ul>

<blockquote>
<p>Given <code>$x\in\mathbb{R}^{n_x}$</code>, want <code>$\hat{y}=\Pr(y=1|x)$</code></p>

<p>Output: <code>$\hat{y}=\sigma(\omega^Tx+b)$</code>, where sigmoid function <code>$\sigma(z)=\frac{1}{1+e^{-z}}$</code></p>

<ul>
<li>if <code>$z$</code> is large, <code>$\sigma(z)\approx 1$</code></li>
<li>if <code>$z$</code> is large negative number, <code>$\sigma(z)\approx 0$</code></li>
<li><code>$\sigma(0)=0.5$</code></li>
</ul>
</blockquote>

<ul>
<li>cost function:</li>
</ul>

<blockquote>
<p>Given <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code>, want <code>$\hat{y}^{(i)}=\sigma(\omega^Tx^{(i)}+b)\approx y^{(i)}$</code></p>

<p>loss(error) function: <code>$L(\hat{y},y)=-\left[y\log \hat{y}+(1-y)\log(1-\hat{y})\right]$</code></p>

<p>cost function: <code>$J(\omega,b)=\frac{1}{m}\sum\limits_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}\log \hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right]$</code></p>
</blockquote>

<ul>
<li>gradient descent:</li>
</ul>

<blockquote>
<ul>
<li><code>$J(\omega,b)$</code> is <strong>convex</strong>!</li>
<li>algorithm:</li>
</ul>

<p>repeat{</p>

<p><code>$\omega\leftarrow \omega - \alpha\frac{\partial}{\partial\omega} J(\omega,b)$</code></p>

<p><code>$b\leftarrow b - \alpha\frac{\partial}{\partial b} J(\omega,b)$</code></p>

<p>}</p>
</blockquote>

<ul>
<li>computation graph:

<ul>
<li><strong>forward propagation</strong>: compute current loss</li>
<li><strong>back propagation</strong>: compute current gradient</li>
</ul></li>
<li>logistic regression gradient descent: update parameters</li>
</ul>

<p><strong>One training example</strong>:</p>

<blockquote>
<p><code>$z= \omega^Tx+b$</code></p>

<p><code>$\hat{y} = a = \sigma(z)$</code></p>

<p><code>$L(a,y)=-(y\log a+(1-y)\log(1-a))$</code></p>

<hr />

<p><code>da</code> := <code>$\frac{dL}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$</code></p>

<p><code>dz</code> := <code>$\frac{dL}{dz}=\frac{dL}{da}\frac{da}{dz}=\left[-\frac{y}{a}+\frac{1-y}{1-a}\right]a(1-a)=a-y$</code></p>

<p><code>dwi</code> := <code>$\frac{\partial L}{\partial \omega_i}=x_i\frac{dL}{dz}$</code> = <code>xi*dz</code></p>

<p><code>db</code> := <code>$\frac{\partial L}{\partial b}=\frac{dL}{dz}$</code> = <code>dz</code></p>

<p><code>$\omega_i\leftarrow \omega_i - \alpha$</code> <code>dwi</code></p>

<p><code>$b\leftarrow b - \alpha$</code> <code>db</code></p>
</blockquote>

<p><code>$m$</code> <strong>training examples</strong>:</p>

<blockquote>
<p><code>$z^{(i)}= \omega^Tx^{(i)}+b$</code></p>

<p><code>$\hat{y}^{(i)} = a^{(i)} = \sigma(z^{(i)})$</code></p>

<p><code>$L(a^{(i)},y^{(i)})=-(y^{(i)}\log a^{(i)}+(1-y^{(i)})\log(1-a^{(i)}))$</code></p>

<p><code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^mL(a^{(i)},y^{(i)})$</code></p>

<hr />

<p><code>$\frac{\partial J}{\partial\omega_i} = \frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial\omega_i}L(a^{(i)},y^{(i)})$</code></p>
</blockquote>

<p>2 <code>for</code> loops: loop over all entries (<code>$m$</code>), loop over all features  (<code>$n$</code>)</p>

<p><code>$\Rightarrow$</code> vectorization, more efficient!</p>

<p><img src="/NN/logistic.PNG" alt="scale" /></p>

<hr />

<h2 id="vectorization">Vectorization</h2>

<ul>
<li>Whenever possible, avoid explicit for-loops.</li>
</ul>

<pre><code class="language-python">def sigmoid(u):
    return 1/(1 + np.exp(-u))

# X: data, n*m, n = NO. of features, m = NO. of examples
# Y: labels, 1*m
# w: weights, n*1
# b: bias, scalar

# compute activation
A = sigmoid(np.dot(w.T, X) + b)

# cost
J = - np.mean(np.log(A) * Y +  np.log(1-A) *(1-Y))

# back propagation (compute gradient)
m = X.shape[1]
dZ = A - Y
db = np.mean(dZ)
dw = np.dot(X, dZ.T)/m

# update params
w -= learning_rate*dw
b -= learning_rate*db
</code></pre>

<h2 id="explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</h2>

<p>$$
y|x \sim Binom(1, \hat{y}), \hat{y} =\sigma(\omega^Tx+b)
$$</p>

<p>$$
\Rightarrow \Pr(y|x)=\hat{y}^y(1-\hat{y})^{1-y}=\begin{cases}
1-\hat{y}, &amp;y=0\newline
\hat{y}, &amp;y=1
\end{cases}
$$</p>

<p>$$
\Rightarrow \log\Pr(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})=-L(\hat{y},y)
$$</p>

<p><strong>Goal</strong>: maximize <code>$\Pr(y|x)$</code> <code>$\Leftrightarrow$</code> minimize <code>$L(\hat{y},y)$</code></p>

<hr />

<p>Cost on <code>$m$</code> examples:</p>

<p>maximize <code>$\Pr(\text{labels in training set}) = \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> maximize <code>$\log  \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})=\sum_{i=1}^m\log\Pr(y^{(i)}|x^{(i)})=-\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> minimize <code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m L(\hat{y}^{(i)},y^{(i)})$</code></p>

<hr />

<h2 id="image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/01-Logistic-Regression-with-a-Neural-Network-mindset/LogisticRegressionwNN.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="shallow-neural-network">Shallow Neural Network</h1>

<h2 id="forward-propagation">Forward Propagation</h2>

<ul>
<li>Neural Network representation: <code>$k$</code>th layer, with activation function <code>$g^{[k]}$</code></li>
</ul>

<table>
<thead>
<tr>
<th align="center">Parameters</th>
<th align="center">Dimension</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$W^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, n^{[0]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, 1)$</code></td>
</tr>

<tr>
<td align="center"><code>$W^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, n^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, 1)$</code></td>
</tr>
</tbody>
</table>

<p>$$
A^{[0]} = X, n^{[2]}=1
$$</p>

<p>$$
\text{Forward Propagation }\begin{cases}
Z^{[k]} &amp;= W^{[k]}A^{[k-1]} + b^{[k]}, (\text{linear combination}) \newline
A^{[k]}&amp;=g^{[k]}(Z^{[k]}),  (activation)
\end{cases}
\ \ k=1,2,\ldots
$$</p>

<p><img src="/NN/nn.PNG" alt="scale" /></p>

<h2 id="activation-function">Activation Function</h2>

<ul>
<li><strong>activation function</strong> can be <em>sigmoid</em>, <em>tanh</em>, <em>ReLU, Leaky ReLU</em>&hellip;

<ul>
<li>for hidden layers, <em>tanh</em> always works better than <em>sigmoid</em> 【the mean of its output is closer to zero, and so it centers the data better for the next layer】</li>
<li>for output layer of binary classification (0/1), <em>sigmoid</em> may be better</li>
<li>different layers can have different activation functions</li>
<li>ReLU is increasingly the default choice (will learn faster)</li>
</ul></li>
</ul>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">function</th>
<th align="center">derivative</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">sigmoid</td>
<td align="center"><code>$\frac{1}{1+e^{-z}}$</code></td>
<td align="center"><code>$g(z)(1-g(z))$</code></td>
</tr>

<tr>
<td align="center">tanh</td>
<td align="center"><code>$\frac{e^z-e^{-z}}{e^z+e^{-z}}$</code></td>
<td align="center"><code>$1-(g(z))^2$</code></td>
</tr>

<tr>
<td align="center">ReLU</td>
<td align="center"><code>$\max(0, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>

<tr>
<td align="center">Leaky ReLU</td>
<td align="center"><code>$\max(0.01z, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0.01, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>
</tbody>
</table>

<h2 id="gradient-descent">Gradient Descent</h2>

<table>
<thead>
<tr>
<th align="center">Single Training Example</th>
<th align="center">m Training Examples</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$dz^{[2]}=a^{[2]}-y$</code></td>
<td align="center"><code>$dZ^{[2]}=A^{[2]}-y$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[2]}=dz^{[2]}a^{[1]T}$</code></td>
<td align="center"><code>$dW^{[2]}=\frac{1}{m} dZ^{[2]} (A^{[1]})^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[2]}=dz^{[2]}$</code></td>
<td align="center"><code>$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$da^{[1]}=(W^{[2]})^T dz^{[2]} \circ g^{[1]'}(z^{[1]})$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[1]}=dz^{[1]}x^T$</code></td>
<td align="center"><code>$dW^{[1]}=\frac{1}{m} dZ^{[1]} X^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[1]}=dz^{[1]}$</code></td>
<td align="center"><code>$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="summary">Summary</h2>

<table>
<thead>
<tr>
<th align="center">Forward Propagation</th>
<th align="center">Backward Propagation</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$Z^{[1]}=W^{[1]}X + b^{[1]}$</code></td>
<td align="center"><code>$dZ^{[2]}=A^{[2]}-Y$</code></td>
</tr>

<tr>
<td align="center"><code>$A^{[1]}=g^{[1]}(Z^{[1]})$</code></td>
<td align="center"><code>$dW^{[2]}=\frac{1}{m} dZ^{[2]} (A^{[1]})^T$</code><br/><code>$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code>  【elementwise product】</td>
</tr>

<tr>
<td align="center"><code>$A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})$</code></td>
<td align="center"><code>$dW^{[1]}=\frac{1}{m} dZ^{[1]} X^T$</code><br/><code>$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="random-initialization">Random Initialization</h2>

<ul>
<li><strong>If initialize weights and biases with 0</strong>: Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.</li>
<li><strong>If initialize weights to relative large values</strong> ( using the tanh activation for all the hidden units): the inputs of the <code>tanh</code> to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.</li>
</ul>

<hr />

<h2 id="planar-data-classification-python-code">Planar Data Classification - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/02-Planar-Data-Classification-with-One-Hidden-Layer/planarDataClassification.ipynb">Jupyter Notebook</a></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

