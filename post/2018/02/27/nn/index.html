<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Neural Networks and Deep Learning | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Neural Networks and Deep Learning</span></h1>

<h2 class="date">2018/02/27</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> 
  
  
  
  Tags: <a href="/tags/logisticregression">logisticRegression</a> <a href="/tags/imageclassification">imageClassification</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a></li>
</ul></li>
<li><a href="#logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</a>
<ul>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</a></li>
<li><a href="#image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</a></li>
</ul></li>
<li><a href="#shallow-neural-network">Shallow Neural Network</a>
<ul>
<li><a href="#forward-propagation">Forward Propagation</a></li>
<li><a href="#activation-function">Activation Function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
<li><a href="#planar-data-classification-python-code">Planar Data Classification - Python Code</a></li>
</ul></li>
<li><a href="#deep-neural-network">Deep Neural Network</a>
<ul>
<li><a href="#notations">Notations</a></li>
<li><a href="#forward-propagation-1">Forward Propagation</a></li>
<li><a href="#check-dimensions">Check Dimensions</a></li>
<li><a href="#forward-and-backward-functions">Forward and Backward Functions</a></li>
<li><a href="#parameters-and-hyperparameters">Parameters and Hyperparameters</a></li>
<li><a href="#build-deep-neural-network-image-classification-cat-non-cat-python-code">Build Deep Neural Network - Image Classification (cat/non-cat) - Python Code</a></li>
</ul></li>
<li><a href="#practical-aspect-of-deep-learning">Practical Aspect of Deep Learning</a>
<ul>
<li><a href="#regularization">Regularization</a>
<ul>
<li><a href="#dropout-regularization">Dropout Regularization</a></li>
<li><a href="#other-regularization-methods">Other Regularization Methods</a></li>
</ul></li>
<li><a href="#speed-up-training">Speed Up Training</a></li>
<li><a href="#debugging-of-backpropagation-gradient-checking">Debugging of Backpropagation: gradient checking</a></li>
</ul></li>
<li><a href="#optimization-algorithms">Optimization Algorithms</a>
<ul>
<li><a href="#mini-batch-gradient-descent">Mini-Batch Gradient Descent</a></li>
<li><a href="#gradient-descent-with-momentum">Gradient Descent with Momentum</a>
<ul>
<li><a href="#exponentially-weighted-averages">Exponentially Weighted Averages</a></li>
<li><a href="#bias-correction-in-exponentially-weighted-averages">Bias Correction in Exponentially Weighted Averages</a></li>
</ul></li>
</ul></li>
</ul>
</nav>


<main>


<h1 id="introduction">Introduction</h1>

<h2 id="what-is-a-neural-network">What is a Neural Network?</h2>

<ul>
<li><strong>ReLU</strong> = Rectified Linear Unit</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Input</th>
<th align="center">Output</th>
<th align="center">Application</th>
<th align="center">Model</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Home Features</td>
<td align="center">Price</td>
<td align="center">Real Estate</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Ad, User info</td>
<td align="center">Click on ad? 0/1</td>
<td align="center">Online Advertising</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Image</td>
<td align="center">Object</td>
<td align="center">Photo Tagging</td>
<td align="center">CNN</td>
</tr>

<tr>
<td align="center">Audio</td>
<td align="center">Text transcript</td>
<td align="center">Speech Recognition</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">English</td>
<td align="center">Chinese</td>
<td align="center">Machine Translation</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">Image, Radar info</td>
<td align="center">Position of other cars</td>
<td align="center">Autonomous Driving</td>
<td align="center">Hybrid</td>
</tr>
</tbody>
</table>

<ul>
<li>Image - convolutional neural network, <strong>CNN</strong></li>
<li>sequence data (temporal data, time series) - recurrent neural network, <strong>RNN</strong></li>
</ul>

<hr />

<ul>
<li><strong>Structured data</strong>: database</li>
<li><strong>Unstructured data</strong>: audio, image, text</li>
</ul>

<hr />

<h2 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h2>

<p>Being able to train a big enough neural network:</p>

<ul>
<li><strong>Data</strong>: huge amount of labeled data</li>
<li><strong>Computation</strong></li>
<li><strong>Algorithms</strong>:

<ul>
<li>try to make NNs run faster</li>
<li>e.g. switching from sigmoid to ReLU makes gradient descent algorithm run much faster</li>
</ul></li>
</ul>

<p><img src="/NN/scale.PNG" alt="scale" /></p>

<hr />

<h1 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h1>

<ul>
<li>binary classification problem: <code>$(x,y), x\in \mathbb{R}^{n_x}, y\in\{0,1\}$</code></li>
<li><code>$m$</code> training examples: <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code></li>
</ul>

<p>$$
X = \begin{pmatrix}
x^{(1)} &amp;x^{(2)} &amp;\cdots &amp;x^{(m)}
\end{pmatrix} \in \mathbb{R}^{n_x \times m}
$$</p>

<p>$$
y= \begin{pmatrix}
y^{(1)} &amp;y^{(2)} &amp;\cdots &amp;y^{(m)}
\end{pmatrix} \in \mathbb{R}^{1 \times m}
$$</p>

<ul>
<li>logistic regression:</li>
</ul>

<blockquote>
<p>Given <code>$x\in\mathbb{R}^{n_x}$</code>, want <code>$\hat{y}=\Pr(y=1|x)$</code></p>

<p>Output: <code>$\hat{y}=\sigma(\omega^Tx+b)$</code>, where sigmoid function <code>$\sigma(z)=\frac{1}{1+e^{-z}}$</code></p>

<ul>
<li>if <code>$z$</code> is large, <code>$\sigma(z)\approx 1$</code></li>
<li>if <code>$z$</code> is large negative number, <code>$\sigma(z)\approx 0$</code></li>
<li><code>$\sigma(0)=0.5$</code></li>
</ul>
</blockquote>

<ul>
<li>cost function:</li>
</ul>

<blockquote>
<p>Given <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code>, want <code>$\hat{y}^{(i)}=\sigma(\omega^Tx^{(i)}+b)\approx y^{(i)}$</code></p>

<p>loss(error) function: <code>$\mathcal{L}(\hat{y},y)=-\left[y\log \hat{y}+(1-y)\log(1-\hat{y})\right]$</code></p>

<p>cost function: <code>$J(\omega,b)=\frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}\log \hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right]$</code></p>
</blockquote>

<ul>
<li>gradient descent:</li>
</ul>

<blockquote>
<ul>
<li><code>$J(\omega,b)$</code> is <strong>convex</strong>!</li>
<li>algorithm:</li>
</ul>

<p>repeat{</p>

<p><code>$\omega\leftarrow \omega - \alpha\frac{\partial}{\partial\omega} J(\omega,b)$</code></p>

<p><code>$b\leftarrow b - \alpha\frac{\partial}{\partial b} J(\omega,b)$</code></p>

<p>}</p>
</blockquote>

<ul>
<li>computation graph:

<ul>
<li><strong>forward propagation</strong>: compute current loss</li>
<li><strong>back propagation</strong>: compute current gradient</li>
</ul></li>
<li>logistic regression gradient descent: update parameters</li>
</ul>

<p><strong>One training example</strong>:</p>

<blockquote>
<p><code>$z= \omega^Tx+b$</code></p>

<p><code>$\hat{y} = a = \sigma(z)$</code></p>

<p><code>$\mathcal{L}(a,y)=-(y\log a+(1-y)\log(1-a))$</code></p>

<hr />

<p><code>da</code> := <code>$\frac{d\mathcal{L}}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$</code></p>

<p><code>dz</code> := <code>$\frac{d\mathcal{L}}{dz}=\frac{d\mathcal{L}}{da}\frac{da}{dz}=\left[-\frac{y}{a}+\frac{1-y}{1-a}\right]a(1-a)=a-y$</code></p>

<p><code>dwi</code> := <code>$\frac{\partial \mathcal{L}}{\partial \omega_i}=x_i\frac{d\mathcal{L}}{dz}$</code> = <code>xi*dz</code></p>

<p><code>db</code> := <code>$\frac{\partial \mathcal{L}}{\partial b}=\frac{d\mathcal{L}}{dz}$</code> = <code>dz</code></p>

<p><code>$\omega_i\leftarrow \omega_i - \alpha$</code> <code>dwi</code></p>

<p><code>$b\leftarrow b - \alpha$</code> <code>db</code></p>
</blockquote>

<p><code>$m$</code> <strong>training examples</strong>:</p>

<blockquote>
<p><code>$z^{(i)}= \omega^Tx^{(i)}+b$</code></p>

<p><code>$\hat{y}^{(i)} = a^{(i)} = \sigma(z^{(i)})$</code></p>

<p><code>$\mathcal{L}(a^{(i)},y^{(i)})=-(y^{(i)}\log a^{(i)}+(1-y^{(i)})\log(1-a^{(i)}))$</code></p>

<p><code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(a^{(i)},y^{(i)})$</code></p>

<hr />

<p><code>$\frac{\partial J}{\partial\omega_i} = \frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial\omega_i}\mathcal{L}(a^{(i)},y^{(i)})$</code></p>
</blockquote>

<p>2 <code>for</code> loops: loop over all entries (<code>$m$</code>), loop over all features  (<code>$n$</code>)</p>

<p><code>$\Rightarrow$</code> vectorization, more efficient!</p>

<p><img src="/NN/logistic.PNG" alt="scale" /></p>

<hr />

<h2 id="vectorization">Vectorization</h2>

<ul>
<li>Whenever possible, avoid explicit for-loops.</li>
</ul>

<pre><code class="language-python">def sigmoid(u):
    return 1/(1 + np.exp(-u))

# X: data, n*m, n = NO. of features, m = NO. of examples
# Y: labels, 1*m
# w: weights, n*1
# b: bias, scalar

# compute activation
A = sigmoid(np.dot(w.T, X) + b)

# cost
J = - np.mean(np.log(A) * Y +  np.log(1-A) *(1-Y))

# back propagation (compute gradient)
m = X.shape[1]
dZ = A - Y
db = np.mean(dZ)
dw = np.dot(X, dZ.T)/m

# update params
w -= learning_rate*dw
b -= learning_rate*db
</code></pre>

<h2 id="explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</h2>

<p>$$
y|x \sim Binom(1, \hat{y}), \hat{y} =\sigma(\omega^Tx+b)
$$</p>

<p>$$
\Rightarrow \Pr(y|x)=\hat{y}^y(1-\hat{y})^{1-y}=\begin{cases}
1-\hat{y}, &amp;y=0\newline
\hat{y}, &amp;y=1
\end{cases}
$$</p>

<p>$$
\Rightarrow \log\Pr(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})=-\mathcal{L}(\hat{y},y)
$$</p>

<p><strong>Goal</strong>: maximize <code>$\Pr(y|x)$</code> <code>$\Leftrightarrow$</code> minimize <code>$\mathcal{L}(\hat{y},y)$</code></p>

<hr />

<p>Cost on <code>$m$</code> examples:</p>

<p>maximize <code>$\Pr(\text{labels in training set}) = \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> maximize <code>$\log  \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})=\sum_{i=1}^m\log\Pr(y^{(i)}|x^{(i)})=-\sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)},y^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> minimize <code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)},y^{(i)})$</code></p>

<hr />

<h2 id="image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/01-Logistic-Regression-with-a-Neural-Network-mindset/LogisticRegressionwNN.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="shallow-neural-network">Shallow Neural Network</h1>

<h2 id="forward-propagation">Forward Propagation</h2>

<ul>
<li>Neural Network representation: <code>$k$</code>th layer, with activation function <code>$g^{[k]}$</code></li>
</ul>

<table>
<thead>
<tr>
<th align="center">Parameters</th>
<th align="center">Dimension</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$W^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, n^{[0]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, 1)$</code></td>
</tr>

<tr>
<td align="center"><code>$W^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, n^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, 1)$</code></td>
</tr>
</tbody>
</table>

<p>$$
A^{[0]} = X, n^{[2]}=1
$$</p>

<p>$$
\text{Forward Propagation }\begin{cases}
Z^{[k]} &amp;= W^{[k]}A^{[k-1]} + b^{[k]}, (\text{linear combination}) \newline
A^{[k]}&amp;=g^{[k]}(Z^{[k]}),  (activation)
\end{cases}
\ \ k=1,2,\ldots
$$</p>

<p><img src="/NN/nn.PNG" alt="scale" /></p>

<h2 id="activation-function">Activation Function</h2>

<ul>
<li><strong>activation function</strong> can be <em>sigmoid</em>, <em>tanh</em>, <em>ReLU, Leaky ReLU</em>&hellip;

<ul>
<li>for hidden layers, <em>tanh</em> always works better than <em>sigmoid</em> 【the mean of its output is closer to zero, and so it centers the data better for the next layer】</li>
<li>for output layer of binary classification (0/1), <em>sigmoid</em> may be better</li>
<li>different layers can have different activation functions</li>
<li>ReLU is increasingly the default choice (will learn faster)</li>
</ul></li>
</ul>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">function</th>
<th align="center">derivative</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">sigmoid</td>
<td align="center"><code>$\frac{1}{1+e^{-z}}$</code></td>
<td align="center"><code>$g(z)(1-g(z))$</code></td>
</tr>

<tr>
<td align="center">tanh</td>
<td align="center"><code>$\frac{e^z-e^{-z}}{e^z+e^{-z}}$</code></td>
<td align="center"><code>$1-(g(z))^2$</code></td>
</tr>

<tr>
<td align="center">ReLU</td>
<td align="center"><code>$\max(0, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>

<tr>
<td align="center">Leaky ReLU</td>
<td align="center"><code>$\max(0.01z, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0.01, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>
</tbody>
</table>

<h2 id="gradient-descent">Gradient Descent</h2>

<table>
<thead>
<tr>
<th align="center">Single Training Example</th>
<th align="center">m Training Examples</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$dz^{[2]}=a^{[2]}-y$</code></td>
<td align="center"><code>$dZ^{[2]}=A^{[2]}-y$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[2]}=dz^{[2]}a^{[1]T}$</code></td>
<td align="center"><code>$dW^{[2]}=\frac{1}{m} dZ^{[2]} (A^{[1]})^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[2]}=dz^{[2]}$</code></td>
<td align="center"><code>$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$da^{[1]}=(W^{[2]})^T dz^{[2]} \circ g^{[1]'}(z^{[1]})$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[1]}=dz^{[1]}x^T$</code></td>
<td align="center"><code>$dW^{[1]}=\frac{1}{m} dZ^{[1]} X^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[1]}=dz^{[1]}$</code></td>
<td align="center"><code>$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="summary">Summary</h2>

<table>
<thead>
<tr>
<th align="center">Forward Propagation</th>
<th align="center">Backward Propagation</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$Z^{[1]}=W^{[1]}X + b^{[1]}$</code></td>
<td align="center"><code>$dZ^{[2]}=A^{[2]}-Y$</code></td>
</tr>

<tr>
<td align="center"><code>$A^{[1]}=g^{[1]}(Z^{[1]})$</code></td>
<td align="center"><code>$dW^{[2]}=\frac{1}{m} dZ^{[2]} (A^{[1]})^T$</code><br/><code>$db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code>  【elementwise product】</td>
</tr>

<tr>
<td align="center"><code>$A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})$</code></td>
<td align="center"><code>$dW^{[1]}=\frac{1}{m} dZ^{[1]} X^T$</code><br/><code>$db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="random-initialization">Random Initialization</h2>

<ul>
<li><strong>If initialize weights and biases with 0</strong>: Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.</li>
<li><strong>If initialize weights to relative large values</strong> ( using the tanh activation for all the hidden units): the inputs of the <code>tanh</code> to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.</li>
</ul>

<hr />

<h2 id="planar-data-classification-python-code">Planar Data Classification - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/02-Planar-Data-Classification-with-One-Hidden-Layer/planarDataClassification.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="deep-neural-network">Deep Neural Network</h1>

<h2 id="notations">Notations</h2>

<p><img src="/NN/deep.PNG" alt="shallow and deep" /></p>

<ul>
<li><code>$L$</code>: # of layers</li>
<li><code>$n^{[\ell]}$</code>: # of units in layer <code>$\ell$</code></li>
<li><code>$a^{[\ell]}$</code>: activations in layer <code>$\ell$</code>

<ul>
<li><code>$a^{[\ell]}=g^{[\ell]}(z^{[\ell]})$</code></li>
</ul></li>
<li><code>$w^{[\ell]}, b^{[\ell]}$</code>: weights and bias for computing <code>$z^{[\ell]}$</code></li>
</ul>

<hr />

<h2 id="forward-propagation-1">Forward Propagation</h2>

<p>Layer <code>$\ell$</code>:</p>

<ul>
<li><p><code>$a^{[0]}:=x$</code></p></li>

<li><p><code>$z^{[\ell]}=W^{[\ell]}a^{[\ell-1]}+b^{[\ell]}$</code></p></li>

<li><p><code>$a^{[\ell]}=g^{[\ell]}(z^{[\ell]})$</code></p></li>
</ul>

<p>(<strong><u>Vectorized</u></strong>) layer <code>$\ell$</code>:</p>

<ul>
<li><p><code>$A^{[0]}:=X$</code></p></li>

<li><p><code>$Z^{[\ell]}=W^{[\ell]}A^{[\ell-1]}+b^{[\ell]}$</code> (numpy broadcasting)</p></li>

<li><p><code>$A^{[\ell]}=g^{[\ell]}(Z^{[\ell]})$</code></p></li>
</ul>

<hr />

<h2 id="check-dimensions">Check Dimensions</h2>

<ul>
<li>shape of <code>$W^{[\ell]}$</code> and <code>$dW^{[\ell]}$</code>: <code>$(n^{[\ell]}, n^{[\ell-1]})$</code></li>
<li>shape of <code>$b^{[\ell]}$</code> and <code>$db^{[\ell]}$</code>: <code>$(n^{[\ell]}, 1)$</code></li>
<li>shape of <code>$Z^{[\ell]}, A^{[\ell]}, dZ^{[\ell]}, dA^{[\ell]}$</code>: <code>$(n^{[\ell]},m)$</code></li>
</ul>

<hr />

<h2 id="forward-and-backward-functions">Forward and Backward Functions</h2>

<p>layer <code>$\ell$</code></p>

<ul>
<li>Forward:

<ul>
<li>input: <code>$a^{[\ell-1]}$</code></li>
<li>output: <code>$a^{[\ell]}$</code>, cache <code>$z^{[\ell]}$</code></li>
</ul></li>
<li>Backward:

<ul>
<li>input: <code>$da^{[\ell]}$</code></li>
<li>output: <code>$da^{[\ell-1]}, dW^{[\ell]}, db^{[\ell]}$</code></li>
<li>computation:</li>
</ul></li>
</ul>

<p>$$
dz^{[\ell]} = da^{[\ell]} * (g^{[\ell]})&lsquo;(z^{[\ell]}) \text{ (elementwise product)}\\<br />
dW^{[\ell]} = dz^{[\ell]}a^{[\ell-1]}\\<br />
db^{[\ell]}=dz^{[\ell]}\\<br />
da^{[\ell-1]}=W^{[\ell]T}dz^{[\ell]} \Rightarrow dz^{[\ell]} =W^{[\ell+1]T}dz^{[\ell+1]} * (g^{[\ell]})&lsquo;(z^{[\ell]})
$$</p>

<p>$$
\text{Vectorized}: dZ^{[\ell]} = dA^{[\ell]} * (g^{[\ell]})&lsquo;(Z^{[\ell]}) \text{ (elementwise product)}\\<br />
dW^{[\ell]} = \frac{1}{m}dZ^{[\ell]}A^{[\ell-1]T}\\<br />
db^{[\ell]}=\frac{1}{m} np.sum(dZ^{[\ell]}, axis=1,keepdims=True)\\<br />
dA^{[\ell-1]}=W^{[\ell]T}dZ^{[\ell]}
$$</p>

<p><img src="/NN/compute.PNG" alt="input and output" /></p>

<hr />

<h2 id="parameters-and-hyperparameters">Parameters and Hyperparameters</h2>

<ul>
<li>parameters: <code>$W^{[\ell]}, b^{[\ell]}$</code></li>
<li>hyperparameters:

<ul>
<li>learning rate <code>$\alpha$</code></li>
<li># of iterations</li>
<li># of hidden layers <code>$L$</code></li>
<li># of hidden units <code>$n^{[\ell]}$</code></li>
<li>choice of activation function</li>
<li>momentum</li>
<li>minibatch size</li>
<li>regularizations</li>
</ul></li>
</ul>

<hr />

<h2 id="build-deep-neural-network-image-classification-cat-non-cat-python-code">Build Deep Neural Network - Image Classification (cat/non-cat) - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/03-Build-Deep-Neural-Network/Building%20Deep%20Neural%20Network.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="practical-aspect-of-deep-learning">Practical Aspect of Deep Learning</h1>

<ul>
<li>train/dev/test sets

<ul>
<li>traditional rules of thumb ratio: 60/20/20</li>
<li>big data set: e.g. 98/1/1, 99.5/.4/.1</li>
</ul></li>
<li>mismatched train/test distribution

<ul>
<li><strong><u>guideline</u></strong>: make sure <strong>dev</strong> and <strong>test</strong> come from same distribution</li>
</ul></li>
<li>not having a test set is ok (only dev set)</li>
<li>bias/variance <code>$\Rightarrow$</code> <strong>Basic Recipe</strong>:

<ul>
<li>high bias? <code>$\rightarrow$</code>  <u>bigger network</u>, train longer, NN architecture search</li>
<li>high variance? <code>$\rightarrow$</code> <u>get more data</u>, regularization, NN architecture search</li>
</ul></li>
<li><strong><u>no more</u></strong> bias/variance tradeoff:

<ul>
<li>pre-deep-learning era: can not just reduce bias or variance without hurting the other</li>
<li>deep-learning big-data era: a bigger network almost always reduces bias without necessarily hurting the variance so long as you regularize properly; getting more data almost always reduces variance without hurting bias much</li>
</ul></li>
</ul>

<hr />

<h2 id="regularization">Regularization</h2>

<ul>
<li>Cost function</li>
</ul>

<p>$$
J(W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]})=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})+\frac{\lambda}{2m}\sum_{\ell=1}^L\Vert W^{[\ell]}\Vert_F^2
$$</p>

<p>where <code>$\Vert\cdot\Vert_F$</code> is the Frobenius norm.</p>

<ul>
<li><code>$dW^{[\ell]}=$</code> (from backprop) <code>$+ \frac{\lambda}{m}W^{[\ell]}$</code>

<ul>
<li>update: <code>$W^{[\ell]} := W^{[\ell]} -\alpha dW^{[\ell]}$</code></li>
<li><code>$\Rightarrow W^{[\ell]}:=(1-\frac{\alpha\lambda}{m})W^{[\ell]}-\alpha$</code>(from backprop)</li>
<li>L2 regularization = weight decay</li>
</ul></li>
<li>prevent overfitting, reduce variance</li>
</ul>

<hr />

<h3 id="dropout-regularization">Dropout Regularization</h3>

<ul>
<li>Implement Dropout (&rdquo;<strong><u>Inverted Dropout</u></strong>&rdquo;)</li>
</ul>

<blockquote>
<p>Illustrate with layer <code>$\ell=3$</code></p>

<pre><code class="language-python">keepProb = 0.8 # probability a unit will be kept
d3 = np.random.rand(a3.shape) &lt; keepProb
a3 *= d3 # elementwise product
a3 /= keepProb # ensure the expected value of a3 remains the same
</code></pre>
</blockquote>

<ul>
<li>make predictions at test time: <strong>no dropout</strong></li>
<li>why does dropout work?

<ul>
<li><strong>intuition</strong>: cannot rely on any one feature, so have to spread out weights</li>
<li>shrink weights (similar to L2)</li>
</ul></li>
<li>vary <code>keepProb</code> by layer</li>
<li>cost function is no longer well-defined</li>
</ul>

<hr />

<h3 id="other-regularization-methods">Other Regularization Methods</h3>

<ul>
<li><strong>Data augmentation</strong>: e.g. image flip/zoom-in/rotate/distortion</li>
<li><strong>Early stopping</strong>:

<ul>
<li>cons: can no longer work on 2 steps of orthogonalization independently</li>
<li>alternative: L2 regularization (cons: more computationally expensive)</li>
</ul></li>
</ul>

<blockquote>
<p><strong>Orthogonalization</strong>:</p>

<ol>
<li>optimize cost function: gradient descent, &hellip;</li>
<li>not overfit: regularization, &hellip;</li>
</ol>
</blockquote>

<p><img src="/NN/early.PNG" alt="early stopping" /></p>

<hr />

<h2 id="speed-up-training">Speed Up Training</h2>

<ul>
<li><strong>normalizing training sets</strong>

<ul>
<li>make sure all features are on similar scale, thus making cost function easier and faster to optimize</li>
<li>use the same mean/sd to normalized the test set</li>
</ul></li>
</ul>

<p><img src="/NN/normal.PNG" alt="normalization" /></p>

<ul>
<li>weight initialization for deep networks

<ul>
<li>partial solution to <strong>vanishing/exploding gradients</strong></li>
<li>use <strong>Xavier initialization</strong>, &hellip;</li>
<li>can also be tune as hyperparameter</li>
</ul></li>
</ul>

<blockquote>
<p><code>$z=w_1x_1 +w_2x_2 +\cdots + w_nx_n$</code> (omit bias)</p>

<p>set <code>$var(w_i)=\frac{1}{n}$</code> (<code>$n=$</code> # of input nodes)</p>

<pre><code class="language-python">W = np.random.randn(node_in, node_out) / np.sqrt(node_in)
# relu activation
# W = np.random.randn(node_in, node_out) / np.sqrt(node_in/2)
# tanh activation
# W = np.random.randn(node_in, node_out) / np.sqrt(node_in)
# W = np.random.randn(node_in, node_out) / np.sqrt((node_in + node_out)/2)
</code></pre>
</blockquote>

<h2 id="debugging-of-backpropagation-gradient-checking">Debugging of Backpropagation: gradient checking</h2>

<ul>
<li>take <code>$W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]}$</code> and reshape into a big vector <code>$\theta$</code></li>
<li>take <code>$dW^{[1]},db^{[1]},\ldots,dW^{[L]},db^{[L]}$</code> and reshape into a big vector <code>$d\theta$</code></li>
<li>grad check:</li>
</ul>

<blockquote>
<p>for each <code>$i$</code>:
$$
d\theta_{approx}[i]:=\frac{J(\theta_1,\theta_2,\ldots,\theta_i+\epsilon,\ldots)-J(\theta_1,\theta_2,\ldots,\theta_i-\epsilon,\ldots)}{2\epsilon}\approx \frac{\partial J}{\partial \theta_i}
$$</p>

<p>check
$$
\frac{\Vert d\theta_{approx}-d\theta\Vert_2}{\Vert d\theta_{approx}\Vert_2+\Vert d\theta\Vert_2} \approx 0, e.g. &lt; 1e-7
$$</p>
</blockquote>

<ul>
<li>do not use in training - only to debug</li>
<li>if algorithm fails grad check, look at components to identify bug</li>
<li>remember regularization term</li>
<li>grad check does not work with dropout

<ul>
<li>implement grad check without dropout (turn off dropout, <code>keepProb = 1</code>)</li>
</ul></li>
<li>run at random initialization; perhaps again after some training</li>
</ul>

<hr />

<h1 id="optimization-algorithms">Optimization Algorithms</h1>

<h2 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h2>

<ul>
<li>batch vs mini-batch</li>
<li>mini-batch <code>$X^{\{t\}}, y^{\{t\}}$</code></li>
<li>mini-batch gradient descent</li>
</ul>

<p><img src="/NN/mini.PNG" alt="mini batch" /></p>

<p><img src="/NN/minigd.PNG" alt="mini batch cost" /></p>

<ul>
<li>choose mini-batch size: hyperparameter

<ul>
<li>size = <code>$m$</code>: batch gradient descent</li>
<li>size = 1: <strong>stochastic gradient descent</strong>, every example is a mini-batch</li>
<li>in practice, size between 1 and <code>$m$</code>

<ul>
<li>batch gradient descent: too long per iteration</li>
<li>gradient gradient descent: lose speedup from vectorization</li>
</ul></li>
<li>small training set (<code>$m\leq 2000$</code>): use batch gradient descent</li>
<li>typical mini-batch size: 64,128,256,512</li>
<li>make sure mini-batch fits in CPU/GPU memory</li>
</ul></li>
</ul>

<p><img src="/NN/sgd.PNG" alt="stochastic gradient descent" /></p>

<hr />

<h2 id="gradient-descent-with-momentum">Gradient Descent with Momentum</h2>

<h3 id="exponentially-weighted-averages">Exponentially Weighted Averages</h3>

<p>$$
v_0=0, v_t = \beta v_{t-1}+(1-\beta)\theta_t
$$</p>

<p>$$
\Rightarrow v_t = \beta^tv_0 +(1-\beta)\left[ \theta_t + \beta\theta_{t-1} +\ldots \beta^{t-1}\theta_1  \right]=(1-\beta)\sum_{i=0}^{t-1}\beta^i\theta_{t-i}
$$</p>

<p>Since
$$
\beta^{\frac{1}{1-\beta}}=\left(1-(1-\beta)\right)^{\frac{1}{1-\beta}}\approx \frac{1}{e},
$$
<code>$v_t\approx$</code> average of the last <code>$\frac{1}{1-\beta}$</code> terms of <code>$\theta$</code>&rsquo;s</p>

<blockquote>
<p><code>$v_{\theta}:=0$</code></p>

<p>repeat{</p>

<p>get next <code>$\theta_t$</code></p>

<p><code>$v_{\theta}:=\beta v_{\theta} + (1-\beta)\theta_t$</code></p>

<p>}</p>
</blockquote>

<hr />

<h3 id="bias-correction-in-exponentially-weighted-averages">Bias Correction in Exponentially Weighted Averages</h3>

<p>Take <code>$v_t := v_t/(1-\beta^t)$</code></p>

<p><img src="/NN/bias.PNG" alt="bias correction" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

