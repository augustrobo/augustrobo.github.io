<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Neural Networks and Deep Learning | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Neural Networks and Deep Learning</span></h1>

<h2 class="date">2018/02/27</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> 
  
  
  
  Tags: <a href="/tags/logisticregression">logisticRegression</a> <a href="/tags/imageclassification">imageClassification</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#what-is-a-neural-network">What is a Neural Network?</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a></li>
</ul></li>
<li><a href="#logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</a>
<ul>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</a></li>
<li><a href="#image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</a></li>
</ul></li>
<li><a href="#shallow-neural-network">Shallow Neural Network</a>
<ul>
<li><a href="#forward-propagation">Forward Propagation</a></li>
<li><a href="#activation-function">Activation Function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
<li><a href="#planar-data-classification-python-code">Planar Data Classification - Python Code</a></li>
</ul></li>
<li><a href="#deep-neural-network">Deep Neural Network</a>
<ul>
<li><a href="#notations">Notations</a></li>
<li><a href="#forward-propagation-1">Forward Propagation</a></li>
<li><a href="#check-dimensions">Check Dimensions</a></li>
<li><a href="#forward-and-backward-functions">Forward and Backward Functions</a></li>
<li><a href="#parameters-and-hyperparameters">Parameters and Hyperparameters</a></li>
<li><a href="#build-deep-neural-network-image-classification-cat-non-cat-python-code">Build Deep Neural Network - Image Classification (cat/non-cat) - Python Code</a></li>
</ul></li>
<li><a href="#practical-aspects-of-deep-learning">Practical Aspects of Deep Learning</a>
<ul>
<li><a href="#regularization">Regularization</a>
<ul>
<li><a href="#l2-regularization">L2 Regularization</a></li>
<li><a href="#dropout-regularization">Dropout Regularization</a></li>
<li><a href="#other-regularization-methods">Other Regularization Methods</a></li>
</ul></li>
<li><a href="#speed-up-training">Speed Up Training</a></li>
<li><a href="#debugging-of-backpropagation-gradient-checking">Debugging of Backpropagation: gradient checking</a></li>
<li><a href="#python-code">Python Code</a></li>
</ul></li>
<li><a href="#optimization-algorithms">Optimization Algorithms</a>
<ul>
<li><a href="#mini-batch-gradient-descent">Mini-Batch Gradient Descent</a></li>
<li><a href="#momentum">Momentum</a>
<ul>
<li><a href="#exponentially-weighted-averages">Exponentially Weighted Averages</a></li>
<li><a href="#bias-correction-in-exponentially-weighted-averages">Bias Correction in Exponentially Weighted Averages</a></li>
<li><a href="#gradient-descent-with-momentum">Gradient Descent With Momentum</a></li>
</ul></li>
<li><a href="#root-mean-square-prop-rms-prop">Root Mean Square Prop (RMS Prop)</a></li>
<li><a href="#adam-optimization-algorithm">Adam Optimization Algorithm</a></li>
<li><a href="#learning-rate-decay">Learning Rate Decay</a></li>
<li><a href="#local-optima-in-neural-networks">Local Optima in Neural Networks</a></li>
<li><a href="#python-code-optimization-algorithms">Python Code - Optimization Algorithms</a></li>
</ul></li>
<li><a href="#tuning-process">Tuning Process</a>
<ul>
<li><a href="#hyperparameters">Hyperparameters</a></li>
<li><a href="#hyperparameters-tuning-in-practice">Hyperparameters Tuning in Practice</a></li>
<li><a href="#batch-normalization">Batch Normalization</a></li>
</ul></li>
<li><a href="#multiclass-classification">Multiclass Classification</a>
<ul>
<li><a href="#softmax-regression">Softmax Regression</a></li>
</ul></li>
<li><a href="#deep-learning-frameworks">Deep Learning Frameworks</a>
<ul>
<li><a href="#choosing-deep-learning-frameworks">Choosing Deep Learning Frameworks</a></li>
</ul></li>
<li><a href="#ml-strategy">ML Strategy</a>
<ul>
<li><a href="#single-number-evaluation-metric">Single Number Evaluation Metric</a></li>
<li><a href="#satisfying-and-optimizing-metric">Satisfying and Optimizing Metric</a></li>
<li><a href="#train-dev-test-distributions">Train/Dev/Test Distributions</a></li>
<li><a href="#size-of-the-dev-and-test-sets">Size of the Dev and Test Sets</a></li>
<li><a href="#when-to-change-dev-test-sets-and-metrics">When to Change Dev/Test Sets and Metrics</a></li>
<li><a href="#comparing-to-human-level-performance">Comparing to Human-Level Performance</a></li>
</ul></li>
<li><a href="#error-analysis">Error Analysis</a>
<ul>
<li><a href="#cleaning-up-incorrectly-labeled-data">Cleaning Up Incorrectly Labeled Data</a></li>
</ul></li>
<li><a href="#mismatched-training-and-dev-test-set">Mismatched Training and Dev/Test Set</a>
<ul>
<li><a href="#addressing-data-mismatch">Addressing Data Mismatch</a></li>
</ul></li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
<li><a href="#multi-task-learning">Multi-task Learning</a></li>
<li><a href="#end-to-end-deep-learning">End-to-End Deep Learning</a></li>
</ul>
</nav>


<main>


<h1 id="introduction">Introduction</h1>

<h2 id="what-is-a-neural-network">What is a Neural Network?</h2>

<ul>
<li><strong>ReLU</strong> = Rectified Linear Unit</li>
</ul>

<table>
<thead>
<tr>
<th align="center">Input</th>
<th align="center">Output</th>
<th align="center">Application</th>
<th align="center">Model</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Home Features</td>
<td align="center">Price</td>
<td align="center">Real Estate</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Ad, User info</td>
<td align="center">Click on ad? 0/1</td>
<td align="center">Online Advertising</td>
<td align="center">NN</td>
</tr>

<tr>
<td align="center">Image</td>
<td align="center">Object</td>
<td align="center">Photo Tagging</td>
<td align="center">CNN</td>
</tr>

<tr>
<td align="center">Audio</td>
<td align="center">Text transcript</td>
<td align="center">Speech Recognition</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">English</td>
<td align="center">Chinese</td>
<td align="center">Machine Translation</td>
<td align="center">RNN</td>
</tr>

<tr>
<td align="center">Image, Radar info</td>
<td align="center">Position of other cars</td>
<td align="center">Autonomous Driving</td>
<td align="center">Hybrid</td>
</tr>
</tbody>
</table>

<ul>
<li>Image - convolutional neural network, <strong>CNN</strong></li>
<li>sequence data (temporal data, time series) - recurrent neural network, <strong>RNN</strong></li>
</ul>

<hr />

<ul>
<li><strong>Structured data</strong>: database</li>
<li><strong>Unstructured data</strong>: audio, image, text</li>
</ul>

<hr />

<h2 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h2>

<p>Being able to train a big enough neural network:</p>

<ul>
<li><strong>Data</strong>: huge amount of labeled data</li>
<li><strong>Computation</strong></li>
<li><strong>Algorithms</strong>:

<ul>
<li>try to make NNs run faster</li>
<li>e.g. switching from sigmoid to ReLU makes gradient descent algorithm run much faster</li>
</ul></li>
</ul>

<p><img src="/NN/scale.PNG" alt="scale" /></p>

<hr />

<h1 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h1>

<ul>
<li>binary classification problem: <code>$(x,y), x\in \mathbb{R}^{n_x}, y\in\{0,1\}$</code></li>
<li><code>$m$</code> training examples: <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code></li>
</ul>

<p>$$
X = \begin{pmatrix}
x^{(1)} &amp;x^{(2)} &amp;\cdots &amp;x^{(m)}
\end{pmatrix} \in \mathbb{R}^{n_x \times m}
$$</p>

<p>$$
y= \begin{pmatrix}
y^{(1)} &amp;y^{(2)} &amp;\cdots &amp;y^{(m)}
\end{pmatrix} \in \mathbb{R}^{1 \times m}
$$</p>

<ul>
<li>logistic regression:</li>
</ul>

<blockquote>
<p>Given <code>$x\in\mathbb{R}^{n_x}$</code>, want <code>$\hat{y}=\Pr(y=1|x)$</code></p>

<p>Output: <code>$\hat{y}=\sigma(\omega^Tx+b)$</code>, where sigmoid function <code>$\sigma(z)=\frac{1}{1+e^{-z}}$</code></p>

<ul>
<li>if <code>$z$</code> is large, <code>$\sigma(z)\approx 1$</code></li>
<li>if <code>$z$</code> is large negative number, <code>$\sigma(z)\approx 0$</code></li>
<li><code>$\sigma(0)=0.5$</code></li>
</ul>
</blockquote>

<ul>
<li>cost function:</li>
</ul>

<blockquote>
<p>Given <code>$\{x^{(i)}, y^{(i)}\}_{i=1}^m$</code>, want <code>$\hat{y}^{(i)}=\sigma(\omega^Tx^{(i)}+b)\approx y^{(i)}$</code></p>

<p>loss(error) function: <code>$\mathcal{L}(\hat{y},y)=-\left[y\log \hat{y}+(1-y)\log(1-\hat{y})\right]$</code></p>

<p>cost function: <code>$J(\omega,b)=\frac{1}{m}\sum\limits_{i=1}^m \mathcal{L}(\hat{y}^{(i)}, y^{(i)})=-\frac{1}{m}\sum\limits_{i=1}^m\left[y^{(i)}\log \hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})\right]$</code></p>
</blockquote>

<ul>
<li>gradient descent:</li>
</ul>

<blockquote>
<ul>
<li><code>$J(\omega,b)$</code> is <strong>convex</strong>!</li>
<li>algorithm:</li>
</ul>

<p>repeat{</p>

<p><code>$\omega\leftarrow \omega - \alpha\frac{\partial}{\partial\omega} J(\omega,b)$</code></p>

<p><code>$b\leftarrow b - \alpha\frac{\partial}{\partial b} J(\omega,b)$</code></p>

<p>}</p>
</blockquote>

<ul>
<li>computation graph:

<ul>
<li><strong>forward propagation</strong>: compute current loss</li>
<li><strong>back propagation</strong>: compute current gradient</li>
</ul></li>
<li>logistic regression gradient descent: update parameters</li>
</ul>

<p><strong>One training example</strong>:</p>

<blockquote>
<p><code>$z= \omega^Tx+b$</code></p>

<p><code>$\hat{y} = a = \sigma(z)$</code></p>

<p><code>$\mathcal{L}(a,y)=-(y\log a+(1-y)\log(1-a))$</code></p>

<hr />

<p><code>da</code> := <code>$\frac{d\mathcal{L}}{da}=-\frac{y}{a}+\frac{1-y}{1-a}$</code></p>

<p><code>dz</code> := <code>$\frac{d\mathcal{L}}{dz}=\frac{d\mathcal{L}}{da}\frac{da}{dz}=\left[-\frac{y}{a}+\frac{1-y}{1-a}\right]a(1-a)=a-y$</code></p>

<p><code>dwi</code> := <code>$\frac{\partial \mathcal{L}}{\partial \omega_i}=x_i\frac{d\mathcal{L}}{dz}$</code> = <code>xi*dz</code></p>

<p><code>db</code> := <code>$\frac{\partial \mathcal{L}}{\partial b}=\frac{d\mathcal{L}}{dz}$</code> = <code>dz</code></p>

<p><code>$\omega_i\leftarrow \omega_i - \alpha$</code> <code>dwi</code></p>

<p><code>$b\leftarrow b - \alpha$</code> <code>db</code></p>
</blockquote>

<p><code>$m$</code> <strong>training examples</strong>:</p>

<blockquote>
<p><code>$z^{(i)}= \omega^Tx^{(i)}+b$</code></p>

<p><code>$\hat{y}^{(i)} = a^{(i)} = \sigma(z^{(i)})$</code></p>

<p><code>$\mathcal{L}(a^{(i)},y^{(i)})=-(y^{(i)}\log a^{(i)}+(1-y^{(i)})\log(1-a^{(i)}))$</code></p>

<p><code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(a^{(i)},y^{(i)})$</code></p>

<hr />

<p><code>$\frac{\partial J}{\partial\omega_i} = \frac{1}{m}\sum_{i=1}^m\frac{\partial}{\partial\omega_i}\mathcal{L}(a^{(i)},y^{(i)})$</code></p>
</blockquote>

<p>2 <code>for</code> loops: loop over all entries (<code>$m$</code>), loop over all features  (<code>$n$</code>)</p>

<p><code>$\Rightarrow$</code> vectorization, more efficient!</p>

<p><img src="/NN/logistic.PNG" alt="scale" /></p>

<hr />

<h2 id="vectorization">Vectorization</h2>

<ul>
<li>Whenever possible, avoid explicit for-loops.</li>
</ul>

<pre><code class="language-python">def sigmoid(u):
    return 1/(1 + np.exp(-u))

# X: data, n*m, n = NO. of features, m = NO. of examples
# Y: labels, 1*m
# w: weights, n*1
# b: bias, scalar

# compute activation
A = sigmoid(np.dot(w.T, X) + b)

# cost
J = - np.mean(np.log(A) * Y +  np.log(1-A) *(1-Y))

# back propagation (compute gradient)
m = X.shape[1]
dZ = (A - Y)/m
db = np.sum(dZ)
dw = np.dot(X, dZ.T)

# update params
w -= learning_rate*dw
b -= learning_rate*db
</code></pre>

<h2 id="explanation-of-logistic-regression-cost-function">Explanation of Logistic Regression Cost Function</h2>

<p>$$
y|x \sim Binom(1, \hat{y}), \hat{y} =\sigma(\omega^Tx+b)
$$</p>

<p>$$
\Rightarrow \Pr(y|x)=\hat{y}^y(1-\hat{y})^{1-y}=\begin{cases}
1-\hat{y}, &amp;y=0\newline
\hat{y}, &amp;y=1
\end{cases}
$$</p>

<p>$$
\Rightarrow \log\Pr(y|x)=y\log\hat{y}+(1-y)\log(1-\hat{y})=-\mathcal{L}(\hat{y},y)
$$</p>

<p><strong>Goal</strong>: maximize <code>$\Pr(y|x)$</code> <code>$\Leftrightarrow$</code> minimize <code>$\mathcal{L}(\hat{y},y)$</code></p>

<hr />

<p>Cost on <code>$m$</code> examples:</p>

<p>maximize <code>$\Pr(\text{labels in training set}) = \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> maximize <code>$\log  \prod_{i=1}^n\Pr(y^{(i)}|x^{(i)})=\sum_{i=1}^m\log\Pr(y^{(i)}|x^{(i)})=-\sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)},y^{(i)})$</code></p>

<p><code>$\Leftrightarrow$</code> minimize <code>$J(\omega,b)=\frac{1}{m}\sum_{i=1}^m \mathcal{L}(\hat{y}^{(i)},y^{(i)})$</code></p>

<hr />

<h2 id="image-classification-cat-non-cat-python-code">Image Classification (cat/non-cat) - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/01-Logistic-Regression-with-a-Neural-Network-mindset/LogisticRegressionwNN.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="shallow-neural-network">Shallow Neural Network</h1>

<h2 id="forward-propagation">Forward Propagation</h2>

<ul>
<li>Neural Network representation: <code>$k$</code>th layer, with activation function <code>$g^{[k]}$</code></li>
</ul>

<table>
<thead>
<tr>
<th align="center">Parameters</th>
<th align="center">Dimension</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$W^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, n^{[0]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[1]}$</code></td>
<td align="center"><code>$(n^{[1]}, 1)$</code></td>
</tr>

<tr>
<td align="center"><code>$W^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, n^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$b^{[2]}$</code></td>
<td align="center"><code>$(n^{[2]}, 1)$</code></td>
</tr>
</tbody>
</table>

<p>$$
A^{[0]} = X, n^{[2]}=1
$$</p>

<p>$$
\text{Forward Propagation }\begin{cases}
Z^{[k]} &amp;= W^{[k]}A^{[k-1]} + b^{[k]}, (\text{linear combination}) \newline
A^{[k]}&amp;=g^{[k]}(Z^{[k]}),  (activation)
\end{cases}
\ \ k=1,2,\ldots
$$</p>

<p><img src="/NN/nn.PNG" alt="scale" /></p>

<h2 id="activation-function">Activation Function</h2>

<ul>
<li><strong>activation function</strong> can be <em>sigmoid</em>, <em>tanh</em>, <em>ReLU, Leaky ReLU</em>&hellip;

<ul>
<li>for hidden layers, <em>tanh</em> always works better than <em>sigmoid</em> 【the mean of its output is closer to zero, and so it centers the data better for the next layer】</li>
<li>for output layer of binary classification (0/1), <em>sigmoid</em> may be better</li>
<li>different layers can have different activation functions</li>
<li>ReLU is increasingly the default choice (will learn faster)</li>
</ul></li>
</ul>

<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">function</th>
<th align="center">derivative</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">sigmoid</td>
<td align="center"><code>$\frac{1}{1+e^{-z}}$</code></td>
<td align="center"><code>$g(z)(1-g(z))$</code></td>
</tr>

<tr>
<td align="center">tanh</td>
<td align="center"><code>$\frac{e^z-e^{-z}}{e^z+e^{-z}}$</code></td>
<td align="center"><code>$1-(g(z))^2$</code></td>
</tr>

<tr>
<td align="center">ReLU</td>
<td align="center"><code>$\max(0, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>

<tr>
<td align="center">Leaky ReLU</td>
<td align="center"><code>$\max(0.01z, z)$</code></td>
<td align="center"><code>$g'(z)=\begin{cases}0.01, &amp;z&lt;0\newline 1, &amp;z\geq 0\end{cases}$</code></td>
</tr>
</tbody>
</table>

<h2 id="gradient-descent">Gradient Descent</h2>

<table>
<thead>
<tr>
<th align="center">Single Training Example</th>
<th align="center">m Training Examples</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$dz^{[2]}=a^{[2]}-y$</code></td>
<td align="center"><code>$dZ^{[2]}=\frac{1}{m}(A^{[2]}-y)$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[2]}=dz^{[2]}a^{[1]T}$</code></td>
<td align="center"><code>$dW^{[2]}=dZ^{[2]} (A^{[1]})^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[2]}=dz^{[2]}$</code></td>
<td align="center"><code>$db^{[2]}=np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$da^{[1]}=(W^{[2]})^T dz^{[2]} \circ g^{[1]'}(z^{[1]})$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code></td>
</tr>

<tr>
<td align="center"><code>$dW^{[1]}=dz^{[1]}x^T$</code></td>
<td align="center"><code>$dW^{[1]}=dZ^{[1]} X^T$</code></td>
</tr>

<tr>
<td align="center"><code>$db^{[1]}=dz^{[1]}$</code></td>
<td align="center"><code>$db^{[1]}=np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="summary">Summary</h2>

<table>
<thead>
<tr>
<th align="center">Forward Propagation</th>
<th align="center">Backward Propagation</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center"><code>$Z^{[1]}=W^{[1]}X + b^{[1]}$</code></td>
<td align="center"><code>$dZ^{[2]}=\frac{1}{m}(A^{[2]}-Y)$</code></td>
</tr>

<tr>
<td align="center"><code>$A^{[1]}=g^{[1]}(Z^{[1]})$</code></td>
<td align="center"><code>$dW^{[2]}= dZ^{[2]} (A^{[1]})^T$</code><br/><code>$db^{[2]}=np.sum(dZ^{[2]}, axis = 1, keepdims = True)$</code></td>
</tr>

<tr>
<td align="center"><code>$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$</code></td>
<td align="center"><code>$dZ^{[1]}=(W^{[2]})^T dZ^{[2]} \circ g^{[1]'}(Z^{[1]})$</code>  【elementwise product】</td>
</tr>

<tr>
<td align="center"><code>$A^{[2]}=g^{[2]}(Z^{[2]})=\sigma(Z^{[2]})$</code></td>
<td align="center"><code>$dW^{[1]}= dZ^{[1]} X^T$</code><br/><code>$db^{[1]}=np.sum(dZ^{[1]}, axis = 1, keepdims = True)$</code></td>
</tr>
</tbody>
</table>

<h2 id="random-initialization">Random Initialization</h2>

<ul>
<li><strong>If initialize weights and biases with 0</strong>: Each neuron in the first hidden layer will perform the same computation. So even after multiple iterations of gradient descent each neuron in the layer will be computing the same thing as other neurons.</li>
<li><strong>If initialize weights to relative large values</strong> ( using the tanh activation for all the hidden units): the inputs of the <code>tanh</code> to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.</li>
</ul>

<hr />

<h2 id="planar-data-classification-python-code">Planar Data Classification - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/02-Planar-Data-Classification-with-One-Hidden-Layer/planarDataClassification.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="deep-neural-network">Deep Neural Network</h1>

<h2 id="notations">Notations</h2>

<p><img src="/NN/deep.PNG" alt="shallow and deep" /></p>

<ul>
<li><code>$L$</code>: # of layers</li>
<li><code>$n^{[\ell]}$</code>: # of units in layer <code>$\ell$</code></li>
<li><code>$a^{[\ell]}$</code>: activations in layer <code>$\ell$</code>

<ul>
<li><code>$a^{[\ell]}=g^{[\ell]}(z^{[\ell]})$</code></li>
</ul></li>
<li><code>$w^{[\ell]}, b^{[\ell]}$</code>: weights and bias for computing <code>$z^{[\ell]}$</code></li>
</ul>

<hr />

<h2 id="forward-propagation-1">Forward Propagation</h2>

<p>Layer <code>$\ell$</code>:</p>

<ul>
<li><p><code>$a^{[0]}:=x$</code></p></li>

<li><p><code>$z^{[\ell]}=W^{[\ell]}a^{[\ell-1]}+b^{[\ell]}$</code></p></li>

<li><p><code>$a^{[\ell]}=g^{[\ell]}(z^{[\ell]})$</code></p></li>
</ul>

<p>(<strong><u>Vectorized</u></strong>) layer <code>$\ell$</code>:</p>

<ul>
<li><p><code>$A^{[0]}:=X$</code></p></li>

<li><p><code>$Z^{[\ell]}=W^{[\ell]}A^{[\ell-1]}+b^{[\ell]}$</code> (numpy broadcasting)</p></li>

<li><p><code>$A^{[\ell]}=g^{[\ell]}(Z^{[\ell]})$</code></p></li>
</ul>

<hr />

<h2 id="check-dimensions">Check Dimensions</h2>

<ul>
<li>shape of <code>$W^{[\ell]}$</code> and <code>$dW^{[\ell]}$</code>: <code>$(n^{[\ell]}, n^{[\ell-1]})$</code></li>
<li>shape of <code>$b^{[\ell]}$</code> and <code>$db^{[\ell]}$</code>: <code>$(n^{[\ell]}, 1)$</code></li>
<li>shape of <code>$Z^{[\ell]}, A^{[\ell]}, dZ^{[\ell]}, dA^{[\ell]}$</code>: <code>$(n^{[\ell]},m)$</code></li>
</ul>

<hr />

<h2 id="forward-and-backward-functions">Forward and Backward Functions</h2>

<p>layer <code>$\ell$</code></p>

<ul>
<li>Forward:

<ul>
<li>input: <code>$a^{[\ell-1]}$</code></li>
<li>output: <code>$a^{[\ell]}$</code>, cache <code>$z^{[\ell]}$</code></li>
</ul></li>
<li>Backward:

<ul>
<li>input: <code>$da^{[\ell]}$</code></li>
<li>output: <code>$da^{[\ell-1]}, dW^{[\ell]}, db^{[\ell]}$</code></li>
<li>computation:</li>
</ul></li>
</ul>

<p>$$
dz^{[\ell]} = da^{[\ell]} * (g^{[\ell]})&lsquo;(z^{[\ell]}) \text{ (elementwise product)}\\<br />
dW^{[\ell]} = dz^{[\ell]}a^{[\ell-1]}\\<br />
db^{[\ell]}=dz^{[\ell]}\\<br />
da^{[\ell-1]}=W^{[\ell]T}dz^{[\ell]} \Rightarrow dz^{[\ell]} =W^{[\ell+1]T}dz^{[\ell+1]} * (g^{[\ell]})&lsquo;(z^{[\ell]})
$$</p>

<p>$$
\text{Vectorized}: dZ^{[\ell]} = dA^{[\ell]} * (g^{[\ell]})&lsquo;(Z^{[\ell]}) \text{ (elementwise product)}\\<br />
dW^{[\ell]} = dZ^{[\ell]}A^{[\ell-1]T}\\<br />
db^{[\ell]}= np.sum(dZ^{[\ell]}, axis=1,keepdims=True)\\<br />
dA^{[\ell-1]}=W^{[\ell]T}dZ^{[\ell]}
$$</p>

<p><img src="/NN/compute.PNG" alt="input and output" /></p>

<hr />

<h2 id="parameters-and-hyperparameters">Parameters and Hyperparameters</h2>

<ul>
<li>parameters: <code>$W^{[\ell]}, b^{[\ell]}$</code></li>
<li>hyperparameters:

<ul>
<li>learning rate <code>$\alpha$</code></li>
<li># of iterations</li>
<li># of hidden layers <code>$L$</code></li>
<li># of hidden units <code>$n^{[\ell]}$</code></li>
<li>choice of activation function</li>
<li>momentum</li>
<li>minibatch size</li>
<li>regularizations</li>
</ul></li>
</ul>

<hr />

<h2 id="build-deep-neural-network-image-classification-cat-non-cat-python-code">Build Deep Neural Network - Image Classification (cat/non-cat) - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/03-Build-Deep-Neural-Network/Building%20Deep%20Neural%20Network.ipynb">Jupyter Notebook</a></p>

<hr />

<h1 id="practical-aspects-of-deep-learning">Practical Aspects of Deep Learning</h1>

<ul>
<li>train/dev/test sets

<ul>
<li>traditional rules of thumb ratio: 60/20/20</li>
<li>big data set: e.g. 98/1/1, 99.5/.4/.1</li>
</ul></li>
<li>mismatched train/test distribution

<ul>
<li><strong><u>guideline</u></strong>: make sure <strong>dev</strong> and <strong>test</strong> come from same distribution</li>
</ul></li>
<li>not having a test set is ok (only dev set)</li>
<li>bias/variance <code>$\Rightarrow$</code> <strong>Basic Recipe</strong>:

<ul>
<li>high bias? <code>$\rightarrow$</code>  <u>bigger network</u>, train longer, NN architecture search</li>
<li>high variance? <code>$\rightarrow$</code> <u>get more data</u>, regularization, NN architecture search</li>
</ul></li>
<li><strong><u>no more</u></strong> bias/variance tradeoff:

<ul>
<li>pre-deep-learning era: can not just reduce bias or variance without hurting the other</li>
<li>deep-learning big-data era: a bigger network almost always reduces bias without necessarily hurting the variance so long as you regularize properly; getting more data almost always reduces variance without hurting bias much</li>
</ul></li>
</ul>

<hr />

<h2 id="regularization">Regularization</h2>

<ul>
<li>reduce variance?

<ul>
<li>more data: expensive</li>
<li>use regularization,  prevent overfitting</li>
</ul></li>
</ul>

<hr />

<h3 id="l2-regularization">L2 Regularization</h3>

<ul>
<li>Cost function</li>
</ul>

<p>$$
J(W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]})=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(\hat{y}^{(i)}, y^{(i)})+\frac{\lambda}{2m}\sum_{\ell=1}^L\Vert W^{[\ell]}\Vert_F^2
$$</p>

<p>where <code>$\Vert\cdot\Vert_F$</code> is the Frobenius norm.</p>

<ul>
<li><code>$dW^{[\ell]}=$</code> (from backprop) <code>$+ \frac{\lambda}{m}W^{[\ell]}$</code>

<ul>
<li>update: <code>$W^{[\ell]} := W^{[\ell]} -\alpha dW^{[\ell]}$</code></li>
<li><code>$\Rightarrow W^{[\ell]}:=(1-\frac{\alpha\lambda}{m})W^{[\ell]}-\alpha$</code>(from backprop)</li>
<li>L2 regularization = weight decay</li>
</ul></li>
<li>why regularization reduces overfitting?

<ul>
<li>large <code>$\lambda$</code> <code>$\Rightarrow$</code>  <code>$W^{[\ell]}\approx 0$</code> <code>$\Rightarrow$</code>  &ldquo;simpler&rdquo; network (not zero out hidden units, but some of them have a smaller effect)</li>
<li>e.g., activation <code>$g = \tanh$</code>, large <code>$\lambda$</code> <code>$\Rightarrow$</code>  small <code>$W^{[\ell]}$</code> <code>$\Rightarrow$</code> <code>$z^{[\ell]}=W^{[\ell]}a^{[\ell-1]}+b^{[\ell]}$</code> small <code>$\Rightarrow$</code> <code>$g(z)\approx$</code> linear function <code>$\Rightarrow$</code> nearly linear decision boundary</li>
</ul></li>
</ul>

<p><img src="/NN/tanh.PNG" alt="tanh" /></p>

<ul>
<li>debugging: with regularization, cost <code>$J$</code> <strong><u>will not</u></strong> decease monotonically with # of iterations</li>
</ul>

<hr />

<h3 id="dropout-regularization">Dropout Regularization</h3>

<ul>
<li>Implement Dropout (&rdquo;<strong><u>Inverted Dropout</u></strong>&rdquo;)</li>
</ul>

<blockquote>
<p>Illustrate with layer <code>$\ell=3$</code></p>

<pre><code class="language-python">keepProb = 0.8 # probability a unit will be kept
d3 = np.random.rand(a3.shape) &lt; keepProb
a3 *= d3 # elementwise product
a3 /= keepProb # ensure the expected value of a3 remains the same
</code></pre>
</blockquote>

<ul>
<li>make predictions at test time: <strong>no dropout</strong></li>
<li>why does dropout work?

<ul>
<li><strong>intuition</strong>: cannot rely on any one feature, so have to spread out weights</li>
<li>shrink weights (similar to L2)</li>
</ul></li>
<li>vary <code>keepProb</code> by layer</li>
<li>cost function is no longer well-defined</li>
</ul>

<hr />

<h3 id="other-regularization-methods">Other Regularization Methods</h3>

<ul>
<li><strong>Data augmentation</strong>: e.g. image flip/zoom-in/rotate/distortion</li>
<li><strong>Early stopping</strong>:

<ul>
<li>cons: can no longer work on 2 steps of orthogonalization independently</li>
<li>alternative: L2 regularization (cons: more computationally expensive)</li>
</ul></li>
</ul>

<blockquote>
<p><strong>Orthogonalization</strong>:</p>

<ol>
<li>optimize cost function: gradient descent, &hellip;</li>
<li>not overfit: regularization, &hellip;</li>
</ol>
</blockquote>

<p><img src="/NN/early.PNG" alt="early stopping" /></p>

<hr />

<h2 id="speed-up-training">Speed Up Training</h2>

<ul>
<li><strong>normalizing training sets</strong>

<ul>
<li>make sure all features are on similar scale, thus making cost function easier and faster to optimize</li>
<li>use the same mean/sd to normalized the test set</li>
</ul></li>
</ul>

<p><img src="/NN/normal.PNG" alt="normalization" /></p>

<ul>
<li>weight initialization for deep networks

<ul>
<li>partial solution to <strong>vanishing/exploding gradients</strong></li>
<li>use <strong>Xavier initialization</strong>, &hellip;</li>
<li>can also be tune as hyperparameter</li>
</ul></li>
</ul>

<blockquote>
<p><code>$z=w_1x_1 +w_2x_2 +\cdots + w_nx_n$</code> (omit bias)</p>

<p>set <code>$var(w_i)=\frac{1}{n}$</code> (<code>$n=$</code> # of input nodes)</p>

<pre><code class="language-python">W = np.random.randn(node_in, node_out) / np.sqrt(node_in)
# relu activation
# W = np.random.randn(node_in, node_out) / np.sqrt(node_in/2)
# tanh activation
# W = np.random.randn(node_in, node_out) / np.sqrt(node_in)
# W = np.random.randn(node_in, node_out) / np.sqrt((node_in + node_out)/2)
</code></pre>
</blockquote>

<h2 id="debugging-of-backpropagation-gradient-checking">Debugging of Backpropagation: gradient checking</h2>

<ul>
<li>take <code>$W^{[1]},b^{[1]},\ldots,W^{[L]},b^{[L]}$</code> and reshape into a big vector <code>$\theta$</code></li>
<li>take <code>$dW^{[1]},db^{[1]},\ldots,dW^{[L]},db^{[L]}$</code> and reshape into a big vector <code>$d\theta$</code></li>
<li>grad check:</li>
</ul>

<blockquote>
<p>for each <code>$i$</code>:
$$
d\theta_{approx}[i]:=\frac{J(\theta_1,\theta_2,\ldots,\theta_i+\epsilon,\ldots)-J(\theta_1,\theta_2,\ldots,\theta_i-\epsilon,\ldots)}{2\epsilon}\approx \frac{\partial J}{\partial \theta_i}
$$</p>

<p>check
$$
\frac{\Vert d\theta_{approx}-d\theta\Vert_2}{\Vert d\theta_{approx}\Vert_2+\Vert d\theta\Vert_2} \approx 0, e.g. &lt; 1e-7
$$</p>
</blockquote>

<ul>
<li>do not use in training - only to debug</li>
<li>if algorithm fails grad check, look at components to identify bug</li>
<li>remember regularization term</li>
<li>grad check does not work with dropout

<ul>
<li>implement grad check without dropout (turn off dropout, <code>keepProb = 1</code>)</li>
</ul></li>
<li>run at random initialization; perhaps again after some training</li>
</ul>

<hr />

<h2 id="python-code">Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/04-Practical-Aspects-of-Deep-Learning/initialization/Initialization.ipynb">Initialization</a></p>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/04-Practical-Aspects-of-Deep-Learning/regularization/Regularization.ipynb">Regularization</a></p>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/04-Practical-Aspects-of-Deep-Learning/gradient-checking/Gradient%20Checking.ipynb">Gradient Checking</a></p>

<hr />

<h1 id="optimization-algorithms">Optimization Algorithms</h1>

<h2 id="mini-batch-gradient-descent">Mini-Batch Gradient Descent</h2>

<ul>
<li>batch vs mini-batch</li>
<li>mini-batch <code>$X^{\{t\}}, y^{\{t\}}$</code></li>
<li>mini-batch gradient descent</li>
</ul>

<p><img src="/NN/mini.PNG" alt="mini batch" /></p>

<p><img src="/NN/minigd.PNG" alt="mini batch cost" /></p>

<ul>
<li>choose mini-batch size: hyperparameter

<ul>
<li>size = <code>$m$</code>: batch gradient descent</li>
<li>size = 1: <strong>stochastic gradient descent</strong>, every example is a mini-batch</li>
<li>in practice, size between 1 and <code>$m$</code>

<ul>
<li>batch gradient descent: too long per iteration</li>
<li>gradient gradient descent: lose speedup from vectorization</li>
</ul></li>
<li>small training set (<code>$m\leq 2000$</code>): use batch gradient descent</li>
<li>typical mini-batch size: 64,128,256,512</li>
<li>make sure mini-batch fits in CPU/GPU memory</li>
</ul></li>
</ul>

<p><img src="/NN/sgd.PNG" alt="stochastic gradient descent" /></p>

<hr />

<h2 id="momentum">Momentum</h2>

<h3 id="exponentially-weighted-averages">Exponentially Weighted Averages</h3>

<p>$$
v_0=0, v_t = \beta v_{t-1}+(1-\beta)\theta_t
$$</p>

<p>$$
\Rightarrow v_t = \beta^tv_0 +(1-\beta)\left[ \theta_t + \beta\theta_{t-1} +\ldots \beta^{t-1}\theta_1  \right]=(1-\beta)\sum_{i=0}^{t-1}\beta^i\theta_{t-i}
$$</p>

<p>Since
$$
\beta^{\frac{1}{1-\beta}}=\left(1-(1-\beta)\right)^{\frac{1}{1-\beta}}\approx \frac{1}{e},
$$
<code>$v_t\approx$</code> average of the last <code>$\frac{1}{1-\beta}$</code> terms of <code>$\theta$</code>&rsquo;s</p>

<blockquote>
<p><code>$v_{\theta}:=0$</code></p>

<p>repeat{</p>

<p>get next <code>$\theta_t$</code></p>

<p><code>$v_{\theta}:=\beta v_{\theta} + (1-\beta)\theta_t$</code></p>

<p>}</p>
</blockquote>

<hr />

<h3 id="bias-correction-in-exponentially-weighted-averages">Bias Correction in Exponentially Weighted Averages</h3>

<p>Take <code>$v_t := v_t/(1-\beta^t)$</code></p>

<p><img src="/NN/bias.PNG" alt="bias correction" /></p>

<ul>
<li>decrease <code>$\beta$</code>:

<ul>
<li>more oscillation</li>
<li>shift the line to the left</li>
</ul></li>
</ul>

<hr />

<h3 id="gradient-descent-with-momentum">Gradient Descent With Momentum</h3>

<blockquote>
<p>on iteration <code>$t$</code></p>

<ul>
<li>compute <code>$dw,db$</code> on current mini-batch</li>
<li><code>$v_{dw} = \beta v_{dw}+(1-\beta)dw$</code> (<code>$v_{dw}$</code>: velocity, <code>$dw$</code>: acceleration, <code>$\beta$</code>: friction)</li>
<li><code>$v_{db} = \beta v_{db}+(1-\beta)db$</code></li>
<li>update: <code>$w := w- \alpha v_{dw}, b :=b- \alpha v_{db}$</code></li>
</ul>
</blockquote>

<ul>
<li>hyperparameters: <code>$\alpha, \beta=0.9$</code></li>
<li>no need to do bias correction</li>
</ul>

<p><img src="/NN/momentum.PNG" alt="momentum" /></p>

<p><img src="/NN/momentum2.PNG" alt="momentum" /></p>

<ol>
<li>gradient descent</li>
<li>with momentum (small <code>$\beta$</code>)</li>
<li>with momentum (large <code>$\beta$</code>)</li>
</ol>

<hr />

<h2 id="root-mean-square-prop-rms-prop">Root Mean Square Prop (RMS Prop)</h2>

<blockquote>
<p>on iteration <code>$t$</code></p>

<ul>
<li>compute <code>$dw,db$</code> on current mini-batch</li>
<li><code>$s_{dw} = \beta s_{dw}+(1-\beta)dw^2$</code>  (elementwise square)</li>
<li><code>$s_{db} = \beta s_{db}+(1-\beta)db^2$</code></li>
<li>update: <code>$w := w- \alpha \frac{dw}{\sqrt{s_{dw}}+\epsilon}, b :=b- \alpha \frac{db}{\sqrt{s_{db}}+\epsilon}$</code></li>
</ul>
</blockquote>

<ul>
<li>then can use a large learning rate (faster learning), without diverging gradients</li>
<li><code>$\epsilon$</code>: avoid divide by 0</li>
</ul>

<hr />

<h2 id="adam-optimization-algorithm">Adam Optimization Algorithm</h2>

<ul>
<li>putting it together: momentum + RMS</li>
<li>adam = adaptive moment estimation</li>
</ul>

<blockquote>
<p><code>$v_{dw}=0, s_{dw}=0,v_{db}=0,s_{db}=0$</code></p>

<p>on iteration <code>$t$</code></p>

<ul>
<li>compute <code>$dw,db$</code> on current mini-batch</li>
<li>momentum: <code>$v_{dw} = \beta_1 v_{dw}+(1-\beta_1)dw$</code>, <code>$v_{db} = \beta_1 v_{db}+(1-\beta_1)db$</code></li>
<li>rms: <code>$s_{dw} = \beta_2 s_{dw}+(1-\beta_2)dw^2$</code>  , <code>$s_{db} = \beta_2 s_{db}+(1-\beta_2)db^2$</code></li>
<li>bias correction:

<ul>
<li><code>$v_{dw}^{corrected}=\frac{v_{dw}}{1-\beta_1^t}, v_{db}^{corrected}=\frac{v_{db}}{1-\beta_1^t}$</code></li>
<li><code>$s_{dw}^{corrected}=\frac{s_{dw}}{1-\beta_2^t}, s_{db}^{corrected}=\frac{s_{db}}{1-\beta_2^t}$</code></li>
</ul></li>
<li>update: <code>$w := w- \alpha \frac{v_{dw}^{corrected} }{\sqrt{s^{corrected}_{dw}}+\epsilon}, b :=b- \alpha \frac{v^{corrected}_{db}}{\sqrt{s^{corrected}_{db}}+\epsilon}$</code></li>
</ul>
</blockquote>

<ul>
<li>hyperparameters:

<ul>
<li><code>$\alpha$</code>: needs to tune</li>
<li><code>$\beta_1$</code>: 0.9 (default)</li>
<li><code>$\beta_2$</code>: 0.99 (default)</li>
<li><code>$\epsilon$</code>: 1e-8 (default)</li>
</ul></li>
</ul>

<hr />

<h2 id="learning-rate-decay">Learning Rate Decay</h2>

<ul>
<li>slowly reduce learning rate</li>
<li>1 epoch = 1 pass thru the data</li>
<li>set</li>
</ul>

<p>$$
\alpha =\frac{1}{1+\text{decay-rate}\times \text{epoch-num}}\alpha_0
$$</p>

<ul>
<li>other learning rate decay methods

<ul>
<li>exponentially decay: <code>$\alpha=0.95^{\text{epoch_num}}\alpha_0$</code></li>
<li><code>$\alpha=\frac{k}{\sqrt{\text{epoch_num}}}\alpha_0$</code> or <code>$\alpha=\frac{k}{\sqrt{t}}\alpha_0$</code> (<code>$t=$</code> mini-batch number)</li>
<li>discrete staircase</li>
<li>manual decay</li>
</ul></li>
</ul>

<hr />

<h2 id="local-optima-in-neural-networks">Local Optima in Neural Networks</h2>

<ul>
<li>unlikely to get stuck in a bad local optima

<ul>
<li>in nn, most point of 0 gradients are not local optima, but saddle points</li>
</ul></li>
</ul>

<p><img src="/NN/saddle.PNG" alt="saddle" /></p>

<ul>
<li>plateaus can make learning slow</li>
</ul>

<p><img src="/NN/plateau.PNG" alt="plateau" /></p>

<hr />

<h2 id="python-code-optimization-algorithms">Python Code - Optimization Algorithms</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/05-Optimization-Algorithms/Optimization%20methods.ipynb">jupyter notebook</a></p>

<hr />

<h1 id="tuning-process">Tuning Process</h1>

<h2 id="hyperparameters">Hyperparameters</h2>

<ul>
<li>learning rate <code>$\alpha$</code></li>
<li>momentum <code>$\beta=0.9$</code>, # of hidden units, mini-batch size</li>
<li># of layers, learning rate decay</li>
<li>adam hyperparameters <code>$\beta_1=0.9, \beta_2=0.99, \epsilon=1e-8$</code></li>
</ul>

<hr />

<ul>
<li>try random values, do not use a grid</li>
<li>coarse to find</li>
<li>appropriate scale for hyperparameters

<ul>
<li>e.g. use <strong>log-scale</strong> to sample (sample more densely than linear-scale) learning rate <code>$\alpha$</code>, <code>$1-\beta$</code></li>
</ul></li>
</ul>

<hr />

<h2 id="hyperparameters-tuning-in-practice">Hyperparameters Tuning in Practice</h2>

<p><img src="/NN/panda.PNG" alt="panda" /></p>

<h2 id="batch-normalization">Batch Normalization</h2>

<ul>
<li>normalizing input features can speed up learning</li>
<li>batch norm: normalize hidden units</li>
</ul>

<p><img src="/NN/batch.PNG" alt="batch norm" /></p>

<p><img src="/NN/batch2.PNG" alt="batch norm" /></p>

<ul>
<li>implementing gradient descent</li>
</ul>

<blockquote>
<p>for <code>$t=1,2,\ldots, $</code> numMiniBatch</p>

<ul>
<li>compute forward prop on <code>$X^{\{t\}}$</code></li>
<li>in each hidden layer, use BN to replace <code>$Z^{[\ell]}$</code> with <code>$\tilde{Z}^{[\ell]}$</code></li>
<li>use back prop to compute <code>$dW^{[\ell]}, d\beta^{[\ell]}, d\gamma^{[\ell]}$</code></li>
<li>update parameters (momentum, RMS prop, adam)</li>
</ul>
</blockquote>

<hr />

<ul>
<li>batch norm reduces <strong>covariate shift</strong></li>
<li>batch norm has slight regularization effect</li>
<li>batch norm at test time:

<ul>
<li>exponentially weighted averages of <code>$\mu$</code> and <code>$\sigma^2$</code> across mini-batch</li>
</ul></li>
</ul>

<hr />

<h1 id="multiclass-classification">Multiclass Classification</h1>

<h2 id="softmax-regression">Softmax Regression</h2>

<ul>
<li><code>$C$</code> = # of classes</li>
<li><code>$n^{[L]}=C$</code></li>
<li>softmax layer:</li>
</ul>

<p>$$
\text{activation: } t = e^{z^{[L]}},\ a^{[L]}=\frac{e^{z^{[L]}}}{\sum_{j=1}^C t_i}
$$</p>

<ul>
<li>loss function:</li>
</ul>

<p>$$
\mathcal{L}(y,\hat{y})=-\sum_{j=1}^Cy_j\log(\hat{y}_j)
$$</p>

<ul>
<li>cost on the entire training set:</li>
</ul>

<p>$$
J=\frac{1}{m}\sum_{i=1}^m\mathcal{L}(y^{(i)},\hat{y}^{(i)})
$$</p>

<ul>
<li>gradient descent with softmax:</li>
</ul>

<p>$$
dZ^{[L]}:=\frac{\partial J}{\partial Z^{[L]}}=\hat{y}-y
$$</p>

<hr />

<h1 id="deep-learning-frameworks">Deep Learning Frameworks</h1>

<ul>
<li>Caffe/Caffe2</li>
<li>CNTK</li>
<li>DL4J</li>
<li>Keras</li>
<li>Lasagne</li>
<li>mxnet</li>
<li>PaddlePaddle</li>
<li>Tensorflow</li>
<li>Theano</li>
<li>Torch</li>
</ul>

<h2 id="choosing-deep-learning-frameworks">Choosing Deep Learning Frameworks</h2>

<ul>
<li>ease of programming (development and deployment)</li>
<li>running speed</li>
<li>truly open (open source with good governance)</li>
</ul>

<hr />

<h1 id="ml-strategy">ML Strategy</h1>

<h2 id="single-number-evaluation-metric">Single Number Evaluation Metric</h2>

<ul>
<li>precision = PPV = TP/(TP + FP) = 1-FDR</li>
<li>recall = TPR = TP/P = TP/(TP + FN)</li>
<li><strong>F1 score</strong> = harmonic mean of precision and recall</li>
</ul>

<hr />

<h2 id="satisfying-and-optimizing-metric">Satisfying and Optimizing Metric</h2>

<ul>
<li><strong>accuracy</strong> vs <strong>running time</strong>

<ul>
<li>cost = accuracy - 0.5 * running time</li>
<li>maximize accuracy s.t. running time <code>$\leq$</code> 100ms</li>
<li>accuracy: <strong>optimizing</strong></li>
<li>running time: <strong>satisfying</strong></li>
</ul></li>
<li>N metrics:

<ul>
<li>pick 1: optimizing</li>
<li>others: satisfying (threshold)</li>
<li>e.g. maximize accuracy s.t. false positives <code>$\leq$</code> threshold</li>
</ul></li>
</ul>

<hr />

<h2 id="train-dev-test-distributions">Train/Dev/Test Distributions</h2>

<ul>
<li>guideline: choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on</li>
<li>dev set and test set should come from the same distribution</li>
</ul>

<hr />

<h2 id="size-of-the-dev-and-test-sets">Size of the Dev and Test Sets</h2>

<ul>
<li>old way of splitting data:

<ul>
<li>train/test = <sup>70</sup>&frasl;<sub>30</sub></li>
<li>train/dev/test = 60/20/20</li>
</ul></li>
<li>deep learning

<ul>
<li>train/dev/test = 98/1/1</li>
<li>set your test set to be big enough to give high confidence in the overall performance of your system</li>
<li>not having a test set is ok, train/dev</li>
</ul></li>
</ul>

<hr />

<h2 id="when-to-change-dev-test-sets-and-metrics">When to Change Dev/Test Sets and Metrics</h2>

<p><img src="/NN/cat.PNG" alt="cat" /></p>

<p><img src="/NN/cat2.PNG" alt="cat" /></p>

<p><img src="/NN/cat3.PNG" alt="cat" /></p>

<hr />

<h2 id="comparing-to-human-level-performance">Comparing to Human-Level Performance</h2>

<ul>
<li>Bayes optimal error</li>
<li>human-level performance might not be far from Bayes error</li>
<li>human-level error as a proxy for Bayes error</li>
<li>avoidable/unavoidable error</li>
<li>difference between human-level performance &amp; training error: <strong>avoidable bias</strong></li>
<li>difference between training error &amp; dev error: <strong>variance</strong></li>
</ul>

<p><img src="/NN/bayes.PNG" alt="bayes" /></p>

<p><img src="/NN/human.PNG" alt="human" /></p>

<ul>
<li>surpassing human-level performance: (structured data, not natural perception/ lots of data, pattern)

<ul>
<li>online advertising</li>
<li>product recommendations</li>
<li>logistics (predicting transit time)</li>
<li>loan approvals</li>
<li>speech recognition</li>
<li>some image recognition</li>
<li>medical: ECG, &hellip;.</li>
</ul></li>
</ul>

<hr />

<ul>
<li>two fundamental assumptions of supervised learning

<ul>
<li>you can fit the training set pretty well (avoidable bias)</li>
<li>the training set performance generalized pretty well to dev/test set (variance)</li>
</ul></li>
<li>reduce <strong>avoidable bias</strong>

<ul>
<li>train bigger model</li>
<li>train longer/ better optimization algo (momentum, RMS prop, Adam)</li>
<li>NN architecture (CNN, RNN)/ hyperpatameters search</li>
</ul></li>
<li>reduce <strong>variance</strong>

<ul>
<li>more data</li>
<li>regularization (L2, dropout, data augmentation)</li>
<li>NN architecture (CNN, RNN)/ hyperpatameters search</li>
</ul></li>
</ul>

<hr />

<h1 id="error-analysis">Error Analysis</h1>

<p><img src="/NN/erroranalysis.PNG" alt="error analysis" /></p>

<p><img src="/NN/erroranalysis2.PNG" alt="error analysis" /></p>

<hr />

<h2 id="cleaning-up-incorrectly-labeled-data">Cleaning Up Incorrectly Labeled Data</h2>

<ul>
<li>DL algorithms are quite robust to random errors in the training set</li>
<li>incorrectly labeled data in the dev/test set: add one col in error analysis</li>
</ul>

<p><img src="/NN/erroranalysis3.PNG" alt="error analysis" /></p>

<hr />

<p><strong>Correcting in correct dev/test set examples</strong></p>

<ul>
<li>apply same process to dev and test sets to make sure they continue to come from the same distribution</li>
<li>consider examining examples your algo got right as well as ones it got wrong</li>
<li>training and dev/test data may now come from slightly different distributions</li>
</ul>

<hr />

<h1 id="mismatched-training-and-dev-test-set">Mismatched Training and Dev/Test Set</h1>

<p><img src="/NN/option.PNG" alt="option" /></p>

<ul>
<li><u>data mismatch problem</u> vs <u>variance problem</u>?

<ul>
<li><strong>training-dev set</strong>: same distribution as training set, but not used for training</li>
</ul></li>
</ul>

<p><img src="/NN/mismatch.PNG" alt="mismatch" /></p>

<ol>
<li>training error - human-level error = avoidable error</li>
<li>training-dev error - training error  = variance</li>
<li>dev error - training error = data mismatch</li>
<li>test error - dev error = degree of overfitting to dev set (larger dev set)</li>
</ol>

<p><img src="/NN/mismatch2.PNG" alt="mismatch" /></p>

<hr />

<h2 id="addressing-data-mismatch">Addressing Data Mismatch</h2>

<ul>
<li>collect more data similar to dev/test sets</li>
<li>artificial synthesized data (e.g. speech recognition)</li>
</ul>

<hr />

<h1 id="transfer-learning">Transfer Learning</h1>

<p><img src="/NN/transfer2.PNG" alt="transfer learning" /></p>

<p>When transfer learning makes sense (Task A <code>$\rightarrow$</code> Task B)</p>

<ul>
<li>Task A and B have the same input</li>
<li>You have a lot more data for Task A than Task B</li>
<li>Low level features from A could be helpful for learning B</li>
</ul>

<hr />

<h1 id="multi-task-learning">Multi-task Learning</h1>

<p><img src="/NN/multi.PNG" alt="multi task" /></p>

<p>When multi-task learning makes sense</p>

<ul>
<li>training on a set of tasks that could benefit from having shared lower-level features</li>
<li>usually: amount of data you have for each task is quite similar</li>
<li>can train a big enough nn to do well on all the tasks</li>
</ul>

<hr />

<h1 id="end-to-end-deep-learning">End-to-End Deep Learning</h1>

<p>pros of end-to-end deep learning</p>

<ul>
<li>let the data speak</li>
<li>less hand-designing of components needed</li>
</ul>

<p>cons</p>

<ul>
<li>may need large amount of data</li>
<li>excludes potentially useful hand-designed components</li>
</ul>

<p><img src="/NN/end-to-end.PNG" alt="end to end" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

