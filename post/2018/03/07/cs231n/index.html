<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CS231n-CNN for Visual Recognition-Assignment1 | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">CS231n-CNN for Visual Recognition-Assignment1</span></h1>

<h2 class="date">2018/03/07</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> 
  
  
  
  Tags: <a href="/tags/knn">knn</a> <a href="/tags/svm">svm</a> <a href="/tags/softmax">softmax</a> <a href="/tags/cnn">cnn</a> <a href="/tags/imageclassification">imageClassification</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#image-classification">Image Classification</a>
<ul>
<li><a href="#challenges">Challenges</a></li>
<li><a href="#data-driven-approach">Data-Driven Approach</a></li>
<li><a href="#image-classification-pipeline">Image Classification Pipeline</a></li>
<li><a href="#example-image-classification-dataset-cifar-10">Example Image Classification Dataset: CIFAR-10</a></li>
</ul></li>
<li><a href="#knn">kNN</a>
<ul>
<li><a href="#load-data">Load Data</a></li>
<li><a href="#train-and-evaluate-a-classifier">Train and Evaluate a Classifier</a></li>
<li><a href="#implementation-of-knn-classifier-with-l2-distance">Implementation of kNN Classifier with L2 Distance</a>
<ul>
<li><a href="#vectorization-of-distance-computation">Vectorization of Distance Computation</a></li>
</ul></li>
<li><a href="#hyperparameter-tuning">Hyperparameter Tuning</a></li>
<li><a href="#pros-and-cons-of-knn">Pros and Cons of kNN</a></li>
<li><a href="#python-code">Python Code</a></li>
<li><a href="#knn-in-practice-never-used-on-images">kNN in Practice (Never Used on Images)</a></li>
</ul></li>
<li><a href="#linear-classification">Linear Classification</a>
<ul>
<li><a href="#multiclass-svm">Multiclass SVM</a></li>
<li><a href="#softmax">Softmax</a></li>
</ul></li>
<li><a href="#two-layer-neural-network">Two-layer Neural Network</a></li>
</ul>
</nav>


<main>


<h1 id="image-classification">Image Classification</h1>

<h2 id="challenges">Challenges</h2>

<p><img src="/cs231n/challenges.jpeg" alt="challenges" /></p>

<ul>
<li><strong>Viewpoint variation</strong>. A single instance of an object can be oriented in many ways with respect to the camera.</li>
<li><strong>Scale variation</strong>. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image).</li>
<li><strong>Deformation</strong>. Many objects of interest are not rigid bodies and can be deformed in extreme ways.</li>
<li><strong>Occlusion</strong>. The objects of interest can be occluded. Sometimes only a small portion of an object (as little as few pixels) could be visible.</li>
<li><strong>Illumination conditions</strong>. The effects of illumination are drastic on the pixel level.</li>
<li><strong>Background clutter</strong>. The objects of interest may <em>blend</em> into their environment, making them hard to identify.</li>
<li><strong>Intra-class variation</strong>. The classes of interest can often be relatively broad, such as <em>chair</em>. There are many different types of these objects, each with their own appearance.</li>
</ul>

<hr />

<h2 id="data-driven-approach">Data-Driven Approach</h2>

<ul>
<li>first accumulating a <em>training dataset</em> of labeled images</li>
<li>then develop learning algorithms that look at these examples and learn about the visual appearance of each class</li>
</ul>

<hr />

<h2 id="image-classification-pipeline">Image Classification Pipeline</h2>

<ul>
<li><strong>Input:</strong> Our input consists of a set of <em>N</em> images, each labeled with one of <em>K</em> different classes. We refer to this data as the <em>training set</em>.</li>
<li><strong>Learning:</strong> Our task is to use the training set to learn what every one of the classes looks like. We refer to this step as <em>training a classifier</em>, or <em>learning a model</em>.</li>
<li><strong>Evaluation:</strong> In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it has never seen before. We will then compare the true labels of these images to the ones predicted by the classifier. Intuitively, we’re hoping that a lot of the predictions match up with the true answers  (which we call the <em>ground truth</em>).</li>
</ul>

<hr />

<h2 id="example-image-classification-dataset-cifar-10">Example Image Classification Dataset: CIFAR-10</h2>

<ul>
<li><a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 dataset</a></li>
<li>60,000 tiny images that are 32 pixels high and wide</li>
<li>Each image is labeled with one of 10 classes (for example <em>“airplane, automobile, bird, etc”</em>)</li>
<li>These 60,000 images are partitioned into a training set of 50,000 images and a test set of 10,000 images.</li>
<li><a href="https://www.kaggle.com/c/cifar-10/leaderboard">kaggle leaderboard</a></li>
</ul>

<hr />

<h1 id="knn">kNN</h1>

<p><img src="/cs231n/knn.jpeg" alt="challenges" /></p>

<ul>
<li><strong>Hyperparameters</strong>: <code>$k$</code>, distance function</li>
<li>Higher <code>$k$</code>, variance <code>$\downarrow$</code> (more resistant to outliers), bias <code>$\uparrow$</code> <code>$\Rightarrow$</code> bias-variance tradeoff</li>
<li>L2 distance prefers many medium disagreements to one big one.</li>
<li>L1 distance: depends on the <strong>choice of coordinate system</strong>

<ul>
<li>if entries of feature vector have important meaning, L1 may be more natural</li>
</ul></li>
<li><a href="http://vision.stanford.edu/teaching/cs231n-demos/knn/">kNN demo</a></li>
</ul>

<h2 id="load-data">Load Data</h2>

<pre><code class="language-python">Xtr_orig, Ytr, Xte_orig, Yte = load_CIFAR10(cifar10_dir) # a magic function we provide
# flatten out all images to be one-dimensional
Xtr = Xtr_orig.reshape(Xtr_orig.shape[0], -1) # Xtr becomes 50000 x 3072
Xte = Xte_orig.reshape(Xte_orig.shape[0], -1) # Xte becomes 10000 x 3072
</code></pre>

<h2 id="train-and-evaluate-a-classifier">Train and Evaluate a Classifier</h2>

<ul>
<li>API <code>KNearestNeighbour()</code></li>
</ul>

<pre><code class="language-python">knn = KNearestNeighbor() # create a kNN classifier instance
knn.train(Xtr, ytr) # train the classifier on the training images and labels
yte_predict = knn.predict(Xte) # predict labels on the test images
# accuracy = fraction of examples that were correctly predicted
accuracy = np.mean(yte_predict == yte)
print('accuracy: %f' % accuracy)
</code></pre>

<h2 id="implementation-of-knn-classifier-with-l2-distance">Implementation of kNN Classifier with L2 Distance</h2>

<h3 id="vectorization-of-distance-computation">Vectorization of Distance Computation</h3>

<p>$$
A = \begin{pmatrix}
a_1\newline
\vdots\newline
a_N
\end{pmatrix} \in \mathbb{R}^{N \times D},\ \
B = \begin{pmatrix}
b_1\newline
\vdots\newline
b_M
\end{pmatrix} \in \mathbb{R}^{M \times D}
$$</p>

<p>$$
E_{ij} = \Vert b_i- a_j\Vert_2, \ \ E=(E_{ij})\in\mathbb{R}^{M\times N}
$$</p>

<hr />

<p>$$
E_{ij}^2=-2b_i a_j^T +\Vert b_i\Vert_2^2 +\Vert a_j\Vert_2^2 =-2 (BA^T)_{ij} +\Vert b_i\Vert_2^2 +\Vert a_j\Vert_2^2
$$</p>

<pre><code class="language-python">b = np.sum(B * B, axis = 1, keepdims = True)
a = np.sum(A * A, axis = 1, keepdims = True)
E = np.sqrt(-2 * np.dot(B, A.T) + b + a.T) # numpy broadcasting
</code></pre>

<hr />

<pre><code class="language-python">import numpy as np
from scipy.stats import mode

class KNearestNeighbor(object):
    &quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;
    def __init__(self):
        pass

    def train(self, X, y):
        &quot;&quot;&quot;
        X: X.shape == (N, D), N examples, each of dim D
        y: y.shape == (N,)
           y[i] is the label of X[i]
        &quot;&quot;&quot;
        # the nearest neighbor classifier simply remembers all the training data
        self.Xtrain = X
        self.ytrain = y

    def computeDistances(self, X):
        &quot;&quot;&quot;
        Compute the distances between each test point in X
        and each training point in self.Xtrain
        Input:
        X: each row is an example we wish to predict label for
           X.shape == (ntest, D)
        Output:
        dists: dists.shape == (ntest, ntrain)
               dists[i, j] == L2 distance between X[i] and self.Xtrain[j]
        &quot;&quot;&quot;
        ntest, ntrain = X.shape[0], self.Xtrain.shape[0]

        te = np.sum(X * X, axis = 1, keepdims = True)
        tr = np.sum(self.Xtrain * self.Xtrain, axis = 1, keepdims = True)
        dists = np.sqrt(-2 * np.dot(X, self.Xtrain.T) + te + tr.T)

    def predict(self, X, k = 1):
        &quot;&quot;&quot;
        Predict labels for test data using this classifier.

        Input:
        X: each row is an example we wish to predict label for
           X.shape == (ntest, D)
        Output:
        ypred: ypred.shape == (ntest,)
               ypred[i] is the predicted label for X[i]
        &quot;&quot;&quot;
        dists = self.computeDistances(X)
        ntest = X.shape[0]
        # ith row: indices of k nearest neighbors of X[i]
        k_idx = dists.argsort(axis = 1)[:, :k]
        # ith row: labels of k nearest neighbors of X[i]
        closest_y = self.ytrain[k_idx]
        # ith row: most common label in closest_y[i]
        y_pred = mode(closest_y, axis = 1).mode

        return np.squeeze(y_pred)

</code></pre>

<h2 id="hyperparameter-tuning">Hyperparameter Tuning</h2>

<ul>
<li>Validation Set (50%-90% for training, rest for validation)</li>
<li>Cross Validation

<ul>
<li><strong>in practice</strong>, tend to avoid CV since CV can be computationally expensive</li>
<li>typical # of folds: 3, 5, 10</li>
</ul></li>
<li>If the # of hyperparameters is large you may prefer to use <strong>bigger validation splits</strong></li>
<li>If the # of examples in the validation set is small (perhaps only a few hundred or so), it is safer to use CV</li>
</ul>

<blockquote>
<p>Evaluate on the test set only a single time, at the very end.</p>

<p>Split your training set into training set and a validation set. Use validation set to tune all hyperparameters. At the end run a single time on the test set and report performance.</p>
</blockquote>

<hr />

<h2 id="pros-and-cons-of-knn">Pros and Cons of kNN</h2>

<p>With <code>$N$</code>examples, how fast are training and prediction?</p>

<ul>
<li>Training: <code>$O(1)$</code>, only need to memorize training data</li>
<li>Prediction: <code>$O(N)$</code>, need to compare test example to each of the sample in the training set</li>
<li>in practice, we want classifiers that are <strong>fast at prediction</strong>; slow for training is ok</li>
</ul>

<p><strong>Approximate Nearest Neighbor</strong> (ANN) algorithms (e.g. <a href="http://www.cs.ubc.ca/research/flann/">FLANN</a>)</p>

<ul>
<li>accelerate the NN lookup</li>
<li>trade off the <strong>correctness</strong> of the nearest neighbor retrieval with its <strong>space/time complexity</strong> during retrieval</li>
<li>rely on a pre-processing/indexing stage that involves building a <strong>KDtree</strong>, or running the k-means algorithm</li>
</ul>

<hr />

<h2 id="python-code">Python Code</h2>

<p><a href="https://github.com/augustrobo/CS231n-assignments/blob/master/assignment1/knn0308.ipynb">Jupyter Notebook</a></p>

<p><a href="https://github.com/augustrobo/CS231n-assignments/blob/master/assignment1/cs231n/neatknn.py">.py</a></p>

<hr />

<h2 id="knn-in-practice-never-used-on-images">kNN in Practice (Never Used on Images)</h2>

<ul>
<li><strong>Normalize</strong> <strong>the features</strong> in your data (e.g. one pixel in images) to have zero mean and unit variance.</li>
<li>If your data is very high-dimensional, consider using a <strong>dimensionality reduction</strong> technique such as PCA (<a href="http://en.wikipedia.org/wiki/Principal_component_analysis">wiki ref</a>, <a href="http://cs229.stanford.edu/notes/cs229-notes10.pdf">CS229ref</a>, <a href="http://www.bigdataexaminer.com/understanding-dimensionality-reduction-principal-component-analysis-and-singular-value-decomposition/">blog ref</a>) or even <a href="http://scikit-learn.org/stable/modules/random_projection.html">Random Projections</a>.</li>
<li>Split your training data randomly into <strong>train/val splits</strong>.</li>
<li>Train and evaluate the kNN classifier on the validation data (for all folds, if doing cross-validation) for many choices of <code>$k$</code> and across different distance types.</li>
<li>If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library (e.g. <a href="http://www.cs.ubc.ca/research/flann/">FLANN</a>) to accelerate the retrieval (at cost of some accuracy).</li>
<li>Take note of the hyperparameters that gave the best results. There is a question of <strong>whether you should use the full training set with the best hyperparameters, since the optimal hyperparameters might change if you were to fold the validation data into your training set (since the size of the data would be larger)</strong>. In practice it is cleaner to not use the validation data in the final classifier and consider it to be <em>burned</em>  on estimating the hyperparameters. Evaluate the best model on the test set. Report the test set accuracy and declare the result to be the performance of the kNN classifier on your data.</li>
</ul>

<hr />

<h1 id="linear-classification">Linear Classification</h1>

<h2 id="multiclass-svm">Multiclass SVM</h2>

<h2 id="softmax">Softmax</h2>

<h1 id="two-layer-neural-network">Two-layer Neural Network</h1>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

