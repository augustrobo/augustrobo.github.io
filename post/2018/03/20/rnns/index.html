<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sequence Models | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Sequence Models</span></h1>

<h2 class="date">2018/03/20</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> <a href="/categories/rnn">rnn</a> <a href="/categories/lstm">lstm</a> <a href="/categories/nlp">nlp</a> 
  
  
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#examples-of-sequence-data">Examples of Sequence Data</a></li>
<li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a>
<ul>
<li><a href="#forward-propagation">Forward Propagation</a></li>
<li><a href="#back-propagation-through-time">Back Propagation Through Time</a></li>
</ul></li>
<li><a href="#different-types-of-rnns">Different Types of RNNs</a></li>
<li><a href="#language-model-and-sequence-generation">Language Model and Sequence Generation</a>
<ul>
<li><a href="#sampling-novel-sequences">Sampling Novel Sequences</a></li>
</ul></li>
<li><a href="#vanishing-gradients">Vanishing Gradients</a>
<ul>
<li><a href="#gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</a></li>
<li><a href="#long-short-term-memory-lstm">Long Short Term Memory (LSTM)</a></li>
</ul></li>
<li><a href="#bidirectional-rnns-brnn">Bidirectional RNNs (BRNN)</a></li>
<li><a href="#deep-rnns">Deep RNNs</a></li>
</ul>
</nav>


<main>


<h1 id="examples-of-sequence-data">Examples of Sequence Data</h1>

<ul>
<li>speech recognition</li>
<li>music generation</li>
<li>sentiment classification</li>
<li>DNA sequence analysis</li>
<li>machine translation</li>
<li>video activity recognition</li>
<li>name entity recognition</li>
</ul>

<hr />

<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>

<ul>
<li>Why not a standard network?

<ul>
<li>inputs and outputs can be different lengths in different examples, e.g. sentence as input</li>
<li>does not share features learned across different positions of text</li>
</ul></li>
</ul>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/NN/rnnforward.PNG" alt="forward propagation" /></p>

<ul>
<li>at time <code>$t$</code>:</li>
</ul>

<p>$$
a^{t} = g(W_{aa}a^{t-1} + W_{ax}x^{t} + b_a)\\<br />
\hat{y}^{t}=g(W_{ya}a^{t}+b_y)
$$</p>

<ul>
<li>simplified notation:</li>
</ul>

<p>$$
a^{t} = g(W_a[a^{t-1},x^{t}] + b_a), \text{where}
$$</p>

<p>$$
\begin{cases}
W_a =[W_{aa}, W_{ax}]\newline
[a^{t-1},x^{t}] =\begin{pmatrix}
a^{t-1}\newline
x^{t}
\end{pmatrix}
\end{cases}\tag{1}
$$</p>

<p>$$
\hat{y}^{t}=g(W_ya^{t}+b_y)\tag{2}
$$</p>

<hr />

<h2 id="back-propagation-through-time">Back Propagation Through Time</h2>

<p><img src="/NN/rnnback.PNG" alt="back propagation" /></p>

<ul>
<li>loss:</li>
</ul>

<p>$$
\mathcal{L}^t(\hat{y}^t,y^{t})=-y^t\log \hat{y}^t-(1-y^t)\log(1-\hat{y}^t)
$$</p>

<ul>
<li>overall loss:</li>
</ul>

<p>$$
\mathcal{L}(\hat{y},y)=\sum_{t=1}^{T_y}\mathcal{L}^t(\hat{y}^t,y^{t})\tag{3}
$$</p>

<hr />

<h1 id="different-types-of-rnns">Different Types of RNNs</h1>

<ul>
<li><code>$T_x=T_y$</code>: many-to-many</li>

<li><p><code>$1&lt; T_x\neq T_y &gt;1$</code>: many-to-many</p>

<ul>
<li>e.g.  machine translation</li>
</ul></li>

<li><p><code>$T_y=1$</code>: many-to-one</p>

<ul>
<li>e.g. sentiment classification</li>
</ul></li>

<li><p><code>$T_x=1$</code>: one-to-many</p>

<ul>
<li>e.g. music generation, <code>$x=$</code> genre</li>
</ul></li>
</ul>

<p><img src="/NN/rnns.PNG" alt="rnns" /></p>

<hr />

<h1 id="language-model-and-sequence-generation">Language Model and Sequence Generation</h1>

<ul>
<li>input sentence:</li>
</ul>

<p>$$
y^1,y^2,\ldots,y^{T_y}
$$</p>

<ul>
<li>language model: to estimate</li>
</ul>

<p>$$
\Pr(y^1,y^2,\ldots,y^{T_y})
$$</p>

<ul>
<li>training set: large corpus of English text

<ul>
<li>tokenize: e.g. map each word to one-hot representation</li>
<li>add &lt;EOS&gt; (<strong>end of sentence</strong> token)</li>
<li>replace word not in the vocabulary with &lt;UNK&gt; (<strong>unknown word</strong> token)</li>
</ul></li>
<li>build RNN model</li>
</ul>

<p><img src="/NN/language.PNG" alt="language model" /></p>

<h2 id="sampling-novel-sequences">Sampling Novel Sequences</h2>

<ul>
<li>given input, sampling according to the predicted softmax distribution</li>
<li><code>np.random.choice</code></li>
<li>use the previous sampled output as latter input</li>
<li>keep sampling until &lt;EOS&gt; token is generated, or decide the max sentence length beforehand</li>
</ul>

<p><img src="/NN/sample.PNG" alt="sample" /></p>

<hr />

<h1 id="vanishing-gradients">Vanishing Gradients</h1>

<ul>
<li>language can have long-term dependency</li>

<li><p>deep RNN, difficult for the gradients of output to propagate back to affect the weights of earlier layers</p>

<ul>
<li><code>$\Rightarrow$</code> vanishing gradient problem</li>
</ul></li>

<li><p>solve exploding gradients: <strong><u>gradient clipping</u></strong></p>

<ul>
<li>if gradients exceed some threshold, rescale gradients</li>
</ul></li>

<li><p>solve vanishing gradients?</p></li>
</ul>

<hr />

<h2 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h2>

<ul>
<li>captures long range connections</li>
<li>helps with vanishing gradient problems</li>
<li>simplified GRU unit: memory cell <code>$c^t = a^t$</code> (need <code>$c^t$</code> to be highly dependent on <code>$c^{t-1}$</code>)</li>
</ul>

<p>$$
\tilde{c}^t=\tanh(W_c[c^{t-1},x^t]+b_c),\\<br />
\Gamma_u=\sigma(W_u[c^{t-1},x^t]+b_u),\ \ \text{u for &lsquo;update&rsquo;, decide when to update } c^t\\<br />
    c^t = \Gamma_u * \tilde{c}^t +(1-\Gamma_u) *c^{t-1},\ \ \text{element-wise multiplication}
$$</p>

<p><img src="/NN/gru.PNG" alt="gru" /></p>

<ul>
<li>full GRU unit: (simplified: $\Gamma_r=1$)</li>
</ul>

<p>$$
\tilde{c}^t=\tanh(W_c[\Gamma_r *c^{t-1},x^t]+b_c),\ \ \text{&lsquo;r&rsquo; for &lsquo;relevant&rsquo;}\\<br />
\Gamma_u=\sigma(W_u[c^{t-1},x^t]+b_u),\\<br />
\Gamma_r=\sigma(W_r[c^{t-1},x^t]+b_r),\\<br />
    c^t = \Gamma_u * \tilde{c}^t +(1-\Gamma_u) *c^{t-1}
$$</p>

<hr />

<h2 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>

<ul>
<li>u, f, o = update, forget, output</li>
<li>GRU: simpler, can build deep model

<ul>
<li><code>$\Gamma_u, 1-\Gamma_u$</code> play the same role as <code>$\Gamma_u, \Gamma_f$</code></li>
</ul></li>
<li>LSTM: more powerful</li>
</ul>

<p><img src="/NN/lstm.PNG" alt="lstm" /></p>

<hr />

<h1 id="bidirectional-rnns-brnn">Bidirectional RNNs (BRNN)</h1>

<ul>
<li>motivation: gain information from the future</li>
</ul>

<p><img src="/NN/brnn.PNG" alt="brnn" /></p>

<hr />

<h1 id="deep-rnns">Deep RNNs</h1>

<p>For layer <code>$\ell$</code>,
$$
a^{[\ell]t}=g(W_a^{[\ell]}[a^{[\ell]t-1}, a^{[\ell-1]t}] +b_a^{[\ell]})
$$
<img src="/NN/deeprnn.PNG" alt="deep rnn" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

