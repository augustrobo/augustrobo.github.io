<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Sequence Models | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Sequence Models</span></h1>

<h2 class="date">2018/03/20</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> <a href="/categories/rnn">rnn</a> <a href="/categories/lstm">lstm</a> <a href="/categories/nlp">nlp</a> 
  
  
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#examples-of-sequence-data">Examples of Sequence Data</a></li>
<li><a href="#recurrent-neural-networks">Recurrent Neural Networks</a>
<ul>
<li><a href="#forward-propagation">Forward Propagation</a></li>
<li><a href="#back-propagation-through-time">Back Propagation Through Time</a></li>
<li><a href="#rnn-step-by-step-python-code">RNN Step by Step - Python Code</a></li>
</ul></li>
<li><a href="#different-types-of-rnns">Different Types of RNNs</a></li>
<li><a href="#language-model-and-sequence-generation">Language Model and Sequence Generation</a>
<ul>
<li><a href="#sampling-novel-sequences">Sampling Novel Sequences</a></li>
<li><a href="#character-level-language-model-python-code">Character Level Language Model - Python Code</a></li>
</ul></li>
<li><a href="#vanishing-gradients">Vanishing Gradients</a>
<ul>
<li><a href="#gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</a></li>
<li><a href="#long-short-term-memory-lstm">Long Short Term Memory (LSTM)</a>
<ul>
<li><a href="#lstm-step-by-step-python-code">LSTM Step by Step - Python Code</a></li>
<li><a href="#text-generation-writing-like-shakespeare-python-code">Text Generation: Writing Like Shakespeare - Python Code</a></li>
</ul></li>
</ul></li>
<li><a href="#bidirectional-rnns-brnn">Bidirectional RNNs (BRNN)</a></li>
<li><a href="#deep-rnns">Deep RNNs</a></li>
<li><a href="#nlp-and-word-embedding">NLP and Word Embedding</a>
<ul>
<li><a href="#word-representation">Word Representation</a></li>
<li><a href="#using-word-embeddings">Using Word Embeddings</a></li>
<li><a href="#properties-of-word-embeddings">Properties of Word Embeddings</a></li>
<li><a href="#embedding-matrix">Embedding Matrix</a></li>
<li><a href="#learning-word-embeddings">Learning Word Embeddings</a></li>
<li><a href="#word2vec">Word2Vec</a></li>
<li><a href="#negative-sampling">Negative Sampling</a></li>
<li><a href="#glove">GloVe</a></li>
<li><a href="#sentiment-classification">Sentiment Classification</a></li>
<li><a href="#debiasing-word-embeddings">Debiasing Word Embeddings</a></li>
</ul></li>
<li><a href="#sequence-to-sequence-model">Sequence to Sequence Model</a>
<ul>
<li><a href="#image-captioning">Image Captioning</a></li>
<li><a href="#machine-translation-as-building-a-conditional-language-model">Machine Translation as Building a Conditional Language Model</a></li>
<li><a href="#beam-search">Beam Search</a></li>
<li><a href="#refinements-to-beam-search">Refinements to Beam Search</a></li>
<li><a href="#error-analysis-in-beam-search">Error Analysis in Beam Search</a></li>
<li><a href="#bleu-score">Bleu Score</a></li>
<li><a href="#attention-model-intuition">Attention Model Intuition</a></li>
</ul></li>
<li><a href="#speech-recognition">Speech Recognition</a>
<ul>
<li><a href="#trigger-word-detection">Trigger Word Detection</a></li>
</ul></li>
</ul>
</nav>


<main>


<h1 id="examples-of-sequence-data">Examples of Sequence Data</h1>

<ul>
<li>speech recognition</li>
<li>music generation</li>
<li>sentiment classification</li>
<li>DNA sequence analysis</li>
<li>machine translation</li>
<li>video activity recognition</li>
<li>name entity recognition</li>
</ul>

<hr />

<h1 id="recurrent-neural-networks">Recurrent Neural Networks</h1>

<ul>
<li>Why not a standard network?

<ul>
<li>inputs and outputs can be different lengths in different examples, e.g. sentence as input</li>
<li>does not share features learned across different positions of text</li>
</ul></li>
</ul>

<h2 id="forward-propagation">Forward Propagation</h2>

<p><img src="/NN/rnnforward.PNG" alt="forward propagation" /></p>

<ul>
<li>at time <code>$t$</code>:</li>
</ul>

<p>$$
a^{t} = g(W_{aa}a^{t-1} + W_{ax}x^{t} + b_a)\\<br />
\hat{y}^{t}=g(W_{ya}a^{t}+b_y)
$$</p>

<ul>
<li>simplified notation:</li>
</ul>

<p>$$
a^{t} = g(W_a[a^{t-1},x^{t}] + b_a), \text{where}
$$</p>

<p>$$
\begin{cases}
W_a =[W_{aa}, W_{ax}]\newline
[a^{t-1},x^{t}] =\begin{pmatrix}
a^{t-1}\newline
x^{t}
\end{pmatrix}
\end{cases}\tag{1}
$$</p>

<p>$$
\hat{y}^{t}=g(W_ya^{t}+b_y)\tag{2}
$$</p>

<hr />

<h2 id="back-propagation-through-time">Back Propagation Through Time</h2>

<p><img src="/NN/rnnback.PNG" alt="back propagation" /></p>

<ul>
<li>logistic loss:</li>
</ul>

<p>$$
\mathcal{L}^t(\hat{y}^t,y^{t})=-y^t\log \hat{y}^t-(1-y^t)\log(1-\hat{y}^t)
$$</p>

<ul>
<li>overall loss:</li>
</ul>

<p>$$
\mathcal{L}(\hat{y},y)=\sum_{t=1}^{T_y}\mathcal{L}^t(\hat{y}^t,y^{t})\tag{3}
$$</p>

<blockquote>
<p>Derivative of softmax loss:
$$
p = sofmax(z),\ \ \mathcal{L}=-\sum_{k=1}^Ky_k\log p_k
$$</p>

<p>$$
\begin{aligned}
\frac{\partial p_k}{\partial z_k}&amp;=\frac{\partial}{\partial z_k}\frac{e^{z_k}}{\sum_j e^{z_j}}\newline
&amp;=\frac{e^{z_k}}{\sum_j e^{z_j}}-\left(\frac{e^{z_k}}{\sum_j e^{z_j}}\right)^2=p_k(1-p_k),
\end{aligned}
$$</p>

<p>$$
\begin{aligned}
\forall j\neq k, \frac{\partial p_j}{\partial z_k}&amp;=\frac{\partial}{\partial z_k}\frac{e^{z_j}}{\sum_j e^{z_j}}\newline
&amp;=-\frac{e^{z_ke^{z_j}}}{(\sum_j e^{z_j})^2}=-p_kp_j
\end{aligned}
$$</p>

<p>$$
\begin{aligned}
\Rightarrow \frac{\partial\mathcal{L}}{\partial{z_k}}&amp;=-\frac{y_k}{p_k}\frac{\partial p_k}{\partial z_k}-\sum_{j\neq k}\frac{y_j}{p_j}\frac{\partial p_j}{\partial z_k}\newline
&amp;=-y_k(1-p_k)+\sum_{j\neq k}y_jp_k\newline
&amp;=-y_k(1-p_k)+p_k(1-y_k)=p_k-y_k,
\end{aligned}
$$</p>

<p>$$
\Rightarrow\frac{\partial \mathcal{L}}{\partial z}=p-y
$$</p>

<hr />

<p>backpropagation: single training example</p>

<ul>
<li><code>$\mathcal{L}(t):=\mathcal{L}^t(\hat{y}^t,y^{t})$</code></li>
<li><code>$\forall t&lt;T_x, \frac{\partial J}{\partial a^t}=\frac{\partial a^{t+1}}{\partial a^t}\frac{\partial J}{\partial a^{t+1}}+\frac{\partial \mathcal{L}(t)}{\partial a^t}$</code></li>
</ul>

<p>$$
\begin{aligned}
\frac{\partial J}{\partial a^t}&amp;=
\sum_{u\geq t}\frac{\partial\mathcal{L}(u)}{\partial a^t}\newline
&amp;=\sum_{u\geq t+1}\frac{\partial\mathcal{L}(u)}{\partial a^t}+\frac{\partial\mathcal{L}(t)}{\partial a^t}\newline
&amp;=\sum_{u\geq t+1}\frac{\partial a^{ t+1 }}{\partial a^{ t }}\frac{\partial\mathcal{L}(u)}{\partial a^{t+1}}
+\frac{\partial\mathcal{L}(t)}{\partial a^{ t }}\newline
&amp;=\frac{\partial a^{ t+1 }}{\partial a^{ t }}\frac{\partial J}{\partial a^{t+1}}
+\frac{\partial\mathcal{L}(t)}{\partial a^{t}}
\end{aligned}
$$</p>

<ul>
<li><code>$\frac{\partial a^{ t+1 }}{\partial a^{ t }}\frac{\partial J}{\partial a^{t+1}}=?$</code></li>
</ul>

<p>$$
z^{t+1}:=W_{aa}a^t + W_{ax}x^{t+1}+b_a,
$$</p>

<p>$$
\begin{aligned}
\frac{\partial a^{ t+1 }}{\partial a^{ t }}\frac{\partial J}{\partial a^{t+1}}
&amp;=\frac{\partial z^{t+1}}{\partial a^t}\frac{\partial a^{ t+1 }}{\partial z^{ t+1 }}\frac{\partial J}{\partial a^{t+1}}\newline
&amp;=W_{aa}^T diag\{\tanh&rsquo;(z^{t+1})_1,\ldots,\tanh&rsquo;(z^{t+1})_{n_a}\} \frac{\partial J}{\partial a^{t+1}}\newline
&amp;=W_{aa}^T\left(tanh&rsquo;(z^{t+1})* \frac{\partial J}{\partial a^{t+1}}\right)\newline
&amp;\text{ (* stands for element-wise multiplication)}
\end{aligned}
$$</p>

<ul>
<li><code>$\frac{\partial J}{\partial W_{aa}}=?$</code></li>
</ul>

<p>$$
\begin{aligned}
\frac{\partial J}{\partial W_{aa}(i,j)}&amp;=\sum_{t=0}^{T_x-1}\frac{\partial J}{\partial a^{t+1}_i}
\frac{\partial a^{t+1}_i}{\partial z^{t+1}_i}
\frac{\partial z^{t+1}_i}{\partial W_{aa}(i,j)}\newline
&amp;=
\sum_{t=0}^{T_x-1}\frac{\partial J}{\partial a^{t+1}_i}
\tanh&rsquo;(z^{t+1}_i)
a^t_j
\end{aligned}
$$</p>

<p>$$
\Rightarrow
\frac{\partial J}{\partial W_{aa}}=\sum_{t=0}^{T_x-1}\left(tanh&rsquo;(z^{t+1})* \frac{\partial J}{\partial a^{t+1}}\right)(a^t)^T
$$</p>

<p>Similarly,
$$
\frac{\partial J}{\partial x^{t+1}}=W_{ax}^T\left(tanh&rsquo;(z^{t+1})* \frac{\partial J}{\partial a^{t+1}}\right)
$$</p>

<p>$$
\frac{\partial J}{\partial W_{ax}}=\sum_{t=0}^{T_x-1}\left(tanh&rsquo;(z^{t+1})* \frac{\partial J}{\partial a^{t+1}}\right)(x^{t+1})^T
$$</p>

<ul>
<li><code>$\frac{\partial J}{\partial b_a}=?$</code></li>
</ul>

<p>$$
\begin{aligned}
\frac{\partial J}{\partial b_a}&amp;=\sum_{t=0}^{T_x-1}\frac{\partial z^{t+1}}{\partial b_a}
\frac{\partial a^{t+1}}{\partial z^{t+1}}
\frac{\partial J}{\partial a^{t+1}}\newline
&amp;=\sum_{t=0}^{T_x-1}\mathbf{1}_{n_a}^T diag\{\tanh&rsquo;(z^{t+1})_1,\ldots,\tanh&rsquo;(z^{t+1})_{n_a}\} \frac{\partial J}{\partial a^{t+1}}\newline
&amp;=\sum_{t=0}^{T_x-1}\mathbf{1}_{n_a}^T \left(tanh&rsquo;(z^{t+1})* \frac{\partial J}{\partial a^{t+1}}\right)
\end{aligned}
$$</p>
</blockquote>

<h2 id="rnn-step-by-step-python-code">RNN Step by Step - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/11-Building-RNN-Step-by-Step/RNN.ipynb">jupyter notebook</a></p>

<hr />

<h1 id="different-types-of-rnns">Different Types of RNNs</h1>

<ul>
<li><code>$T_x=T_y$</code>: many-to-many</li>

<li><p><code>$1&lt; T_x\neq T_y &gt;1$</code>: many-to-many</p>

<ul>
<li>e.g.  machine translation</li>
</ul></li>

<li><p><code>$T_y=1$</code>: many-to-one</p>

<ul>
<li>e.g. sentiment classification</li>
</ul></li>

<li><p><code>$T_x=1$</code>: one-to-many</p>

<ul>
<li>e.g. music generation, <code>$x=$</code> genre</li>
</ul></li>
</ul>

<p><img src="/NN/rnns.PNG" alt="rnns" /></p>

<hr />

<h1 id="language-model-and-sequence-generation">Language Model and Sequence Generation</h1>

<ul>
<li>input sentence:</li>
</ul>

<p>$$
y^1,y^2,\ldots,y^{T_y}
$$</p>

<ul>
<li>language model: to estimate</li>
</ul>

<p>$$
\Pr(y^1,y^2,\ldots,y^{T_y})
$$</p>

<ul>
<li>training set: large corpus of English text

<ul>
<li>tokenize: e.g. map each word to one-hot representation</li>
<li>add &lt;EOS&gt; (<strong>end of sentence</strong> token)</li>
<li>replace word not in the vocabulary with &lt;UNK&gt; (<strong>unknown word</strong> token)</li>
</ul></li>
<li>build RNN model</li>
</ul>

<p><img src="/NN/language.PNG" alt="language model" /></p>

<h2 id="sampling-novel-sequences">Sampling Novel Sequences</h2>

<ul>
<li>given input, sampling according to the predicted softmax distribution</li>
<li><code>np.random.choice</code></li>
<li>use the previous sampled output as latter input</li>
<li>keep sampling until &lt;EOS&gt; token is generated, or decide the max sentence length beforehand</li>
</ul>

<p><img src="/NN/sample.PNG" alt="sample" /></p>

<hr />

<h2 id="character-level-language-model-python-code">Character Level Language Model - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/11-Building-RNN-Step-by-Step/Dinosaurus%20Island%20-%20Character%20level%20language%20model.ipynb">jupyter notebook</a></p>

<hr />

<h1 id="vanishing-gradients">Vanishing Gradients</h1>

<ul>
<li>language can have long-term dependency</li>

<li><p>deep RNN, difficult for the gradients of output to propagate back to affect the weights of earlier layers</p>

<ul>
<li><code>$\Rightarrow$</code> vanishing gradient problem</li>
</ul></li>

<li><p>solve exploding gradients: <strong><u>gradient clipping</u></strong></p>

<ul>
<li>if gradients exceed some threshold, rescale gradients</li>
</ul></li>

<li><p>solve vanishing gradients?</p></li>
</ul>

<hr />

<h2 id="gated-recurrent-unit-gru">Gated Recurrent Unit (GRU)</h2>

<ul>
<li>captures long range connections</li>
<li>helps with vanishing gradient problems</li>
<li>simplified GRU unit: memory cell <code>$c^t = a^t$</code> (need <code>$c^t$</code> to be highly dependent on <code>$c^{t-1}$</code>)</li>
</ul>

<p>$$
\tilde{c}^t=\tanh(W_c[c^{t-1},x^t]+b_c),\\<br />
\Gamma_u=\sigma(W_u[c^{t-1},x^t]+b_u),\ \ \text{u for &lsquo;update&rsquo;, decide when to update } c^t\\<br />
    c^t = \Gamma_u * \tilde{c}^t +(1-\Gamma_u) *c^{t-1},\ \ \text{element-wise multiplication}
$$</p>

<p><img src="/NN/gru.PNG" alt="gru" /></p>

<ul>
<li>full GRU unit: (simplified: <code>$\Gamma_r=1$</code>)</li>
</ul>

<p>$$
\tilde{c}^t=\tanh(W_c[\Gamma_r *c^{t-1},x^t]+b_c),\ \ \text{&lsquo;r&rsquo; for &lsquo;relevant&rsquo;}\\<br />
\Gamma_u=\sigma(W_u[c^{t-1},x^t]+b_u),\\<br />
\Gamma_r=\sigma(W_r[c^{t-1},x^t]+b_r),\\<br />
    c^t = \Gamma_u * \tilde{c}^t +(1-\Gamma_u) *c^{t-1}
$$</p>

<hr />

<h2 id="long-short-term-memory-lstm">Long Short Term Memory (LSTM)</h2>

<ul>
<li>u, f, o = update, forget, output</li>
<li>GRU: simpler, can build deep model

<ul>
<li><code>$\Gamma_u, 1-\Gamma_u$</code> play the same role as <code>$\Gamma_u, \Gamma_f$</code></li>
</ul></li>
<li>LSTM: more powerful</li>
</ul>

<p><img src="/NN/lstm.PNG" alt="lstm" /></p>

<hr />

<h3 id="lstm-step-by-step-python-code">LSTM Step by Step - Python Code</h3>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/11-Building-RNN-Step-by-Step/LSTM.ipynb">jupyter notebook</a></p>

<hr />

<h3 id="text-generation-writing-like-shakespeare-python-code">Text Generation: Writing Like Shakespeare - Python Code</h3>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/11-Building-RNN-Step-by-Step/Writing-Like-Shakespeare.ipynb">jupyter notebook</a></p>

<hr />

<h1 id="bidirectional-rnns-brnn">Bidirectional RNNs (BRNN)</h1>

<ul>
<li>motivation: gain information from the future</li>
</ul>

<p><img src="/NN/brnn.PNG" alt="brnn" /></p>

<hr />

<h1 id="deep-rnns">Deep RNNs</h1>

<p>For layer <code>$\ell$</code>,
$$
a^{[\ell]t}=g(W_a^{[\ell]}[a^{[\ell]t-1}, a^{[\ell-1]t}] +b_a^{[\ell]})
$$
<img src="/NN/deeprnn.PNG" alt="deep rnn" /></p>

<hr />

<h1 id="nlp-and-word-embedding">NLP and Word Embedding</h1>

<h2 id="word-representation">Word Representation</h2>

<ul>
<li>one-hot representation</li>
<li>featurized representation: word embedding</li>
</ul>

<p><img src="/NN/embedding.PNG" alt="word embedding" /></p>

<ul>
<li>t-SNE: visualizing word embeddings</li>
</ul>

<p><img src="/NN/tsne.PNG" alt="tsne" /></p>

<hr />

<h2 id="using-word-embeddings">Using Word Embeddings</h2>

<ul>
<li>name entity recognition</li>
<li>transfer learning

<ul>
<li>learn word embeddings from large text corpus (1-100b words) (or download pre-trained embedding online)</li>
<li>transfer embedding to new task with smaller training set (say, 100k words)</li>
<li>optional: continue to fine-tune the word embeddings with new data</li>
</ul></li>
<li>BRNN</li>
</ul>

<p><img src="/NN/entity.PNG" alt="name entity" /></p>

<ul>
<li>relation to face encoding</li>
</ul>

<p><img src="/NN/faceencoding.PNG" alt="face encoding" /></p>

<hr />

<h2 id="properties-of-word-embeddings">Properties of Word Embeddings</h2>

<ul>
<li>analogies: man:woman is king:?</li>
<li>find word <code>$w$</code> :</li>
</ul>

<p>$$
\arg\max_w sim(e_w, e_{king}-e_{man}+e_{woman})
$$</p>

<p><img src="/NN/analogy.PNG" alt="analogy" /></p>

<p><img src="/NN/analogy2.PNG" alt="analogy" /></p>

<ul>
<li>similarity function:

<ul>
<li><strong>cosine similarity</strong> (most common)</li>
</ul></li>
</ul>

<p>$$
sim(u,v)=\frac{u^Tv}{\Vert u\Vert_2 \Vert v\Vert_2}
$$</p>

<hr />

<h2 id="embedding-matrix">Embedding Matrix</h2>

<ul>
<li>embedding matrix <code>$E$</code></li>
<li>embedding for word <code>$w$</code>: <code>$e_w$</code></li>
<li>one-hot representation for word <code>$w$</code>: <code>$O_w$</code></li>
</ul>

<p>$$
EO_w = e_w
$$</p>

<ul>
<li><strong>in practice</strong>, not efficient to use matrix multiplication to look up an embedding</li>
</ul>

<hr />

<h2 id="learning-word-embeddings">Learning Word Embeddings</h2>

<ul>
<li>neural language model</li>
<li>fixed history</li>
</ul>

<p><img src="/NN/learn.PNG" alt="learn" /></p>

<ul>
<li>context/target pairs <code>$\rightarrow$</code>  context

<ul>
<li><strong>goal</strong>: build language model

<ul>
<li>last 4 words</li>
</ul></li>
<li><strong>goal</strong>: learn embedding

<ul>
<li>4 words on left &amp; right</li>
<li>last 1 word</li>
<li>nearby 1 word</li>
</ul></li>
</ul></li>
</ul>

<hr />

<h2 id="word2vec">Word2Vec</h2>

<ul>
<li>skip-grams

<ul>
<li>randomly pick a word to be the <strong>context</strong> word</li>
<li>randomly pick another word within some window to be the <strong>target</strong> word</li>
</ul></li>
<li>supervised model</li>
</ul>

<blockquote>
<ul>
<li>vocab size <code>$= v$</code></li>
<li>context <code>$c\rightarrow$</code> target <code>$t$</code></li>
</ul>

<p>$$
O_c\rightarrow E\rightarrow e_c\rightarrow softmax\rightarrow \hat{y}
$$</p>

<p>$$
softmax: \Pr(t|c)=\frac{e^{\theta_t^Te_c}}{\sum_{j=1}^{v}e^{\theta_j^Te_c}}
$$</p>

<ul>
<li>loss:</li>
</ul>

<p>$$
\mathcal{L}(\hat{y},y)=-\sum_{i=1}^{v}y_i\log\hat{y}_i
$$</p>
</blockquote>

<ul>
<li>problems: computationally expensive to compute <code>$\Pr(t|c)$</code></li>
<li>solutions:

<ul>
<li>hierarchical softmax:

<ul>
<li>tree of binary classifiers</li>
<li>cost: <code>$O(\log v)$</code> better than <code>$O(v)$</code> (softmax classifier)</li>
<li>build unbalanced tree:  common words on the top</li>
</ul></li>
<li><strong>negative sampling</strong></li>
</ul></li>
<li>how to sample context <code>$c$</code>?

<ul>
<li>not uniform sampling: <em>the, of, a, &hellip;</em></li>
</ul></li>
</ul>

<hr />

<h2 id="negative-sampling">Negative Sampling</h2>

<ul>
<li>pick a context word</li>
<li>generate a positive example:

<ul>
<li>for the context word, look around a window</li>
</ul></li>
<li>generate <code>$K$</code> negative examples:

<ul>
<li>for the context word, randomly pick a word</li>
</ul></li>
<li>supervised learning  problem</li>
</ul>

<p><img src="/NN/negative.PNG" alt="negative sampling" /></p>

<ul>
<li>logistic regression model:</li>
</ul>

<p>$$
\Pr(y=1|c,t)=\sigma(\theta_t^Te_c)
$$</p>

<ul>
<li>instead of a large softmax model, train <code>$v$</code> logistic models</li>
<li>how to sample random target words?

<ul>
<li>heuristic: between uniform distribution and empirical distribution</li>
</ul></li>
</ul>

<p>$$
\Pr(w_i)=\frac{f(w_i)^{0.75}}{\sum_jf(w_j)^{0.75}}, \ \ f(w_i) = \text{ frequency of word }w_i
$$</p>

<hr />

<h2 id="glove">GloVe</h2>

<ul>
<li>Global Vectors for Word Representation</li>
</ul>

<p>$$
X_{ct} = \text{# of times word }c \text{ appears in context of word }t = X_{tc}
$$</p>

<ul>
<li>model: minimize</li>
</ul>

<p>$$
\sum_{c,t=1}^v f(X_{ct})(\theta_c^Te_t+b_c+b&rsquo;_t-\log X_{ct})^2, \text{where weighting term}
$$</p>

<p>$$
f(X_{ct})=0 \text{ for } X_{ct}=0 \text{ and gives more weight to uncommon words}
$$</p>

<ul>
<li>since <code>$\theta_c,e_t$</code> are symmetric</li>
</ul>

<p>$$
e_w^{(final)}:=\frac{\theta_w+e_w}{2}
$$</p>

<hr />

<h2 id="sentiment-classification">Sentiment Classification</h2>

<ul>
<li>simple sentiment classification</li>
</ul>

<p><img src="/NN/sentiment.PNG" alt="sentiment" /></p>

<ul>
<li>(many-to-one) RNN for sentiment classification</li>
</ul>

<p><img src="/NN/sentiment2.PNG" alt="sentiment" /></p>

<hr />

<h2 id="debiasing-word-embeddings">Debiasing Word Embeddings</h2>

<p><img src="/NN/debias.PNG" alt="debias" /></p>

<hr />

<h1 id="sequence-to-sequence-model">Sequence to Sequence Model</h1>

<ul>
<li>encoder network</li>
<li>decoder network</li>
</ul>

<p><img src="/NN/sequence.PNG" alt="sequence" /></p>

<hr />

<h2 id="image-captioning">Image Captioning</h2>

<ul>
<li>use pre-trained cnn</li>
</ul>

<p><img src="/NN/caption.PNG" alt="caption" /></p>

<hr />

<h2 id="machine-translation-as-building-a-conditional-language-model">Machine Translation as Building a Conditional Language Model</h2>

<ul>
<li>language model vs machine translation model</li>
</ul>

<p><img src="/NN/translation.PNG" alt="caption" /></p>

<ul>
<li>finding the most likely translation</li>
</ul>

<p>$$
\arg\max_{y^1,y^2,\ldots,y^{T_y}}\Pr(y^1,\ldots,y^{T_y}|x)
$$</p>

<ul>
<li><strong>beam search</strong> vs <strong>greedy search</strong>

<ul>
<li>greedy search: each time, find the most likely single word</li>
</ul></li>
</ul>

<hr />

<h2 id="beam-search">Beam Search</h2>

<ul>
<li>first word? <code>$\Pr(y^1|x)$</code>

<ul>
<li>parameter <code>$B$</code>: beam width (consider <code>$B$</code> possibilities at one time)</li>
<li><code>$B=1$</code>, greedy search</li>
</ul></li>
<li>second word?  <code>$y^1,y^2$</code> with top <code>$B$</code></li>
</ul>

<p>$$
\Pr(y^1,y^2|x)=\Pr(y^1|x)\Pr(y^2|y^1,x)
$$</p>

<hr />

<h2 id="refinements-to-beam-search">Refinements to Beam Search</h2>

<ul>
<li>BS unnaturally prefers shorter sentences</li>
</ul>

<p>$$
\arg\max_y\prod_{t=1}^{T_y}\Pr(y^t|x,y^1,\ldots,y^{t-1})
$$</p>

<p>$$
\overset{\text{avoid numerical rounding error}}{\Longrightarrow} \arg\max_y\sum_{t=1}^{T_y}\log\Pr(y^t|x,y^1,\ldots,y^{t-1})
$$</p>

<ul>
<li>length normalization

<ul>
<li>heuristic: <code>$\alpha \in (0,1)$</code></li>
</ul></li>
</ul>

<p>$$
\frac{1}{T_y^{\alpha}}\arg\max_y\sum_{t=1}^{T_y}\log\Pr(y^t|x,y^1,\ldots,y^{t-1})
$$</p>

<ul>
<li>choice of <code>$B$</code>

<ul>
<li>large beam width: consider large possibilities, better result, slower</li>
<li>small beam width: worse result, faster</li>
</ul></li>
<li>unlike exact search algorithms like BFS or DFS, BS runs faster but is not guaranteed to find exact maximum</li>
</ul>

<hr />

<h2 id="error-analysis-in-beam-search">Error Analysis in Beam Search</h2>

<ul>
<li>human translation <code>$y^*$</code>, algorithm output <code>$\hat{y}$</code></li>
<li>case 1:  <code>$\Pr(y^*|x)&gt;\Pr(\hat{y}|x)$</code>

<ul>
<li>BS chose <code>$\hat{y}$</code>, but <code>$y^*$</code> attains higher <code>$\Pr(y|x)$</code></li>
<li>BS is at fault</li>
</ul></li>
<li>case 2: <code>$\Pr(y^*|x)&lt;\Pr(\hat{y}|x)$</code>

<ul>
<li><code>$y^*$</code> is a better translation than <code>$\hat{y}$</code>, but RNN predicted <code>$\Pr(y^*|x)&lt;\Pr(\hat{y}|x)$</code></li>
<li>RNN model is at fault</li>
</ul></li>
</ul>

<p><img src="/NN/error.PNG" alt="error" /></p>

<hr />

<h2 id="bleu-score">Bleu Score</h2>

<ul>
<li>evaluate machine translation</li>
<li>bleu = bilingual evaluation understudy</li>
<li><code>$p_n$</code> = Bleu score on n-grams only</li>
<li>combined Bleu score:</li>
</ul>

<p>$$
BP \exp\left(\frac{1}{K}\sum_{i=1}^K p_K\right)
$$</p>

<p>$$
\text{BP penalty (brevity penalty)}:=
\begin{cases}
1, &amp;\text{if MT_output_length&gt;reference_output_length}\newline
\exp(1-\text{ MT_output_length/reference_output_length}),&amp;\text{otherwise}
\end{cases}
$$</p>

<ul>
<li>download open source implementation</li>
</ul>

<p><img src="/NN/bleu.PNG" alt="bleu" /></p>

<p><img src="/NN/bleu2.PNG" alt="bleu" /></p>

<p><img src="/NN/bleu3.PNG" alt="bleu" /></p>

<hr />

<h2 id="attention-model-intuition">Attention Model Intuition</h2>

<ul>
<li>problem of long sequences</li>
<li>BRNN, attention weight</li>
</ul>

<p><img src="/NN/attention.PNG" alt="attention" /></p>

<p><img src="/NN/attention2.PNG" alt="attention" /></p>

<p><img src="/NN/attention3.PNG" alt="attention" /></p>

<hr />

<h1 id="speech-recognition">Speech Recognition</h1>

<ul>
<li><p>attention models</p></li>

<li><p>CTC cost (connectionist temporal classification)</p>

<ul>
<li>collapse repeated characters note separated by &ldquo;blank&rdquo;</li>
</ul></li>
</ul>

<p><img src="/NN/ctc.PNG" alt="ctc" /></p>

<hr />

<h2 id="trigger-word-detection">Trigger Word Detection</h2>

<p><img src="/NN/trigger.PNG" alt="trigger" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

