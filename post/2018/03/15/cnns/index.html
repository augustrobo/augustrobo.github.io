<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Convolutional Neural Networks | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/ascetic.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Convolutional Neural Networks</span></h1>

<h2 class="date">2018/03/15</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> <a href="/categories/cnn">cnn</a> 
  
  
  
  Tags: <a href="/tags/computervision">computerVision</a> <a href="/tags/cnn">cnn</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#building-blocks-of-cnn">Building Blocks of CNN</a>
<ul>
<li><a href="#edge-detection">Edge Detection</a>
<ul>
<li><a href="#vertical-edge-detection">Vertical Edge Detection</a></li>
<li><a href="#vertical-and-horizontal-edge-detection">Vertical and Horizontal Edge Detection</a></li>
<li><a href="#convolution-operation">Convolution Operation</a></li>
</ul></li>
<li><a href="#padding">Padding</a>
<ul>
<li><a href="#valid-and-same-convolutions">Valid and Same Convolutions</a></li>
</ul></li>
<li><a href="#strided-convolution">Strided Convolution</a></li>
<li><a href="#convolution-over-volume">Convolution Over Volume</a></li>
<li><a href="#one-layer-of-a-convolutional-network">One Layer of a Convolutional Network</a></li>
<li><a href="#simple-convolutional-network-example">Simple Convolutional Network Example</a></li>
<li><a href="#pooling-layer">Pooling Layer</a></li>
<li><a href="#neural-network-example-inspired-by-lenet-5">Neural Network Example (Inspired by LeNet-5)</a></li>
<li><a href="#why-convolutions">Why Convolutions?</a></li>
<li><a href="#cnn-step-by-step-python-code">CNN Step By Step - Python Code</a></li>
</ul></li>
<li><a href="#case-studies">Case Studies</a>
<ul>
<li><a href="#classic-networks">Classic Networks</a>
<ul>
<li><a href="#lenet-5">LeNet-5</a></li>
<li><a href="#alexnet">AlexNet</a></li>
<li><a href="#vgg-16">VGG-16</a></li>
</ul></li>
<li><a href="#residual-networks-resnets">Residual Networks (ResNets)</a>
<ul>
<li><a href="#residual-block">Residual Block</a></li>
</ul></li>
<li><a href="#one-by-one-convolution">One-By-One Convolution</a></li>
<li><a href="#inception-network">Inception Network</a>
<ul>
<li><a href="#inception-module">Inception Module</a></li>
</ul></li>
</ul></li>
<li><a href="#practical-advices-for-using-convnets">Practical Advices for Using ConvNets</a>
<ul>
<li><a href="#using-open-source-implementation">Using Open-Source Implementation</a></li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
<li><a href="#data-augmentation">Data Augmentation</a>
<ul>
<li><a href="#implementing-distortion-during-training">Implementing Distortion During Training</a></li>
</ul></li>
<li><a href="#tips-for-doing-well-on-benchmarks-winning-competitions">Tips for Doing Well on Benchmarks/Winning Competitions</a></li>
</ul></li>
<li><a href="#detection-algorithms">Detection Algorithms</a>
<ul>
<li><a href="#object-localization">Object Localization</a></li>
</ul></li>
</ul>
</nav>


<main>


<h1 id="building-blocks-of-cnn">Building Blocks of CNN</h1>

<h2 id="edge-detection">Edge Detection</h2>

<h3 id="vertical-edge-detection">Vertical Edge Detection</h3>

<p><img src="/NN/edge.PNG" alt="vertical edge detection" /></p>

<h3 id="vertical-and-horizontal-edge-detection">Vertical and Horizontal Edge Detection</h3>

<p><img src="/NN/hvedge.PNG" alt="horizontal and vertical edge detection" /></p>

<h3 id="convolution-operation">Convolution Operation</h3>

<ul>
<li>tensorflow: <code>tf.nn.conv2d</code></li>
<li>keras: <code>conv2D</code></li>
</ul>

<hr />

<h2 id="padding">Padding</h2>

<ul>
<li><code>$n\times n$</code> image, convolves with a <code>$f\times f$</code> filter <code>$\Rightarrow (n-f+1)\times(n-f+1)$</code> output</li>
<li>downside:

<ol>
<li>shrinking output</li>
<li>pixels at the corner is touched only once, information from the edge is thrown away</li>
</ol></li>
<li>padding with zeros, <code>$p=$</code> padding amount</li>
<li>output: <code>$(n+2p-f+1)\times (n+2p-f+1)$</code></li>
</ul>

<hr />

<h3 id="valid-and-same-convolutions">Valid and Same Convolutions</h3>

<ul>
<li><strong>valid</strong>: no padding</li>
<li><strong>same</strong>: output size = input size, <code>$2p+1=f$</code>, <code>$f$</code> is usually odd</li>
</ul>

<hr />

<h2 id="strided-convolution">Strided Convolution</h2>

<ul>
<li><code>$n\times n$</code> image, convolves with a <code>$f\times f$</code> filter, with padding <code>$p$</code> and stride <code>$s$</code>

<ul>
<li><code>$\Rightarrow$</code> output size <code>$=\left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)\times \left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)$</code></li>
</ul></li>
</ul>

<hr />

<h2 id="convolution-over-volume">Convolution Over Volume</h2>

<p><img src="/NN/rgb.PNG" alt="rgb" /></p>

<ul>
<li>input size: <code>$n\times n\times n_C$</code> (<code>$n_C = $</code> # of channels)</li>
<li><code>$n_C'$</code> filters, each of size: <code>$f\times f\times n_C$</code>  (each has <code>$f\times f\times n_C$</code> parameters)</li>
<li>output size: <code>$(n-f+1)\times (n-f+1)\times n_C'$</code></li>
</ul>

<p><img src="/NN/volume.PNG" alt="multiple filters" /></p>

<h2 id="one-layer-of-a-convolutional-network">One Layer of a Convolutional Network</h2>

<ul>
<li>Notations: layer <code>$\ell$</code>

<ul>
<li><code>$f^{[\ell]}$</code>  = filter size</li>
<li><code>$p^{[\ell]}$</code> = padding</li>
<li><code>$s^{[\ell]}$</code> = stride</li>
<li><code>$n^{[\ell]}_C$</code> = # of filters</li>
<li>input size: <code>$n^{[\ell-1]}_H\times n^{[\ell-1]}_W\times n^{[\ell-1]}_C$</code></li>
<li>each filter size: <code>$f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C$</code></li>
<li>output size: <code>$n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$</code>  , where $$n^{[\ell]}_H=\left\lfloor \frac{n^{[\ell-1]}_H +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor,\\<br />
n^{[\ell]}_W=\left\lfloor \frac{n^{[\ell-1]}_W +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor$$</li>
<li>activations: <code>$n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$</code></li>
<li>weights: <code>$f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C\times n^{[\ell]}_C$</code></li>
<li>bias: <code>$1\times 1\times 1\times n^{[\ell]}_C$</code></li>
</ul></li>
</ul>

<p><img src="/NN/layer.PNG" alt="one layer of cnn" /></p>

<h2 id="simple-convolutional-network-example">Simple Convolutional Network Example</h2>

<p><strong><u>Types of layer in a convolutional network</u></strong>:</p>

<ul>
<li>Convolution (CONV)</li>
<li>Pooling (POOL)</li>
<li>Fully connected (FC)</li>
</ul>

<p><img src="/NN/cnn.PNG" alt="simple cnn" /></p>

<h2 id="pooling-layer">Pooling Layer</h2>

<ul>
<li>hyperparameters

<ul>
<li><code>$f$</code>: filter size</li>
<li><code>$s$</code>: stride</li>
<li><strong>max</strong> or <strong>average</strong> pooling</li>
<li><code>$p$</code>: padding (rarely use)</li>
</ul></li>
<li>no parameters to learn!</li>
<li>input size: <code>$n_H\times n_W\times n_c$</code></li>
<li>output size (no padding): <code>$\lfloor \frac{n_H-f}{s}+1\rfloor \times \lfloor \frac{n_W-f}{s}+1\rfloor\times n_c$</code></li>
</ul>

<hr />

<h2 id="neural-network-example-inspired-by-lenet-5">Neural Network Example (Inspired by LeNet-5)</h2>

<p><img src="/NN/lenet.PNG" alt="lenet5" /></p>

<h2 id="why-convolutions">Why Convolutions?</h2>

<ul>
<li><strong>parameter sharing</strong>: a <u>feature detector</u> (such as a vertical edge detector) that is useful in one part of the image is probably useful in another part of the image; reduces the total number of parameters, thus <strong>reducing overfitting</strong></li>
<li><strong>sparsity of connections</strong>: in each layer, each output value depends only on a small number of inputs</li>
</ul>

<hr />

<h2 id="cnn-step-by-step-python-code">CNN Step By Step - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/07-CNN-Step-by-Step/CNN-Step-by-Step.ipynb">Building a CNN Step by Step</a></p>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/07-CNN-Step-by-Step/CNN-Application.ipynb">CNN Application-Sign Recognition</a></p>

<hr />

<h1 id="case-studies">Case Studies</h1>

<h2 id="classic-networks">Classic Networks</h2>

<h3 id="lenet-5">LeNet-5</h3>

<ul>
<li>structure: CONV - POOL - CONV - POOL - FC - FC - OUTPUT</li>
<li><code>$n_H, n_W \downarrow$</code>, <code>$n_C\uparrow$</code></li>
<li># of parameters: 60k</li>
<li>advanced:

<ul>
<li>not use sigmoid/tanh, use ReLU</li>
<li>use max pool, not avg pool</li>
<li>LeNet-5 adds nonlinearity after pooling</li>
</ul></li>
</ul>

<p><img src="/NN/lenet5.PNG" alt="lenet5" /></p>

<h3 id="alexnet">AlexNet</h3>

<ul>
<li>similar to LeNet, but much bigger, 60m parameters</li>
</ul>

<p>$$
(11\times 11\times 3+1)\times 96+(5\times 5\times 96+1)\times 256 + (3\times 3\times 256+1)\times 384 \\<br />
+(3\times3\times 384+1)\times 384+ (3\times3\times 384 +1)\times 256 + (9216+1)\times 4096 \\<br />
+(4096+1)\times 4096 + (4096+1)\times 10 =58,322,314
$$</p>

<ul>
<li>ReLU activation</li>
<li>multiple GPUs</li>
<li>local response normalization (LRN) 【does not help much】</li>
</ul>

<p><img src="/NN/alex.PNG" alt="alex" /></p>

<h3 id="vgg-16">VGG-16</h3>

<ul>
<li># of parameters: 138m</li>
<li>VGG-19, an even bigger version</li>
<li><code>$n_H, n_W \downarrow$</code> by a factor of 2, <code>$n_C\uparrow$</code>  by a factor of 2</li>
</ul>

<p><img src="/NN/vgg.PNG" alt="vgg" /></p>

<h2 id="residual-networks-resnets">Residual Networks (ResNets)</h2>

<ul>
<li>very deep neural networks are difficult to train because of <strong>vanishing/exploding gradients</strong></li>
<li>skip connection</li>
</ul>

<h3 id="residual-block">Residual Block</h3>

<ul>
<li>short-cut</li>
</ul>

<p>$$
a^{[\ell+2]}=g(z^{[\ell +2]}+a^{[\ell]})
$$</p>

<p><img src="/NN/resblock.PNG" alt="residual block" /></p>

<ul>
<li>residual networks: helps to build deep networks</li>
</ul>

<p><img src="/NN/resnet.PNG" alt="resnet" /></p>

<ul>
<li>why resnets work?

<ul>
<li>if <code>$W^{[\ell+2]}=0$</code>, <code>$b^{[\ell+2]}=0$</code>, <code>$g$</code> is ReLU activation, <code>$\Rightarrow a^{[\ell+2]}=a^{[\ell]}$</code> (identity function is easy for resnet to learn) <code>$\Rightarrow$</code> guarantees not to hurt performance</li>
<li><code>$z^{[\ell+2]}$</code> needs to be the same dimension as <code>$a^{[\ell]}$</code>:

<ul>
<li>use SAME convolution, or</li>
<li>add extra matrix,  <code>$a^{[\ell+2]}=g(z^{[\ell +2]}+W_s a^{[\ell]})$</code>, <code>$W_s$</code> can be parameters to learn, or zero padding</li>
</ul></li>
</ul></li>
</ul>

<p><img src="/NN/resnet2.PNG" alt="resnet" /></p>

<hr />

<h2 id="one-by-one-convolution">One-By-One Convolution</h2>

<ul>
<li>pooling: shrinks <code>$n_H, n_W$</code></li>
<li><code>$1\times 1$</code> convolution, aka networks in network

<ul>
<li>interpretation: a <u><strong>fully-connect</strong></u> neural network applied to each of the <code>$n_H\times n_W$</code> positions</li>
<li>increases or decreases <code>$n_C$</code></li>
<li>adds nonlinearity</li>
</ul></li>
</ul>

<p><img src="/NN/1by1.PNG" alt="one by one" /></p>

<p><img src="/NN/1by12.PNG" alt="one by one" /></p>

<hr />

<h2 id="inception-network">Inception Network</h2>

<p><img src="/NN/inception.PNG" alt="inception" /></p>

<ul>
<li># of multiplications: <code>$28\times 28\times 192\times 5\times 5\times 32\approx 120$</code>m</li>
</ul>

<p><img src="/NN/inception2.PNG" alt="inception" /></p>

<ul>
<li># of multiplications: <code>$28\times 28\times 192\times 16 + 28\times28\times16\times5\times5\times 32\approx 12.4$</code>m</li>
</ul>

<p><img src="/NN/inception3.PNG" alt="inception" /></p>

<h3 id="inception-module">Inception Module</h3>

<p><img src="/NN/inception4.PNG" alt="inception" /></p>

<p><img src="/NN/inceptionnetwork.PNG" alt="inception network" /></p>

<hr />

<h1 id="practical-advices-for-using-convnets">Practical Advices for Using ConvNets</h1>

<ol>
<li>use architectures of networks published in the literature</li>
<li>use open source implementations if possible</li>
<li>use pretrained models and fine-tune on your dataset</li>
</ol>

<h2 id="using-open-source-implementation">Using Open-Source Implementation</h2>

<ul>
<li>github: open-source code</li>
</ul>

<h2 id="transfer-learning">Transfer Learning</h2>

<ul>
<li><p>open-source weights</p></li>

<li><p>small training set:</p>

<ul>
<li>train only the softmax layer weights, freeze all of the earlier layers&rsquo; weights</li>
<li>precompute the last activation, save to disk, as input features of a shallow nn</li>
</ul></li>

<li><p>large training set:</p>

<ul>
<li>freeze fewer layers, train latter layers or your own network</li>
<li>open weights as initialization, then train the whole network</li>
</ul></li>
</ul>

<p><img src="/NN/transfer.PNG" alt="transfer" /></p>

<hr />

<h2 id="data-augmentation">Data Augmentation</h2>

<ul>
<li>mirroring</li>
<li>random cropping</li>
<li>rotation</li>
<li>shearing</li>
<li>local warping</li>
<li>color shifting

<ul>
<li>advanced: <strong>PCA</strong> [alexnet paper, PCA color augmentation]</li>
</ul></li>
</ul>

<hr />

<h3 id="implementing-distortion-during-training">Implementing Distortion During Training</h3>

<p><img src="/NN/cpu.PNG" alt="cpu" /></p>

<hr />

<h2 id="tips-for-doing-well-on-benchmarks-winning-competitions">Tips for Doing Well on Benchmarks/Winning Competitions</h2>

<ul>
<li>Ensembling:  train several NNs independently and average their outputs</li>
<li>Multi-crop at test time:

<ul>
<li>run classifier on multiple versions of test images and average results</li>
<li>e.g. 10-crop</li>
</ul></li>
</ul>

<p><img src="/NN/crop.PNG" alt="crop" /></p>

<hr />

<h1 id="detection-algorithms">Detection Algorithms</h1>

<h2 id="object-localization">Object Localization</h2>

<p><img src="/NN/localization.PNG" alt="localization" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

