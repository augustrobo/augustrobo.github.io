<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Convolutional Neural Networks | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Convolutional Neural Networks</span></h1>

<h2 class="date">2018/03/15</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/neuralnetworks">neuralNetworks</a> <a href="/categories/deeplearning">deepLearning</a> <a href="/categories/cnn">cnn</a> 
  
  
  
  Tags: <a href="/tags/computervision">computerVision</a> <a href="/tags/cnn">cnn</a> 
  
  
</p>
</div>


<nav id="TableOfContents">
<ul>
<li><a href="#building-blocks-of-cnn">Building Blocks of CNN</a>
<ul>
<li><a href="#edge-detection">Edge Detection</a>
<ul>
<li><a href="#vertical-edge-detection">Vertical Edge Detection</a></li>
<li><a href="#vertical-and-horizontal-edge-detection">Vertical and Horizontal Edge Detection</a></li>
<li><a href="#convolution-operation">Convolution Operation</a></li>
</ul></li>
<li><a href="#padding">Padding</a>
<ul>
<li><a href="#valid-and-same-convolutions">Valid and Same Convolutions</a></li>
</ul></li>
<li><a href="#strided-convolution">Strided Convolution</a></li>
<li><a href="#convolution-over-volume">Convolution Over Volume</a></li>
<li><a href="#one-layer-of-a-convolutional-network">One Layer of a Convolutional Network</a></li>
<li><a href="#simple-convolutional-network-example">Simple Convolutional Network Example</a></li>
<li><a href="#pooling-layer">Pooling Layer</a></li>
<li><a href="#neural-network-example-inspired-by-lenet-5">Neural Network Example (Inspired by LeNet-5)</a></li>
<li><a href="#why-convolutions">Why Convolutions?</a></li>
<li><a href="#cnn-step-by-step-python-code">CNN Step By Step - Python Code</a></li>
</ul></li>
<li><a href="#case-studies">Case Studies</a>
<ul>
<li><a href="#classic-networks">Classic Networks</a>
<ul>
<li><a href="#lenet-5">LeNet-5</a></li>
<li><a href="#alexnet">AlexNet</a></li>
<li><a href="#vgg-16">VGG-16</a></li>
</ul></li>
<li><a href="#residual-networks-resnets">Residual Networks (ResNets)</a>
<ul>
<li><a href="#residual-block">Residual Block</a></li>
<li><a href="#signs-recognition-with-resnet-python-code">Signs Recognition with ResNet - Python Code</a></li>
</ul></li>
<li><a href="#one-by-one-convolution">One-By-One Convolution</a></li>
<li><a href="#inception-network">Inception Network</a>
<ul>
<li><a href="#inception-module">Inception Module</a></li>
</ul></li>
</ul></li>
<li><a href="#practical-advices-for-using-convnets">Practical Advices for Using ConvNets</a>
<ul>
<li><a href="#using-open-source-implementation">Using Open-Source Implementation</a></li>
<li><a href="#transfer-learning">Transfer Learning</a></li>
<li><a href="#data-augmentation">Data Augmentation</a>
<ul>
<li><a href="#implementing-distortion-during-training">Implementing Distortion During Training</a></li>
</ul></li>
<li><a href="#tips-for-doing-well-on-benchmarks-winning-competitions">Tips for Doing Well on Benchmarks/Winning Competitions</a></li>
</ul></li>
<li><a href="#detection-algorithms">Detection Algorithms</a>
<ul>
<li><a href="#classification-with-localization">Classification With Localization</a></li>
<li><a href="#landmark-detection">Landmark Detection</a></li>
<li><a href="#object-detection">Object Detection</a></li>
<li><a href="#convolutional-implementation-of-sliding-windows">Convolutional Implementation of Sliding Windows</a></li>
<li><a href="#bounding-box-prediction">Bounding Box Prediction</a></li>
<li><a href="#intersection-over-union-iou">Intersection Over Union (IOU)</a></li>
<li><a href="#non-max-suppression">Non-max Suppression</a></li>
<li><a href="#anchor-boxes">Anchor Boxes</a></li>
<li><a href="#yolo-algorithm">YOLO Algorithm</a></li>
<li><a href="#region-proposals-r-cnn">Region Proposals: R-CNN</a></li>
</ul></li>
<li><a href="#face-verification-vs-face-recognition">Face Verification vs Face Recognition</a>
<ul>
<li><a href="#one-shot-learning">One Shot Learning</a></li>
<li><a href="#siamese-network">Siamese Network</a></li>
<li><a href="#triplet-loss">Triplet Loss</a></li>
<li><a href="#face-verification-and-binary-classification">Face Verification and Binary Classification</a></li>
</ul></li>
<li><a href="#neural-style-transfer">Neural Style Transfer</a>
<ul>
<li><a href="#what-are-deep-convnets-learning">What are Deep ConvNets Learning?</a></li>
<li><a href="#neural-style-transfer-cost-function">Neural Style Transfer Cost Function</a>
<ul>
<li><a href="#content-cost-function">Content Cost Function</a></li>
<li><a href="#style-cost-function">Style Cost Function</a></li>
</ul></li>
<li><a href="#convolutions-in-1d-and-3d">Convolutions in 1D and 3D</a></li>
</ul></li>
</ul>
</nav>


<main>


<h1 id="building-blocks-of-cnn">Building Blocks of CNN</h1>

<h2 id="edge-detection">Edge Detection</h2>

<h3 id="vertical-edge-detection">Vertical Edge Detection</h3>

<p><img src="/NN/edge.PNG" alt="vertical edge detection" /></p>

<h3 id="vertical-and-horizontal-edge-detection">Vertical and Horizontal Edge Detection</h3>

<p><img src="/NN/hvedge.PNG" alt="horizontal and vertical edge detection" /></p>

<h3 id="convolution-operation">Convolution Operation</h3>

<ul>
<li>tensorflow: <code>tf.nn.conv2d</code></li>
<li>keras: <code>conv2D</code></li>
</ul>

<hr />

<h2 id="padding">Padding</h2>

<ul>
<li><code>$n\times n$</code> image, convolves with a <code>$f\times f$</code> filter <code>$\Rightarrow (n-f+1)\times(n-f+1)$</code> output</li>
<li>downside:

<ol>
<li>shrinking output</li>
<li>pixels at the corner is touched only once, information from the edge is thrown away</li>
</ol></li>
<li>padding with zeros, <code>$p=$</code> padding amount</li>
<li>output: <code>$(n+2p-f+1)\times (n+2p-f+1)$</code></li>
</ul>

<hr />

<h3 id="valid-and-same-convolutions">Valid and Same Convolutions</h3>

<ul>
<li><strong>valid</strong>: no padding</li>
<li><strong>same</strong>: output size = input size, <code>$2p+1=f$</code>, <code>$f$</code> is usually odd</li>
</ul>

<hr />

<h2 id="strided-convolution">Strided Convolution</h2>

<ul>
<li><code>$n\times n$</code> image, convolves with a <code>$f\times f$</code> filter, with padding <code>$p$</code> and stride <code>$s$</code>

<ul>
<li><code>$\Rightarrow$</code> output size <code>$=\left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)\times \left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)$</code></li>
</ul></li>
</ul>

<hr />

<h2 id="convolution-over-volume">Convolution Over Volume</h2>

<p><img src="/NN/rgb.PNG" alt="rgb" /></p>

<ul>
<li>input size: <code>$n\times n\times n_C$</code> (<code>$n_C = $</code> # of channels)</li>
<li><code>$n_C'$</code> filters, each of size: <code>$f\times f\times n_C$</code>  (each has <code>$f\times f\times n_C$</code> parameters)</li>
<li>output size: <code>$(n-f+1)\times (n-f+1)\times n_C'$</code></li>
</ul>

<p><img src="/NN/volume.PNG" alt="multiple filters" /></p>

<h2 id="one-layer-of-a-convolutional-network">One Layer of a Convolutional Network</h2>

<ul>
<li>Notations: layer <code>$\ell$</code>

<ul>
<li><code>$f^{[\ell]}$</code>  = filter size</li>
<li><code>$p^{[\ell]}$</code> = padding</li>
<li><code>$s^{[\ell]}$</code> = stride</li>
<li><code>$n^{[\ell]}_C$</code> = # of filters</li>
<li>input size: <code>$n^{[\ell-1]}_H\times n^{[\ell-1]}_W\times n^{[\ell-1]}_C$</code></li>
<li>each filter size: <code>$f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C$</code></li>
<li>output size: <code>$n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$</code>  , where $$n^{[\ell]}_H=\left\lfloor \frac{n^{[\ell-1]}_H +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor,\\<br />
n^{[\ell]}_W=\left\lfloor \frac{n^{[\ell-1]}_W +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor$$</li>
<li>activations: <code>$n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$</code></li>
<li>weights: <code>$f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C\times n^{[\ell]}_C$</code></li>
<li>bias: <code>$1\times 1\times 1\times n^{[\ell]}_C$</code></li>
</ul></li>
</ul>

<p><img src="/NN/layer.PNG" alt="one layer of cnn" /></p>

<h2 id="simple-convolutional-network-example">Simple Convolutional Network Example</h2>

<p><strong><u>Types of layer in a convolutional network</u></strong>:</p>

<ul>
<li>Convolution (CONV)</li>
<li>Pooling (POOL)</li>
<li>Fully connected (FC)</li>
</ul>

<p><img src="/NN/cnn.PNG" alt="simple cnn" /></p>

<h2 id="pooling-layer">Pooling Layer</h2>

<ul>
<li>hyperparameters

<ul>
<li><code>$f$</code>: filter size</li>
<li><code>$s$</code>: stride</li>
<li><strong>max</strong> or <strong>average</strong> pooling</li>
<li><code>$p$</code>: padding (rarely use)</li>
</ul></li>
<li>no parameters to learn!</li>
<li>input size: <code>$n_H\times n_W\times n_c$</code></li>
<li>output size (no padding): <code>$\lfloor \frac{n_H-f}{s}+1\rfloor \times \lfloor \frac{n_W-f}{s}+1\rfloor\times n_c$</code></li>
</ul>

<hr />

<h2 id="neural-network-example-inspired-by-lenet-5">Neural Network Example (Inspired by LeNet-5)</h2>

<p><img src="/NN/lenet.PNG" alt="lenet5" /></p>

<h2 id="why-convolutions">Why Convolutions?</h2>

<ul>
<li><strong>parameter sharing</strong>: a <u>feature detector</u> (such as a vertical edge detector) that is useful in one part of the image is probably useful in another part of the image; reduces the total number of parameters, thus <strong>reducing overfitting</strong></li>
<li><strong>sparsity of connections</strong>: in each layer, each output value depends only on a small number of inputs</li>
</ul>

<hr />

<h2 id="cnn-step-by-step-python-code">CNN Step By Step - Python Code</h2>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/07-CNN-Step-by-Step/CNN-Step-by-Step.ipynb">Building a CNN Step by Step</a></p>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/07-CNN-Step-by-Step/CNN-Application.ipynb">CNN Application-Sign Recognition</a></p>

<hr />

<h1 id="case-studies">Case Studies</h1>

<h2 id="classic-networks">Classic Networks</h2>

<h3 id="lenet-5">LeNet-5</h3>

<ul>
<li>structure: CONV - POOL - CONV - POOL - FC - FC - OUTPUT</li>
<li><code>$n_H, n_W \downarrow$</code>, <code>$n_C\uparrow$</code></li>
<li># of parameters: 60k</li>
<li>advanced:

<ul>
<li>not use sigmoid/tanh, use ReLU</li>
<li>use max pool, not avg pool</li>
<li>LeNet-5 adds nonlinearity after pooling</li>
</ul></li>
</ul>

<p><img src="/NN/lenet5.PNG" alt="lenet5" /></p>

<h3 id="alexnet">AlexNet</h3>

<ul>
<li>similar to LeNet, but much bigger, 60m parameters</li>
</ul>

<p>$$
(11\times 11\times 3+1)\times 96+(5\times 5\times 96+1)\times 256 + (3\times 3\times 256+1)\times 384 \\<br />
+(3\times3\times 384+1)\times 384+ (3\times3\times 384 +1)\times 256 + (9216+1)\times 4096 \\<br />
+(4096+1)\times 4096 + (4096+1)\times 10 =58,322,314
$$</p>

<ul>
<li>ReLU activation</li>
<li>multiple GPUs</li>
<li>local response normalization (LRN) 【does not help much】</li>
</ul>

<p><img src="/NN/alex.PNG" alt="alex" /></p>

<h3 id="vgg-16">VGG-16</h3>

<ul>
<li># of parameters: 138m</li>
<li>VGG-19, an even bigger version</li>
<li><code>$n_H, n_W \downarrow$</code> by a factor of 2, <code>$n_C\uparrow$</code>  by a factor of 2</li>
</ul>

<p><img src="/NN/vgg.PNG" alt="vgg" /></p>

<h2 id="residual-networks-resnets">Residual Networks (ResNets)</h2>

<ul>
<li>very deep neural networks are difficult to train because of <strong>vanishing/exploding gradients</strong></li>
<li>skip connection</li>
</ul>

<h3 id="residual-block">Residual Block</h3>

<ul>
<li>short-cut</li>
</ul>

<p>$$
a^{[\ell+2]}=g(z^{[\ell +2]}+a^{[\ell]})
$$</p>

<p><img src="/NN/resblock.PNG" alt="residual block" /></p>

<ul>
<li>residual networks: helps to build deep networks</li>
</ul>

<p><img src="/NN/resnet.PNG" alt="resnet" /></p>

<ul>
<li>why resnets work?

<ul>
<li>if <code>$W^{[\ell+2]}=0$</code>, <code>$b^{[\ell+2]}=0$</code>, <code>$g$</code> is ReLU activation, <code>$\Rightarrow a^{[\ell+2]}=a^{[\ell]}$</code> (identity function is easy for resnet to learn) <code>$\Rightarrow$</code> guarantees not to hurt performance</li>
<li><code>$z^{[\ell+2]}$</code> needs to be the same dimension as <code>$a^{[\ell]}$</code>:

<ul>
<li>use SAME convolution, or</li>
<li>add extra matrix,  <code>$a^{[\ell+2]}=g(z^{[\ell +2]}+W_s a^{[\ell]})$</code>, <code>$W_s$</code> can be parameters to learn, or zero padding</li>
</ul></li>
</ul></li>
</ul>

<p><img src="/NN/resnet2.PNG" alt="resnet" /></p>

<hr />

<h3 id="signs-recognition-with-resnet-python-code">Signs Recognition with ResNet - Python Code</h3>

<p><a href="https://github.com/augustrobo/Neural-Networks-and-Deep-Learning/blob/master/08-Residual-Networks/keras-residual-network/Residual%20Networks.ipynb">jupyter notebook</a></p>

<hr />

<h2 id="one-by-one-convolution">One-By-One Convolution</h2>

<ul>
<li>pooling: shrinks <code>$n_H, n_W$</code></li>
<li><code>$1\times 1$</code> convolution, aka networks in network

<ul>
<li>interpretation: a <u><strong>fully-connect</strong></u> neural network applied to each of the <code>$n_H\times n_W$</code> positions</li>
<li>increases or decreases <code>$n_C$</code></li>
<li>adds nonlinearity</li>
</ul></li>
</ul>

<p><img src="/NN/1by1.PNG" alt="one by one" /></p>

<p><img src="/NN/1by12.PNG" alt="one by one" /></p>

<hr />

<h2 id="inception-network">Inception Network</h2>

<p><img src="/NN/inception.PNG" alt="inception" /></p>

<ul>
<li># of multiplications: <code>$28\times 28\times 192\times 5\times 5\times 32\approx 120$</code>m</li>
</ul>

<p><img src="/NN/inception2.PNG" alt="inception" /></p>

<ul>
<li># of multiplications: <code>$28\times 28\times 192\times 16 + 28\times28\times16\times5\times5\times 32\approx 12.4$</code>m</li>
</ul>

<p><img src="/NN/inception3.PNG" alt="inception" /></p>

<h3 id="inception-module">Inception Module</h3>

<p><img src="/NN/inception4.PNG" alt="inception" /></p>

<p><img src="/NN/inceptionnetwork.PNG" alt="inception network" /></p>

<hr />

<h1 id="practical-advices-for-using-convnets">Practical Advices for Using ConvNets</h1>

<ol>
<li>use architectures of networks published in the literature</li>
<li>use open source implementations if possible</li>
<li>use pretrained models and fine-tune on your dataset</li>
</ol>

<h2 id="using-open-source-implementation">Using Open-Source Implementation</h2>

<ul>
<li>github: open-source code</li>
</ul>

<h2 id="transfer-learning">Transfer Learning</h2>

<ul>
<li><p>open-source weights</p></li>

<li><p>small training set:</p>

<ul>
<li>train only the softmax layer weights, freeze all of the earlier layers&rsquo; weights</li>
<li>precompute the last activation, save to disk, as input features of a shallow nn</li>
</ul></li>

<li><p>large training set:</p>

<ul>
<li>freeze fewer layers, train latter layers or your own network</li>
<li>open weights as initialization, then train the whole network</li>
</ul></li>
</ul>

<p><img src="/NN/transfer.PNG" alt="transfer" /></p>

<hr />

<h2 id="data-augmentation">Data Augmentation</h2>

<ul>
<li>mirroring</li>
<li>random cropping</li>
<li>rotation</li>
<li>shearing</li>
<li>local warping</li>
<li>color shifting

<ul>
<li>advanced: <strong>PCA</strong> [alexnet paper, PCA color augmentation]</li>
</ul></li>
</ul>

<hr />

<h3 id="implementing-distortion-during-training">Implementing Distortion During Training</h3>

<p><img src="/NN/cpu.PNG" alt="cpu" /></p>

<hr />

<h2 id="tips-for-doing-well-on-benchmarks-winning-competitions">Tips for Doing Well on Benchmarks/Winning Competitions</h2>

<ul>
<li>Ensembling:  train several NNs independently and average their outputs</li>
<li>Multi-crop at test time:

<ul>
<li>run classifier on multiple versions of test images and average results</li>
<li>e.g. 10-crop</li>
</ul></li>
</ul>

<p><img src="/NN/crop.PNG" alt="crop" /></p>

<hr />

<h1 id="detection-algorithms">Detection Algorithms</h1>

<ul>
<li>image classification</li>
<li>classification with localization</li>
<li>detection</li>
</ul>

<p><img src="/NN/localization.PNG" alt="localization" /></p>

<h2 id="classification-with-localization">Classification With Localization</h2>

<ul>
<li>outputs:

<ul>
<li>class label</li>
<li>bounding box

<ul>
<li>mid-point <code>$(b_x,b_y)$</code></li>
<li>height: <code>$b_h$</code></li>
<li>weight: <code>$b_w$</code></li>
</ul></li>
</ul></li>
</ul>

<p>$$
y = \begin{pmatrix}
p_c\newline
b_x\newline
b_y\newline
b_h\newline
b_w\newline
c_1\newline
c_2\newline
c_3\newline
\end{pmatrix},\  p_c=\begin{cases}
1,&amp; \exists\text{ object}\newline
0,&amp;\text{otherwise}
\end{cases},\ c_i=\begin{cases}
1,&amp; \exists\text{ object }i\newline
0,&amp;\text{otherwise}
\end{cases}
$$</p>

<p><img src="/NN/local.PNG" alt="localization" /></p>

<ul>
<li>loss function:</li>
</ul>

<p>$$
\mathcal{L}(\hat{y},y)=\begin{cases}
\Vert \hat{y}-y\Vert_2^2, &amp;\text{ if }y_1=1 \newline
(\hat{y}_1-y_1)^2, &amp;\text{ if }y_1=0
\end{cases}
$$</p>

<ul>
<li>in practice, can use

<ul>
<li>logistic loss on <code>$p_c$</code></li>
<li>squared error on <code>$b_.$</code></li>
<li>log-likelihood loss on the softmax output of <code>$c_.$</code></li>
</ul></li>
</ul>

<hr />

<h2 id="landmark-detection">Landmark Detection</h2>

<ul>
<li>output xy-coordinates of important points</li>
</ul>

<p><img src="/NN/landmark.PNG" alt="landmark" /></p>

<h2 id="object-detection">Object Detection</h2>

<ul>
<li>closely-cropped images</li>
<li><strong>sliding window detection</strong>

<ul>
<li>before the rise of nn, use linear classifiers</li>
<li>if use cnn, computationally expensive</li>
</ul></li>
</ul>

<p><img src="/NN/sliding.PNG" alt="sliding window" /></p>

<h2 id="convolutional-implementation-of-sliding-windows">Convolutional Implementation of Sliding Windows</h2>

<ul>
<li>turning FC layer into convolutional layers</li>
</ul>

<p><img src="/NN/fc.PNG" alt="fc" /></p>

<p><img src="/NN/sw.PNG" alt="sliding window" /></p>

<ul>
<li>make all the predictions at the same time</li>
</ul>

<p><img src="/NN/sw2.PNG" alt="sliding window" /></p>

<hr />

<h2 id="bounding-box-prediction">Bounding Box Prediction</h2>

<ul>
<li><p>how to output accurate bounding box? YOLO algorithm</p></li>

<li><p>YOLO = You Only Look Once</p></li>

<li><p>convolutional implementation</p></li>
</ul>

<p><img src="/NN/yolo.PNG" alt="yolo" /></p>

<ul>
<li>specify the bounding boxes</li>
</ul>

<p><img src="/NN/bounding.PNG" alt="bounding boxes" /></p>

<hr />

<h2 id="intersection-over-union-iou">Intersection Over Union (IOU)</h2>

<ul>
<li>evaluating object localization</li>
</ul>

<p>$$
\text{&ldquo;correct&rdquo; if }IOU = \frac{\text{size of intersection area}}{\text{size of union area}} \geq 0.5
$$</p>

<p><img src="/NN/iou.PNG" alt="iou" /></p>

<hr />

<h2 id="non-max-suppression">Non-max Suppression</h2>

<ul>
<li>problem: multiple detection of the same object</li>
</ul>

<p><img src="/NN/nonmax.PNG" alt="nonmax" /></p>

<p><img src="/NN/nonmax2.PNG" alt="nonmax" /></p>

<hr />

<h2 id="anchor-boxes">Anchor Boxes</h2>

<ul>
<li>detect overlapping objects</li>
</ul>

<p><img src="/NN/anchor.PNG" alt="anchor" /></p>

<p><img src="/NN/anchor2.PNG" alt="anchor" /></p>

<hr />

<h2 id="yolo-algorithm">YOLO Algorithm</h2>

<p><img src="/NN/yolotraining.PNG" alt="yolo training" /></p>

<p><img src="/NN/yoloprediction.PNG" alt="yolo prediction" /></p>

<p><img src="/NN/yolooutput.PNG" alt="yolo output" /></p>

<hr />

<h2 id="region-proposals-r-cnn">Region Proposals: R-CNN</h2>

<ul>
<li>R-CNN:

<ul>
<li>propose regions (segmentation algorithm)</li>
<li>classify proposed regions one at a time, output label + bounding box</li>
<li>slow</li>
</ul></li>
<li>Fast R-CNN:

<ul>
<li>use convolutional implementation of sliding windows to classify all the proposed windows</li>
</ul></li>
<li>Faster R-CNN:

<ul>
<li>use convolutional network to propose regions</li>
</ul></li>
</ul>

<p><img src="/NN/rcnn.PNG" alt="rcnn" /></p>

<hr />

<h1 id="face-verification-vs-face-recognition">Face Verification vs Face Recognition</h1>

<ul>
<li>verification:

<ul>
<li>input image, name/ID</li>
<li>output whether the input image is that of the claimed person</li>
</ul></li>
<li>recognition:

<ul>
<li>has a database of <code>$K$</code> persons</li>
<li>get an input image</li>
<li>output ID if the image is any of the <code>$K$</code> persons (or &ldquo;not recognized&rdquo;)</li>
</ul></li>
</ul>

<hr />

<h2 id="one-shot-learning">One Shot Learning</h2>

<ul>
<li>learn from one example to recognize the person again</li>
<li>learn a similarity function:

<ul>
<li><code>$d(img1, img2)$</code> = degrees of difference between images</li>
<li>if <code>$d(img1, img2)\leq \tau$</code> , &lsquo;same&rsquo; , otherwise, &lsquo;different&rsquo;</li>
</ul></li>
<li>how to train the function?</li>
</ul>

<hr />

<h2 id="siamese-network">Siamese Network</h2>

<ul>
<li>output: <code>$f(\cdot)$</code>, encoding of image example</li>
<li>define</li>
</ul>

<p>$$
d(x^{(1)}, x^{(2)})=\Vert f(x^{(1)})-f(x^{(2)})\Vert_2^2
$$</p>

<p><img src="/NN/siamese.PNG" alt="siamese" /></p>

<ul>
<li>goal of learning:

<ul>
<li><code>$\Vert f(x^{(i)})-f(x^{(j)})\Vert_2^2$</code> is small of <code>$x^{(i)},x^{(j)}$</code> are the same person</li>
<li><code>$\Vert f(x^{(i)})-f(x^{(j)})\Vert_2^2$</code> is large of <code>$x^{(i)},x^{(j)}$</code> are different persons</li>
</ul></li>
</ul>

<hr />

<h2 id="triplet-loss">Triplet Loss</h2>

<ul>
<li>Anchor/Positive/Negative image</li>
<li>learning objective:

<ul>
<li>want <code>$\Vert f(A)-f(P)\Vert^2\leq \Vert f(A)-f(N)\Vert^2$</code></li>
<li>make sure not output trivial encoding</li>
<li>modification: <code>$\Vert f(A)-f(P)\Vert^2- \Vert f(A)-f(N)\Vert^2+\alpha\leq 0$</code></li>
<li><code>$\alpha$</code> = margin</li>
</ul></li>
<li>loss function: given 3 images <code>$(A, P, N)$</code>,</li>
</ul>

<p>$$
\mathcal{L}(A,P,N)=max(\Vert f(A)-f(P)\Vert^2- \Vert f(A)-f(N)\Vert^2+\alpha, 0)
$$</p>

<ul>
<li>overall cost:</li>
</ul>

<p>$$
J = \sum_{i=1}^m\mathcal{L}(A^{(i)}, P^{(i)}, N^{(i)})
$$</p>

<ul>
<li>training set: 10k pictures of 1k person (multiple pics of the same person)</li>
<li>choosing the triplets <code>$A,P,N$</code>:

<ul>
<li>during training, if <code>$A,P,N$</code> are randomly chosen, <code>$d(A,P)+\alpha\leq d(A,N)$</code> is easily satisfied</li>
<li>need to choose triplets that are hard to train on</li>
</ul></li>
</ul>

<hr />

<h2 id="face-verification-and-binary-classification">Face Verification and Binary Classification</h2>

<p><img src="/NN/face.PNG" alt="face" /></p>

<p><img src="/NN/face2.PNG" alt="face" /></p>

<hr />

<h1 id="neural-style-transfer">Neural Style Transfer</h1>

<ul>
<li>Content/Style/Generated image</li>
</ul>

<p><img src="/NN/style.PNG" alt="style" /></p>

<h2 id="what-are-deep-convnets-learning">What are Deep ConvNets Learning?</h2>

<p><img src="/NN/visual.PNG" alt="visual" /></p>

<ul>
<li>Pick a unit in layer 1. Find the nine image patches that maximize the unit&rsquo;s activation.</li>
<li>Repeat for other 9 units.</li>
<li>Deeper layers will see larger image patches.</li>
</ul>

<p><img src="/NN/visual2.PNG" alt="visual" /></p>

<p><a href="https://arxiv.org/pdf/1311.2901.pdf">Visualizing and understanding convolutional networks</a></p>

<hr />

<h2 id="neural-style-transfer-cost-function">Neural Style Transfer Cost Function</h2>

<p>$$
J(G)=\alpha J_{content}(C,G) +\beta J_{style}(S,G)
$$</p>

<ul>
<li>initiate G randomly</li>
<li>use gradient descent to minimize <code>$J(G)$</code></li>
</ul>

<hr />

<h3 id="content-cost-function">Content Cost Function</h3>

<ul>
<li>similarity between C and G</li>
<li>use pre-trained ConvNet (e.g. VGG network)</li>
<li>say you use hidden layer <code>$\ell$</code> to compute cost</li>
<li>let <code>$a^{[\ell](C)}$</code> and  <code>$a^{[\ell](G)}$</code> be  the activation of layer <code>$\ell$</code> on the images</li>
<li>if   <code>$a^{[\ell](C)}$</code> and  <code>$a^{[\ell](G)}$</code> are similar, both images gave similar content</li>
</ul>

<p>$$
J_{content}(C,G):=\frac{1}{2}\Vert a^{[\ell](C)}-a^{[\ell] (G)}\Vert^2
$$</p>

<hr />

<h3 id="style-cost-function">Style Cost Function</h3>

<ul>
<li><p>say  you use hidden layer <code>$\ell$</code> &rsquo;s activation to measure <strong>style</strong></p></li>

<li><p>define <strong>style</strong> as correlation between activations across channels</p></li>

<li><p>style matrix (gram matrix)</p></li>
</ul>

<p>$$
a^{[\ell]}_{i,j,k}=\text{activation at }(i,j,k).\\<br />
G_{kk&rsquo;}^{[\ell]}=\sum_{i=1}^{n_H^{[\ell]}}\sum_{j=1}^{n_W^{[\ell]}} a_{ijk}^{[\ell]}a_{ijk&rsquo;}^{[\ell]},\ \ k,k&rsquo;=1,2,\ldots,n_C^{[\ell]}.\\<br />
G^{[\ell]}=(G_{kk&rsquo;}^{[\ell]})\in\mathbb{R}^{n_C^{[\ell]}\times n_C^{[\ell]}}.
$$</p>

<ul>
<li>style matrices for the style image and generated image <code>$G^{[\ell](S)}, G^{[\ell](G)}$</code></li>
<li>style cost</li>
</ul>

<p>$$
J_{style}^{[\ell]}(S,G)=\frac{1}{(2n_{H}^{[\ell]}n_W^{[\ell]}n_C^{[\ell]})^2}\sum_{k,k&rsquo;}\Vert G^{[\ell](S)}-G^{[\ell](G)}\Vert^2_F
$$</p>

<p>$$
J_{style}(S,G)=\sum_l \lambda^{[\ell]} J_{style}^{[\ell]}(S,G)
$$</p>

<hr />

<h2 id="convolutions-in-1d-and-3d">Convolutions in 1D and 3D</h2>

<p><img src="/NN/1d.PNG" alt="1d" /></p>

<p><img src="/NN/3d.PNG" alt="3d" /></p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

