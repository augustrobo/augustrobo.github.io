<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>面试想法 - CNN | NOWHERESVILLE</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <header>

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/vs.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">/home/nowheresville</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/">~/home</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/categories/">~/categories</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/tags/">~/tags</a>
      </li>
      

      
      
      <li class="pull-right">
        <a href="/index.xml">~/subscribe</a>
      </li>
      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">面试想法 - CNN</span></h1>

<h2 class="date">2018/04/11</h2>
<p class="terms">
  
  
  Categories: <a href="/categories/diary">diary</a> 
  
  
  
  
</p>
</div>





<main>
<p>今天面试岗位的面试官让我讲一下CNN，就糊里糊涂把最近学的和实践的一点点东西讲了一下，脑海中却有一个疑问，貌似一直都没思考过CNN为什么这么强大？</p>

<p>在吴恩达的深度学习专项课程中，并不是我第一次知道卷积神经网络了，读研时就和同学在讨论班讨论过，模模糊糊知道做了什么数学运算（学数学的毛病，一切都从公式出发，才是<strong>舒适区</strong>），至于为什么要做convolution？CNN有多强大？为什么这么强大？完全不了解，以为是个很艰深的课题。吴恩达的课程讲的真的很直观透彻，边听边有种感觉：懂了，原来这么简单。然而事后回想，真的懂了吗？</p>

<p>读研时借了一本机器学习图像识别的书，看完了第一章，觉得自己懂了，后面长篇大论的方法根本懒得看：所谓图像识别，就是把图像像素灰度或RGB数据拉长成为一个向量嘛，那跟一般的特征有什么区别？殊途同归，没什么意思。然而心中隐隐知道有种<strong>不对劲</strong>：真的这么简单粗暴的方法就可以吗？假如按照列拉成向量，普通的机器学习方法真的能够自动发现同一行像素之间的关联吗？能发现在附近的像素之间的关联吗？假如通过神经网络来学习，需要多深的网络才能把握这些复杂的邻近关系呢？正是心中有这些疑问，对这本书有点不以为然。</p>

<p>后来又听了一个图像识别的统计报告，听众寥寥无几，但是我对作者的方法很感兴趣，貌似是照列和行都拉长成两个向量？记不太清，总之作者是试图把握上下左右像素之间的关系，令我很感兴趣，一直念念不忘这个想法（虽然作者自称成果不太好，后续还要改进）。</p>

<p>今日突然联系起来，恍然大悟，这不就是CNN的想法吗？每次的convolution运算，都是学习了图像上一小片邻近像素的特征！心中感到一重震惊：为什么会有人这么聪明，能想出这样的方法；二重震惊：为什么之前没有人想出这样的简单明快的方法；三重震惊：LeNet-5在1995年就提出了，我竟然在2014年开始读研直到2017年毕业都不知道这么强大的方法。</p>

<p>在CNN之前，图像识别都要通过复杂的方法去提取线条等等特征，而CNN可以直接免去痛苦的特征工程，让算法自己去学习图像中从简单、到复杂的特征，在吴恩达的课中，贴出了这篇论文【<a href="https://arxiv.org/pdf/1311.2901.pdf">Visualizing and understanding convolutional networks</a>】，完全直观地感受到CNN的奇妙。</p>

<p>由此想到，对一些高维数据，很多算法都需要先做特征选择或者模型选择，以免特征高度共线性影响算法的表现。然而，高度相关的特征真的不应该共存吗？这难道不是算法的局限，而不是数据的问题吧！假如把这些特征排列成一定的几何结构，比如说和图像一样的矩阵，邻近的都是相关的特征，那么同样可以适用CNN的想法，使用同一个filter和这些特征去相乘。就像课程中提到的：</p>

<blockquote>
<p>Why Convolutions?</p>

<ul>
<li><strong>parameter sharing</strong>: a feature detector (such as a vertical edge detector) that is useful in one part of the image is probably useful in another part of the image; reduces the total number of parameters, thus <strong>reducing overfitting</strong></li>
<li><strong>sparsity of connections</strong>: in each layer, each output value depends only on a small number of inputs</li>
</ul>
</blockquote>

<p>学完CNN后，我立刻去参加了hackathon的一个图像年龄识别比赛，使用ResNet有点过拟合，于是就稍微改进了一下，加了几层Dropout，又调换了BN和ReLU的顺序，很顺利正确率达到了83%，排到了13名，如果再做ensemble，data augmentation,  10-crop，结果肯定会更好，只是有点犯懒了（对已知结果的事情我都有点犯懒）……并且有点惭愧，只是做了一点微小的工作，剩下的都是电脑自己在算。图像识别有了目前这些强大的CNN算法，提高准确率只是一些调参或者计算机算力的问题了吧，功绩是属于提出这些方法的人，让我再一次感觉：为什么能想到？为什么想到的又不是我？</p>

<p>翻翻各种数据的招聘信息，今年已完全是深度学习的天下，也不过研究生毕业一年有余，之前还是机器学习的江山，在之前是数据分析、统计分析，对毕业生的要求似乎越来越“高”？但其实就是把握这股风潮的时机准不准，我似乎落后于时代了，学校教育落后于时代了，是终身学习，还是认命淘汰？又或是在线教育的春天？想去kaggle参加几个比赛充当简历上的项目，看到动辄几百G的数据只好放弃。</p>

<p>在懂的人眼里，所谓掌握了多种算法的人，是有多掌握呢？我对数据分析感到灰心，似乎一切都是套路，总觉得自己做的种种，都可以用if/else来替代，深度学习调参也是如此吧，假如计算机算力进一步提升，现在的调参工作就好像以前工厂里流水线熟练工那般没技术含量，所谓的数据复杂，不过是人的懒惰以及算力有限，有了神经网络，特征工程也没那么重要了，垃圾数据的产生，也是人的问题，我想设计一个自动傻瓜式数据分析软件，把数据扔进去，问你几个问题，自动帮你处理缺失值等等，自动跑几个常用的算法，自动出结果，我做的事，有什么不能自动化的吗？</p>

</main>

    <footer>
      <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

<script src="//yihui.name/js/math-code.js"></script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>

<script async src="//yihui.name/js/center-img.js"></script>

      
      <hr/>
      <a href="mailto:zhuxm2017@163.com">Email</a> | <a href="https://github.com/augustrobo">Github</a>
      
    </footer>
  </body>
</html>

