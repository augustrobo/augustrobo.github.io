<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NeuralNetworks on NOWHERESVILLE</title>
    <link>/categories/neuralnetworks/</link>
    <description>Recent content in NeuralNetworks on NOWHERESVILLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/neuralnetworks/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Sequence Models</title>
      <link>/post/2018/03/20/rnns/</link>
      <pubDate>Tue, 20 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/20/rnns/</guid>
      <description>Examples of Sequence Data  speech recognition music generation sentiment classification DNA sequence analysis machine translation video activity recognition name entity recognition  Recurrent Neural Networks Forward Propagation  at time $t$:  $$ a^{t} = g(W_{aa}a^{t-1} + W_{ax}x^{t} + b_a)\\
\hat{y}^{t}=g(W_{ya}a^{t}+b_y) $$
 simplified notation:  $$ a^{t} = g(W_a[a^{t-1},x^{t}] + b_a), \text{where} $$
$$ \begin{cases} W_a =[W_{aa}, W_{ax}]\newline [a^{t-1},x^{t}] =\begin{pmatrix} a^{t-1}\newline x^{t} \end{pmatrix} \end{cases}\tag{1} $$
$$ \hat{y}^{t}=g(W_ya^{t}+b_y)\tag{2} $$</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>/post/2018/03/15/cnns/</link>
      <pubDate>Thu, 15 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/15/cnns/</guid>
      <description>Building Blocks of CNN Edge Detection Vertical Edge Detection Vertical and Horizontal Edge Detection Convolution Operation  tensorflow: tf.nn.conv2d keras: conv2D  Padding  $n\times n$ image, convolves with a $f\times f$ filter $\Rightarrow (n-f+1)\times(n-f+1)$ output downside:  shrinking output pixels at the corner is touched only once, information from the edge is thrown away  padding with zeros, $p=$ padding amount output: $(n+2p-f+1)\times (n+2p-f+1)$  Valid and Same Convolutions  valid: no padding same: output size = input size, $2p+1=f$, $f$ is usually odd  Strided Convolution  $n\times n$ image, convolves with a $f\times f$ filter, with padding $p$ and stride $s$  $\Rightarrow$ output size $=\left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)\times \left(\lfloor\frac{n+2p-f+1}{s}\rfloor+ 1\right)$   Convolution Over Volume  input size: $n\times n\times n_C$ ($n_C = $ # of channels) $n_C&#39;$ filters, each of size: $f\times f\times n_C$ output size: $(n-f+1)\times (n-f+1)\times n_C&#39;$  One Layer of a Convolutional Network  Notations: layer $\ell$  $f^{[\ell]}$ = filter size $p^{[\ell]}$ = padding $s^{[\ell]}$ = stride $n^{[\ell]}_C$ = # of filters input size: $n^{[\ell-1]}_H\times n^{[\ell-1]}_W\times n^{[\ell-1]}_C$ each filter size: $f^{[\ell]}\times f^{[\ell]}\times n^{[\ell-1]}_C$ output size: $n^{[\ell]}_H\times n^{[\ell]}_W\times n^{[\ell]}_C$ , where $$n^{[\ell]}_H=\left\lfloor \frac{n^{[\ell-1]}_H +2p^{[\ell]}-f^{[\ell]}}{s^{[\ell]}} +1 \right\rfloor,\\</description>
    </item>
    
    <item>
      <title>CS231n-CNN for Visual Recognition-Assignment1</title>
      <link>/post/2018/03/07/cs231n/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/03/07/cs231n/</guid>
      <description>Image Classification Challenges  Viewpoint variation. A single instance of an object can be oriented in many ways with respect to the camera. Scale variation. Visual classes often exhibit variation in their size (size in the real world, not only in terms of their extent in the image). Deformation. Many objects of interest are not rigid bodies and can be deformed in extreme ways. Occlusion. The objects of interest can be occluded.</description>
    </item>
    
    <item>
      <title>Neural Networks and Deep Learning</title>
      <link>/post/2018/02/27/nn/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/27/nn/</guid>
      <description>Introduction What is a Neural Network?  ReLU = Rectified Linear Unit     Input Output Application Model     Home Features Price Real Estate NN   Ad, User info Click on ad? 0/1 Online Advertising NN   Image Object Photo Tagging CNN   Audio Text transcript Speech Recognition RNN   English Chinese Machine Translation RNN   Image, Radar info Position of other cars Autonomous Driving Hybrid     Image - convolutional neural network, CNN sequence data (temporal data, time series) - recurrent neural network, RNN   Structured data: database Unstructured data: audio, image, text  Why is Deep Learning taking off?</description>
    </item>
    
    <item>
      <title>ML - Neural Networks</title>
      <link>/post/2017/11/04/neural-networks/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/04/neural-networks/</guid>
      <description>Model Representation $$ a_i^{[j]} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$} \\
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} $$
Forward Propagation: Vectorized Implementation $$ [x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots $$
 input: $x$. Setting $a^{[1]}=x$ linear combination: $z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$ activation: $a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$  Non-linear Classification Example: XNOR operator    x1 x2 XNOR = NOT XOR     0 0 1   0 1 0   1 0 0   1 1 1     using a hidden layer with two nodes (sigmoid activation function)  Multiclass Classification Cost Function  $\{x^{(i)},y^{(i)}\}$, $y\in\mathbb{R}^K$</description>
    </item>
    
  </channel>
</rss>