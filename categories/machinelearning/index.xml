<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MachineLearning on NOWHERESVILLE</title>
    <link>/categories/machinelearning/</link>
    <description>Recent content in MachineLearning on NOWHERESVILLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Data Manipulation at Scale 大规模数据处理</title>
      <link>/post/2018/02/01/data-manipulation-at-scale/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/2018/02/01/data-manipulation-at-scale/</guid>
      <description>课程介绍  Data Manipulation at Scale: Systems and Algorithms 大规模数据处理：系统和算法【 Coursera链接 】 Data Science at Scale Specialization 的第一门课，共4门 制作方： University of Washington 教学方： Bill Howe  Twitter Sentiment Analysis 情感分析 分析目的  access the twitter Application Programming Interface(API) using python estimate the public&amp;rsquo;s perception (the sentiment) of a particular term or phrase analyze the relationship between location and mood based on a sample of twitter data  步骤  CLI安装 oauth2 package [用于授权登录]  pip3 install oauth2   创建 twitter 账号 到 https://apps.</description>
    </item>
    
    <item>
      <title>ML - Application Example - Photo OCR</title>
      <link>/post/2017/11/11/ocr/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/11/ocr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Large Scale Machine Learning</title>
      <link>/post/2017/11/10/large-scale-ml/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/10/large-scale-ml/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Recommender Systems</title>
      <link>/post/2017/11/09/recommender-systems/</link>
      <pubDate>Thu, 09 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/09/recommender-systems/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Anomaly Detection</title>
      <link>/post/2017/11/08/anomaly/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/08/anomaly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Dimensionality Reduction</title>
      <link>/post/2017/11/08/dimension-reduction/</link>
      <pubDate>Wed, 08 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/08/dimension-reduction/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Unsupervised Learning</title>
      <link>/post/2017/11/07/unsupervised/</link>
      <pubDate>Tue, 07 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/07/unsupervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Support Vector Machines</title>
      <link>/post/2017/11/06/svms/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/06/svms/</guid>
      <description>Cost Function Logistic regression $$ \min_{\theta}\frac{1}{N}\sum_{i=1}^N \left[y^{(i)}\left(-\log \frac{1}{1+e^{-\theta^T x^{(i)}}}\right)+(1-y^{(i)})\left(-\log \frac{e^{-\theta^T x^{(i)}}}{1+e^{-\theta^T x^{(i)}}}\right)\right] +\frac{\lambda}{2N}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\gg 0$ if $y=0$, want $\theta^Tx\ll 0$  SVM $$ \min_{\theta} C\sum_{i=1}^N \left[y^{(i)}cost_1(\theta^Tx^{(i)}) +(1-y^{(i)})cost_0(\theta^T x^{(i)})\right] +\frac{1}{2}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\geq 1$ if $y=0$, want $\theta^Tx\leq -1$
  Large Margin Classification  SVM Decision Boundary  $$ \min_{\theta} \frac{1}{2}\sum_{j=1}^p\theta_j^2\\
s.t. \begin{cases} \theta^Tx^{(i)}\geq 1 &amp;amp;\text{if } y^{(i)}=1\newline \theta^Tx^{(i)}\leq -1 &amp;amp;\text{if } y^{(i)}=0\newline \end{cases} $$ Simplification: $\theta_0=0, p=2$ $$ \min_{\theta} \frac{1}{2}\Vert\theta\Vert_2^2\\</description>
    </item>
    
    <item>
      <title>ML - Machine Learning System Design</title>
      <link>/post/2017/11/05/system-design/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/05/system-design/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML - Neural Networks</title>
      <link>/post/2017/11/04/neural-networks/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/04/neural-networks/</guid>
      <description> Model Representation $$ a_i^{[j]} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$} \\
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} $$
Forward Propagation: Vectorized Implementation $$ [x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots $$
 input: $x$. Setting $a^{[1]}=x$ linear combination: $z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$ activation: $a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$  Non-linear Classification Example: XNOR operator    x1 x2 XNOR = NOT XOR     0 0 1   0 1 0   1 0 0   1 1 1     using a hidden layer with two nodes (sigmoid activation function)  Multiclass Classification </description>
    </item>
    
    <item>
      <title>ML - Logistic Regression</title>
      <link>/post/2017/11/02/logistic/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/02/logistic/</guid>
      <description>Logistic Regression Model  goal: want $h_{\theta}(x) \in [0,1]$ $h_{\theta}(x)=g(\theta^Tx)$, where $g(z)=\frac{1}{1+e^{-z}}$ (sigmoid function/ logistic function) interpretations: $h_{\theta}(x)$ = estimated probability that $y=1$ on input $x$, that is  $$h(x)=h_{\theta}(x) = \Pr(y=1|x;\theta)$$
 prediction: predict $y=1$ if $h_{\theta}(x)\geq 0.5 \Leftrightarrow \theta^Tx\geq 0$
 Decision Boundary: $\theta^Tx= 0$
 Nonlinear Decision Boundary: add complex (i.e. polynomial) terms
 Notations: training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where
  $$ x = \begin{pmatrix}x_0\newline x_1\newline\vdots\newline x_p\end{pmatrix}\in\mathbb{R}^{p+1}, x_0=1,y\in\{0,1\} $$</description>
    </item>
    
    <item>
      <title>ML - Linear Regression with Multiple Variables</title>
      <link>/post/2017/11/01/lr/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/01/lr/</guid>
      <description>Linear Regression with One Variable  $X$: space of input values, $Y$: space of output values
 training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$
 goal: given a training set, to learn a function $h : X \rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. For historical reasons, this function $h$ is called a hypothesis.
 $h(x)=h_{\theta}(x) = \theta_0 + \theta_1 x$
 cost function (squared error function, or mean squared error, MSE):</description>
    </item>
    
    <item>
      <title>ISLR CH10 Unsupervised Learning</title>
      <link>/post/2017/10/10/unsupervised/</link>
      <pubDate>Tue, 10 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/10/unsupervised/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH9 Support Vector Machines</title>
      <link>/post/2017/10/09/svm/</link>
      <pubDate>Mon, 09 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/09/svm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH8 Tree-Based Methods</title>
      <link>/post/2017/10/08/tree-based-methods/</link>
      <pubDate>Sun, 08 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/08/tree-based-methods/</guid>
      <description>IntuitionModelsRegression TreesPredicting Baseball Players’ SalariesAlgorithmTree PruningReferencesIntuitionRegression/Classification
Stratify or segment the predictor space into simple regions
Prediction for a given observation: mean or mode of the training observations in the region to which it belongs
The set of splitting rules can be summarized in a tree \(\Rightarrow\) Decision Tree Methods
Pros: simple, easy to interpret, and nice to visualize</description>
    </item>
    
    <item>
      <title>ISLR CH7 Moving Beyond Linearity</title>
      <link>/post/2017/10/07/beyond-linearity/</link>
      <pubDate>Sat, 07 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/07/beyond-linearity/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH6 Linear Model Selection and Regularization</title>
      <link>/post/2017/10/06/model-selection/</link>
      <pubDate>Fri, 06 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/06/model-selection/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH5 Resampling Method</title>
      <link>/post/2017/10/05/resampling/</link>
      <pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/05/resampling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH4 Classification</title>
      <link>/post/2017/10/04/classification/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/04/classification/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ISLR CH3 Linear Regression</title>
      <link>/post/2017/10/03/linear-regression/</link>
      <pubDate>Tue, 03 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/10/03/linear-regression/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>