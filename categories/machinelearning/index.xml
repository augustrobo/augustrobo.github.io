<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MachineLearning on NOWHERESVILLE</title>
    <link>/categories/machinelearning/</link>
    <description>Recent content in MachineLearning on NOWHERESVILLE</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 Nov 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML - Support Vector Machines</title>
      <link>/post/2017/11/06/svms/</link>
      <pubDate>Mon, 06 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/06/svms/</guid>
      <description>Cost Function Logistic regression $$ \min_{\theta}\frac{1}{N}\sum_{i=1}^N \left[y^{(i)}\left(-\log \frac{1}{1+e^{-\theta^T x^{(i)}}}\right)+(1-y^{(i)})\left(-\log \frac{e^{-\theta^T x^{(i)}}}{1+e^{-\theta^T x^{(i)}}}\right)\right] +\frac{\lambda}{2N}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\gg 0$ if $y=0$, want $\theta^Tx\ll 0$  SVM $$ \min_{\theta} C\sum_{i=1}^N \left[y^{(i)}cost_1(\theta^Tx^{(i)}) +(1-y^{(i)})cost_0(\theta^T x^{(i)})\right] +\frac{1}{2}\sum_{j=1}^p\theta_j^2 $$
 if $y=1$, want $\theta^Tx\geq 1$ if $y=0$, want $\theta^Tx\leq -1$
  Large Margin Classification  SVM Decision Boundary  $$ \min_{\theta} \frac{1}{2}\sum_{j=1}^p\theta_j^2\\
s.t. \begin{cases} \theta^Tx^{(i)}\geq 1 &amp;amp;\text{if } y^{(i)}=1\newline \theta^Tx^{(i)}\leq -1 &amp;amp;\text{if } y^{(i)}=0\newline \end{cases} $$ Simplification: $\theta_0=0, p=2$ $$ \min_{\theta} \frac{1}{2}\Vert\theta\Vert_2^2\\</description>
    </item>
    
    <item>
      <title>ML - Machine Learning System Design</title>
      <link>/post/2017/11/05/system-design/</link>
      <pubDate>Sun, 05 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/05/system-design/</guid>
      <description> How to Improve Learning Algorithms  get more training examples feature selection get additional features add polynomial features decrease/increase $\lambda$  Evaluate a Learning Algorithm  training/validation/test set, e.g. 60/20/20 split training/validation/test error model selection:  optimize parameters by minimizing training error for each model select the model with the least validation error (e.g. select polynomial degree $d$)  estimate generalization error using test error  Machine Learning Diagnostic: Bias vs Variance  can rule out certain courses of action as being unlikely to improve the performance of your learning algorithm significantly high bias = underfit = high training error, validation error $\approx$ training error high variance = overfit = low training error, validation error $\gg$ training error  Regularization and Bias/Variance  large $\lambda$ $\Rightarrow$ high bias = underfit small $\lambda$ $\Rightarrow$ high variance = overfit choose regularization parameter  create a list of $\lambda$s create a list of models iterate through the $\lambda$s and for each $\lambda$ go through all the models to optimize parameter $\Theta$ compute validation error using the learned $\Theta$ select the best combo with the least validation error estimate generalization error using test error   Learning Curves  as the training set gets larger, the training error increases error value will plateau out after a certain training set size Experiencing high bias:  Low training set size: low training error, high validation error Large training set size: high training error, validation error $\approx$ training error getting more training data will not (by itself) help much    Experiencing high variance:  Low training set size: low training error, high validation error Large training set size: training error increases with training set size, validation error continues to decrease without leveling off; difference between 2 errors remains significant getting more training data is likely to help   Debugging a Learning Algorithm  get more training examples $\rightarrow$ fit high variance feature selection $\rightarrow$ fit high variance get additional features $\rightarrow$ fit high bias add polynomial features $\rightarrow$ fit high bias increase $\lambda$ $\rightarrow$ fit high variance decrease $\lambda$ $\rightarrow$ fit high bias  Neural Networks and Overfitting  small nn  fewer parameters more prone to underfitting computationally cheaper  larger nn  more parameters more prone to overfitting computationally expensive use regularization to address overfitting   </description>
    </item>
    
    <item>
      <title>ML - Neural Networks</title>
      <link>/post/2017/11/04/neural-networks/</link>
      <pubDate>Sat, 04 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/04/neural-networks/</guid>
      <description>Model Representation $$ a_i^{[j]} = \text{&amp;ldquo;activation&amp;rdquo; of unit $i$ in layer $j$} \\
\Theta^{[j]} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$} $$
Forward Propagation: Vectorized Implementation $$ [x] \rightarrow [a^{[2]}] \rightarrow [a^{[3]}]\rightarrow \cdots $$
 input: $x$. Setting $a^{[1]}=x$ linear combination: $z^{[j]}=\Theta^{[j-1]}a^{[j-1]}, \ \ j=2,3,\ldots$ activation: $a^{[j]}=g(z^{[j]}), \ \ j=2,3,\ldots$  Non-linear Classification Example: XNOR operator    x1 x2 XNOR = NOT XOR     0 0 1   0 1 0   1 0 0   1 1 1     using a hidden layer with two nodes (sigmoid activation function)  Multiclass Classification Cost Function  $\{x^{(i)},y^{(i)}\}$, $y\in\mathbb{R}^K$</description>
    </item>
    
    <item>
      <title>ML - Logistic Regression</title>
      <link>/post/2017/11/02/logistic/</link>
      <pubDate>Thu, 02 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/02/logistic/</guid>
      <description>Logistic Regression Model  goal: want $h_{\theta}(x) \in [0,1]$ $h_{\theta}(x)=g(\theta^Tx)$, where $g(z)=\frac{1}{1+e^{-z}}$ (sigmoid function/ logistic function) interpretations: $h_{\theta}(x)$ = estimated probability that $y=1$ on input $x$, that is  $$h(x)=h_{\theta}(x) = \Pr(y=1|x;\theta)$$
 prediction: predict $y=1$ if $h_{\theta}(x)\geq 0.5 \Leftrightarrow \theta^Tx\geq 0$
 Decision Boundary: $\theta^Tx= 0$
 Nonlinear Decision Boundary: add complex (i.e. polynomial) terms
 Notations: training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$, where
  $$ x = \begin{pmatrix}x_0\newline x_1\newline\vdots\newline x_p\end{pmatrix}\in\mathbb{R}^{p+1}, x_0=1,y\in\{0,1\} $$</description>
    </item>
    
    <item>
      <title>ML - Linear Regression with Multiple Variables</title>
      <link>/post/2017/11/01/lr/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017/11/01/lr/</guid>
      <description>Linear Regression with One Variable  $X$: space of input values, $Y$: space of output values
 training set $\{(x^{(i)}, y^{(i)})\}_{i=1}^N$
 goal: given a training set, to learn a function $h : X \rightarrow Y$ so that $h(x)$ is a “good” predictor for the corresponding value of $y$. For historical reasons, this function $h$ is called a hypothesis.
 $h(x)=h_{\theta}(x) = \theta_0 + \theta_1 x$
 cost function (squared error function, or mean squared error, MSE):</description>
    </item>
    
  </channel>
</rss>